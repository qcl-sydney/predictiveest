\section{Introduction} 

Machine learning frameworks are advancing predictive estimation capabilities in diverse fields such as engineering, finance, econometrics, meteorology, seismology, and physics. In predictive estimation, a dynamically evolving system is observed and any temporal correlations encoded in the observations are used to predict the future state of the system. For sophisticated predictive estimation problems, machine learning tools offer unique opportunities to develop better predictors under alternative theoretical frameworks; and /or to leverage computational resources to optimise predictors using large datasets. For classical systems, machine learning techniques have enabled state tracking, control, and forecasting for highly non-linear and noisy dynamical trajectories or complex measurement protocols (e.g. \cite{garcia2016optimal, bach2004learning, tatinati2013hybrid, hall2011reinforcement, hamilton2016ensemble}). Particle-based Bayesian frameworks (e.g. particle filtering, unscented or sigma-point filtering) yield significant advantages in accommodating non-linear models arising from a physical system, system dynamics, or measurement protocols \cite{candy2016bayesian}. Recently, an ensemble of unscented Kalman filters demonstrated state estimation and forward predictions for chaotic, non-linear systems in the absence of a prescribed model and instead used nearest neighbour strategies between `particles' to model dynamics  \cite{hamilton2016ensemble}. For non-chaotic, multi-component stationary random signals, other algorithmic approaches have been particularly useful for tracking instantaneous frequency and phase information (e.g. \cite{boashash1992estimating2, ji2016gradient}) to enable short-run forecasting.  
\\
\\ 
However, it is not straightforward to extend machine learning predictive estimation techniques to non-classical systems with observations consisting of quantum projective measurements. A projective measurement forces a freely evolving quantum system to assume a particular quantum state. In contrast, observing a classical system does not influence its inherent dynamical evolution. Hence, predictive estimation problems using projective measurement records for quantum systems are fundamentally different to their purely classical counterparts. The analysis of projective measurement records is often conceptualised as pattern recognition or image reconstruction problems in machine learning, as examples, in characterising the initial or final state of quantum system (e.g. \cite{struchalin2016experimental, sergeevich2011characterization, mahler2013adaptive}) or reconstructing the historical evolution of a quantum system based on large measurement records (e.g. \cite{stenberg2016characterization, shabani2011efficient, shen2014reconstructing, de2016estimation, tan2015prediction, huang2017neural}). In adaptive or sequential Bayesian learning applications, it is often the case that a projective measurement protocol is designed or adaptively manipulated to efficiently yield noise filtered information about a quantum system (e.g. \cite{bonato2016optimized, wiebe2015bayesian}). Typically, the object of interest is either static or dynamically uncorrelated in time (white) as  measurement protocols are reapplied. Hence, the canonical real-time tracking and prediction problem in classical applications - where a non-linear, stochastic trajectory of a system is tracked using noisy measurements and short-run forecasts are made - is under-explored for quantum systems with projective measurements.
\\ 
\\
In this manuscript, we develop numerical approaches to track the stochastic evolution of the state of a single two level system (qubit) using a sequence of projective measurements and we forecast the qubit state once the measurement record ceases. A sequence of binary data is obtained by projectively measuring - and resetting - the qubit, where  a 0 or 1 outcome represents the qubit state in a single measurement. Under a slowly drifting, non-Markovian environmental dephasing noise, the probability of measuring a qubit state inherits properties of dephasing noise. Consequently, our binary, time-series data encodes that the probability of getting a 0 or 1 qubit state is slowly drifting. If each qubit outcome is effectively a biased coin flip, then in our application, the bias on the coin changes stochastically under dephasing noise. Further, a coin-flip (projective measurement) discretises a continous time random dephasing process into a sequence of \textit{stochastic but temporally correlated} qubit phases. These phases govern the superposition of 0 and 1 qubit states prior to measurement and hence they affect the probability of a seeing a 0 or 1 qubit state. We use machine learning algorithms to extract temporal correlations from measurements and forecast the qubit state. The performance benchmark of an algorithm is to maximise the forecast period where its qubit state predictions are better than predicting the mean behaviour of the qubit under dephasing noise. We test our algorithms in numerical experiments representing realistic operating environments encountered in a laboratory. Our analysis considers two important types of measurement records, such that (a) single shot qubit outcomes are pre-processed before being given to an algorithm and (b) an algorithm acts directly on 0 or 1 qubit outcomes. The latter measurement record necessitates a complex, \textit{non-linear} measurement model for predictive estimation algorithms.
\\
\\
We seek appropriate machine learning frameworks  firstly, to track and predict non-Markovian qubit state evolution, and secondly, to accommodate non-linear measurement models. In standard Bayesian learning protocols, the output of Bayesian analysis at one instant of time is a probability distribution describing best possible knowledge of a true current state of a system. Typically, this knowledge is propagated in time using a theoretically known transition probability distribution. This transition probability distribution encodes, in signal processing language, time-domain (stochastic) `dynamics' for the true state. For Markov (temporally uncorrelated) processes, the form of the transition probability is simple and the `one step ahead' true state is conditioned only on the current state of the system and not on the system's history. An analytically simple transition probability distribution is widely used for resampling procedures in particle-based methods and used for marginalisation procedures in sequential Bayesian methods \cite{candy2016bayesian}. In our application, the Markov condition is immediately violated. One may design an appropriate transition probability distribution such that increasing non-Markovianity would contribute the increasing dimensionality of the problem as the `one-step ahead' state is conditioned on a increasing set of past states. Developing a theoretical, non-Markovian transition probability distribution in the context of qubit tracking in our application is beyond the scope of this manuscript, though it is the subject of recent research for classical applications (e.g  \cite{jacob2017bayesian}).   
\\ 
\\ 
As an alternative approach, we design a deterministic way to correlate Markovian processes such that a certain general class of non-Markovian dynamics can be approximately tracked without violating assumptions of a machine learning protocol. While this approach applies to a range of machine learning frameworks, we choose theoretically accessible and computationally efficient frameworks for encoding non-Markovian stochastic dynamics  - namely, Kalman Filtering and Gaussian Process Regression. Kalman Filtering (KF) and Gaussian Process Regression (GPR) are both examples of well established Bayesian approaches for tracking stochastic, non-linear true trajectories with Gaussian Markov noise inputs. Both KF and GPR represent mechanisms by which temporal correlations (equally, dynamics) are encoded into an algorithm's structure such that projection of datasets onto this structure enables meaningful learning, white noise filtering and forward prediction.  KF represents a recursive learning technique that easily lends itself to real time, adaptive filtering and control protocols. The wide-scale success of KF frameworks is based on well established extensions to non-linear, non-Gaussian regimes \cite{grewal2001theory}. GPR represents a batch learning algorithm with immense flexibility for tracking non-linear stochastic state dynamics, but tolerates only a linear measurement action \cite{rasmussen2005gaussian}. Both  KF and GPR allow us to approximately track non-Markovian stochastic dynamics, and additionally, a Kalman framework allows us to incorporate non-linear measurement models.
\\
\\ 
We simulate the use of suitably modified KF and GPR algorithms for real-time qubit state tracking and short-run predictions in realistic operating environments. It can be shown that for a certain class of true stochastic qubit state trajectories, a general representation using a collection of oscillators or using so-called `autoregressive' processes of finite order are guaranteed to converge to the true, unknown trajectory \cite{karlin2012first}. Using pre-processed measurements, we design a KF algorithm and a GPR algorithm to track general qubit dynamics using an oscillator approach. We contrast oscillator approaches against a Kalman filter with autoregressive dynamics. We use machine learning protocols and simulated measurements to train algorithms such that we can track and predict the qubit state trajectory for a \textit{single realisation} of non-Markovian dephasing noise engineered from arbitrary power spectral densities. These numerical experiments investigate whether theoretical convergence occurs on timescales which enable meaningful qubit state predictions, where timescales are relative to sampling rates and properties of `true' (engineered) dephasing noise. We find that an autoregressive Kalman framework yields model-robust forward prediction horizons compared to other approaches. We extend autoregressive Kalman framework to incorporate a non-linear, coin-flip measurement model and we observe that a Kalman filter can use binary qubit outcomes to conduct qubit state prediction.
\\
\\
In what follows, we describe our physical setting in \cref{sec:main:PhysicalSetting}. We provide an overview of GPR and KF frameworks in  \cref{sec:main:OverviewofPredictive Methodologies}, and we specify algorithms under consideration in this paper. For pre-processed measurement records, we consider four algorithmic approaches: a Least Squares Filter (LSF) from \cite{mavadia2017}; an Autoregressive Kalman Filter (AKF); a so-called Liska Kalman Filter from \cite{livska2007} adapted for a Fixed oscillator Basis (LKFFB); and a suitably designed GPR learning protocol. For binary qubit outcomes, we extend AKF to a Quantised Kalman Filter (QKF). In \cref{sec:main:Optimisation}, we present optimisation procedures for tuning all algorithms. Results from numerical investigations are presented in \cref{sec:main:Performance} and predictive performance of all algorithms is discussed in \cref{sec:main:discussion}. 


\section{Physical Setting \label{sec:main:PhysicalSetting}}  
\label{sec:main:1} 

Our physical set-up considers a sequence of projective measurements performed on a qubit. Each projective measurement yields a 0 or 1 outcome representing the state of the qubit. The qubit is then reset, and the exact procedure is repeated. If no dephasing is present, then the probability of obtaining a binary outcome does not change as more qubit measurements are performed. If slowly drifting environmental dephasing is present, then the probability of obtaining a given binary outcome also drifts stochastically. In essence, we are using a qubit to probe dephasing noise and our procedure encodes a continuous time dephasing process into time-stamped, discrete binary samples.  The correlation between any two time-separated qubit measurements arises entirely from a classical, non-Markov dephasing noise field. 
\\
\\
Formally, an arbitrary environmental dephasing process manifests as time-dependent stochastic detuning, $\delta \omega (t)$, between the qubit frequency and the master clock. This detuning is an experimentally measurable quantity in a Ramsey protocol, as in \cref{fig:main:Predive_control_Fig_overview_17_one} (a). A non-zero detuning induces a relative stochastic phase accumulation between the two possible $0$ and $1$ states of a qubit, thereby affecting the statistical likelihood of measuring a particular qubit outcome. 
\\
\\
In a sequence of $n$ Ramsey measurements spaced $\Delta t$ apart with a \textit{fixed Ramsey wait time}, $\tau$, the change in the statistics of measured outcomes over this measurement record depends solely on the dephasing  $\delta \omega(t)$.   We assume that the measurement action over $\tau$ timescales is much faster than the slow time dependence of dephasing, and $\Delta t \gg \tau$. The resulting measurement record is a set of binary outcomes,  $\{d_n\}$,  where qubit state dynamics were governed by $n$ true stochastic phases, $\state := \{\state_n\}$. We define the statistical likelihood for observing a single shot, $d_n$, using Born's rule \cite{ferrie2013}:

\begin{align}
P(d_n | \state_n, \tau, n \Delta t) &= \begin{cases} \cos(\frac{\state(n \Delta t, n\Delta t + \tau)}{2})^2 \quad \text{for $d=1$} \\   \sin(\frac{\state(n \Delta t, n\Delta t + \tau)}{2})^2  \quad \text{for $ d=0$} \end{cases} \label{eqn:main:likelihood}
\end{align}
where  $ \state(n \Delta t, n\Delta t + \tau) \equiv \int_{n \Delta t}^{n \Delta t +\tau} \delta \omega(t') dt'$ and we use the shorthand $\state(n \Delta t , n\Delta t + \tau) \equiv \state_n$. The notation $P(d_n | \state_n, \tau, n \Delta t)$ refers to the conditional probability of seeing a measurement $d_n$ given that a stochastic phase, $\state_n$, accumulated over the qubit at $t = n \Delta t$. In the noiseless case, $P(d=1| f, \tau) = 1 \quad \forall n $, such that a qubit can be manipulated perfectly in the absence of net phase accumulation due to environmental dephasing. This procedure discretises $\delta \omega(t)$ into a random process, $f$, governing qubit dynamics. 
\\
\\
\begin{figure}[h!]
    \includegraphics[scale=1]{Predive_control_Fig_overview_17_one} 
    \caption{ \label{fig:main:Predive_control_Fig_overview_17_one} Physical Setting: In (a), we define the Hamiltonian for stochastic qubit dynamics under arbitrary environmental dephasing using a covariance stationary, non-Markovian detuning $\delta \omega(t)$ with an arbitrary power spectral density. A sequence of Ramsey experiments with fixed wait time $\tau$ yield single shot outcomes $\{ d_n \}$, with likelihood $P(d_n|\state_n, \tau,n)$,  conditioned on a mean-square ergodic sequence of true phases, $\{ f_n\}$, with $n \in [-N_T, 0]$ indexing time during data collection. Our objective is to maximise forward time $n \in [0, N_P]$ for which an algorithm uses measurement data to predict a future qubit state and incurs a lower Bayes prediction risk relative to predicting the mean value of the dephasing noise [dark gray shaded]. In (b), single shot outcomes are processed to yield noisy accumulated phase estimates, $\{ y_n\}$, corrupted by measurement noise $\{v_n\}$. The choice of $\{d_n\}$ or $\{y_n\}$ as datasets for predictive estimation corresponds to non-linear or linear measurement records in (b) and (c).}
\end{figure} 
\cref{fig:main:Predive_control_Fig_overview_17_one}(b) depicts a non-linear measurement record, $\{ d_n\}$. Each $d_n$ [black dots] corresponds to a single projective measurement on a qubit yielding a 0 or 1 outcome. The sequence $\{ d_n\}$ can be considered as a sequence of biased coin flips, where the underlying bias of the coin is a non-Markovian, discrete time process. The value of the bias is given by \cref{eqn:main:likelihood} at each $n$. Subsequently, the qubit state is reset, but the dephasing noise correlations manifest again through Born's rule to yield another random value for the bias at $n+1$. The non-linearity of the measurement model is defined with respect to $f$ where \cref{eqn:main:likelihood} is interpreted as a non-linear measurement action on $f$ for Bayesian learning frameworks. 
\\
\\
\cref{fig:main:Predive_control_Fig_overview_17_one}(c) depicts a linear measurement record, $\{ y_n\}$.  Each $y_n$ is the sum of a true qubit phase, $\state_n$, and Gaussian white measurement noise, $v_n$.  The sequence $\{ y_n\}$ is generated by pre-processing binary measurements, $\{ d_n\}$. Pre-processing refers to averaging procedures over $\tau$-like timescales much faster than drift of $\delta \omega (t)$ such that $\{ y_n\}$ is a measurement record fed to learning algorithms. Pre-processing summarises a range of experimental techniques to extract $\{ y_n\}$ from $\{ d_n\}$. Firstly, one may low-pass (or decimation) filtered a sequence of $\{ d_n\}$ binary outcomes to yield $\hat{P}(d_t | \state_t, \tau, t)$ from which accumulated phase corrupted by measurement noise, $\{ y_n\}$, can be obtained from \cref{eqn:main:likelihood} (see Appendices). Secondly, one may perform $M$ runs of the experiment over which $\delta \omega (t)$ is approximately constant under the slow drift assumption. For $M\tau << \Delta t$, we obtain an estimate of  $\state_n$ at $t = n \Delta t $ using a Bayesian scheme or Fourier analysis. 
\\
\\
In the case of a qubit evolving under stochastic dephasing, we have no apriori dynamical model for a qubit state evolution under dephasing. Our task is to build a dynamical model to approximately track $\state$ and enable qubit state predictions. We impose properties on environmental dephasing such that our theoretical design in GPR and KF enable meaningful predictions. We assume dephasing is non-Markovian, covariance stationary and mean square ergodic, that is, a single realisation of the process $\state$ is drawn from a power spectral density of arbitrary but non-Markovian shape. We further assume that $\state$  is a Gaussian process. In the subsequent section, we define the theoretical structure of KF and GPR algorithms. 

\section{Overview of Predictive Methodologies \label{sec:main:OverviewofPredictive Methodologies}}
% \begin{widetext}
\begin{figure*}
    \includegraphics[scale=1.]{Predive_control_Fig_overview_17_two} 
    \caption{ \label{fig:main:Predive_control_Fig_overview_17_two} Predictive Methodologies: (a) In GPR, a prior  distribution over true phase sequences $P(\state), \state \equiv \{ \state_n \}$ is constrained by a linear Bayesian likelihood of observed data, $\{ y_n\}$. The prior encodes dephasing noise correlations by defining covariance relations for the $i, j$-th time points using  $\Sigma_\state^{i, j}$ and optimising over its free parameters during training. The moments of the resulting predictive distribution $P(\state^*|y)$ are interpreted as pointwise predictions and their pointwise uncertainties when evaluated for $n>0$.  (b) In KF, the Kalman state and its variance correspond to moments of a Gaussian distribution propagated in time via $\Phi$, and filtered via the Kalman gain, $\gamma$ at timestep $n$. The design of $\Phi$ deterministically colors a white noise process $\{w_n \}$ and `encodes' an apriori structure for learning dephasing noise correlations. Prediction proceeds by propagating forwards with $\gamma_n=0, n>0$. Additive white Gaussian measurement noise $v_n$ corrupts all measurement records.}
\end{figure*}
% \end{widetext}
We introduce algorithmic learning under KF and GPR frameworks in \cref{fig:main:Predive_control_Fig_overview_17_two}. Stochastic qubit evolution is depicted for one realisation of $\state$ [red] given noisy observations [black dots] corrupted by Gaussian white measurement noise $v_n$.  Both frameworks start with a prior Gaussian distribution over qubit states that is constrained by the measurement record to yield a posterior Gaussian distribution of the qubit state. The prior captures assumptions about the qubit state before any data is seen and the posterior captures our best knowledge of qubit state under a Bayesian framework.  Both KF and GPR yield a posterior distribution which is used to generate qubit state estimates for $n<0$ and predictions for $n>0$ [black solid]. For linear measurement records, $h(x) \mapsto Hx$ and $f \equiv Hx$ linking GPR and KF notation. 
\\
\\
The key feature of a Kalman filter is a recursive learning procedure shown in \cref{fig:main:Predive_control_Fig_overview_17_two} (a) and we comment on the physical interpretation of Kalman notation. Our knowledge of the qubit state is summarised by the prior and a posteriori Gaussian probability distributions and these are created and collapsed at each time step. The mean of these distributions is the true Kalman state, $x$, and the covariance of these distributions, $P$, is the uncertainty in our knowledge of $x$. The Kalman gain, $\gamma_n$, updates our knowledge of $(x_n, P_n)$ within each time step $n$. The dynamical model  $\Phi_n$ propagates the $(x_n, P_n)$ to the next time step, such that the posterior moments at $n$ define the prior at $n+1$.  Predictions at $n=0$ occur when moments are propagated using $\Phi_n, n>0 $ but for zero gain. 
\\
\\
In order to encode stochastic qubit dynamics in KF algorithms, we deviate from standard implementations such that our true Kalman state and its uncertainty, $(x, P)$, do not have a direct physical interpretation. In standard KF implementations, the sequence $\{x_n\}$, defines a hidden signal that cannot be observed with incurring measurement noise and $\Phi_n$ is known. Often, Kalman $x$ can represent a multi-component signal and while $x$ is driven by white noise, typical filtering implementations specify a deterministic component to the evolution of $x$ or provide a desired reference trajectory for the filter to follow. In our application, we define the Kalman state, $x$, the dynamical model $\Phi$, and a measurement action $h(x)$ such that the Kalman Filtering framework can track a non-Markovian qubit state trajectory due to an arbitrary realisation of $\state$.  Kalman $x$ has no apriori deterministic component and corresponds to arbitrary power spectral densities associated with $f$. As an illustrative example, in a linear regime, a true qubit phase sequence $\state$ has a physical interpretation, but $x$ and $\Phi$ are abstract entities designed to yield a sequence $\{x_n\}$  such that upon noiseless measurement, we recover $\state \equiv Hx$. Hence, the role of the Kalman $x$ is to represented a correlated process that, upon measurement, yields physically relevant quantities governing qubit dynamics.
\\
\\
A GPR learning protocol in \cref{fig:main:Predive_control_Fig_overview_17_two} (b)  chooses \textit{a random process} to best describe overall dynamical behaviour of the qubit state under one realisation of $f$. The key point is that sampling the prior or a posterior distribution in GPR yields random realisations of discrete time \textit{sequences}, not  individual random variables, and GPR considers the entire measurement record at once. The output of a GPR protocol is predictive distribution which we can evaluate at arbitrarily chosen collection of time labels, $n^*$, where we interpret the result as state estimation if $n^* < n =0$ and predictions if $n^* > n =0$. 
\\
\\
We encode stochastic qubit dynamics in GPR using a so-called `periodic kernel'. In standard GPR implementations, if any two observations are correlated such that the correlation strength depends only on the separation distance of the index of these observations, then these correlations are given by the covariance matrix, $\Sigma_\state$. Each $\Sigma_f^{i, j}$ element describe how any two observations at time step $i$ and $j$ separated by a distance $|i-j|$ must be correlated. For us, the non-Markovian dynamics of $f$ are not specified explicitly but are encoded in a general way through the choice of a `kernel' or covariance function, prescribing how $\Sigma_\state^{i,j}$ should be calculated. The Fourier transform of the kernel represents a power spectral density in Fourier space. A general design of $\Sigma_f^{i, j}$ allows one to probe arbitrary stochastic dynamics and equivalently, explore arbitrary regions in the Fourier domain. For example, Gaussian kernels (RBF) and mixtures of Gaussian kernels (RB) capture the continuity assumption that correlations die out as separation distances increase. We choose an infinite basis of oscillators summarised by the periodic kernel to enable us to probe arbitrary power spectral densities for $f$.
\\
\\
Irrespective of the choice of KF or GPR framework, an algorithm must maximise the forward prediction horizon. The forward prediction horizon is the number of time steps beyond the measurement record for which predictions of the qubit state are better than predicting the average behaviour of a qubit under dephasing. The fidelity of our algorithm during state estimation and prediction relative to the true state is expressed by the mathematical quantity known as a Bayes Risk, where a zero risk value corresponds to perfect predictive estimation. At each timestep, $n$, the Bayes risk is a mean square distance between truth, $\state$ , and prediction, $\hat{\state}$ calculated over an ensemble of $M$ different realisations of truth $\state$ and noisy datasets $\mathcal{D}$:
\begin{align}
L_{BR}(n | I) & \equiv \langle(\state_n - \hat{\state}_n)^2 \rangle_{\state,\mathcal{D}} \label{eqn:main:sec:ap_opt_LossBR}
\end{align}
The notation $L_{BR}(n | I)$ expresses that the Bayes Risk value at $n$ is conditioned on, $I$, a placeholder for free parameters in the design of the predictor,  $\hat{\state}_n$. State estimation risk is Bayes Risk incurred during $n \in [-N_T, 0]$; prediction risk is the Bayes Risk incurred during $n \in [0, N_P]$. State estimation and prediction risk regions for one realisation of dephasing noise are shaded in \cref{fig:main:Predive_control_Fig_overview_17_one,fig:main:Predive_control_Fig_overview_17_two,Predive_control_Fig_overview_17_three}.  The forward prediction horizon is the number of time steps for $ n \in [0, N_P]$ during which a predictive algorithm incurs a lower Bayes prediction risk than predicting $\hat{\state}_n \equiv \mu_f = 0 \quad \forall n$, namely, the mean qubit behaviour under zero mean dephasing noise. 
\\
\\
To follow, we introduce Kalman Filtering (KF) algorithms acting on linear and non-linear measurement records, and Gaussian Process Regression (GPR) on linear measurement records. 

\subsection{ Kalman Filtering (KF)}

A Kalman Filter recursively tracks the stochastic evolution of a hidden true state. An incoming stream of unreliable (noisy) observations are fed to a Kalman Filter, and the objective of the Kalman Filter is to recursively improve its estimate of the true state at any time, $n\Delta t$, given the past $n$ measurements. In order for a Kalman Filter to track a stochastically evolving qubit state in our application, the hidden true Kalman state $x_n$ must mimic stochastic dynamics  of a qubit under environmental dephasing. We propagate the hidden state $x_n$ according to a dynamical model $\Phi_n$ corrupted by Gaussian white process  noise, $w_n$.  
\begin{align}
x_n & = \Phi_n x_{n-1} + \Gamma_n w_n \label{eqn:KF:dynamics} \\
w_n & \sim \mathcal{N}(0, \sigma^2) \quad \forall n 
\end{align}
Process noise has no physical meaning in our application - $w_n$ is shaped by $\Gamma_n$ and deterministically colored by the dynamical model $\Phi_n$ to yield a non-Markovian $x_n$ representing qubit dynamics under generalised environmental dephasing. 
\\
\\
We measure $x_n$ using an ideal measurement protocol $h(x_n)$ and incur additional Gaussian white measurement noise $v_n$ with scalar covariance strength $R$, yielding scalar noisy observations $y_n$:
\begin{align}
y_n &= z_n + v_n \\
z_n & \equiv  h(x_n) \\
v_n & \sim \mathcal{N}(0, R) \quad \forall n
\end{align}
The measurement procedure, $h(x_n)$, can be linear or non-linear, allowing us to explore both regimes in our physical application.
\\
\\
Since we do not have a known dynamical model $\Phi$ for describing stochastic qubit dynamics under $\state$, we will need to make design choices for  $\{ x, \Phi, h(x), \Gamma \}$  such that $\state$ can be approximately tracked. These design choices will completely specify algorithms in this manuscript. For a linear measurement record,   $h(x) \mapsto Hx$ and we compare predictive performance if $\Phi$ models stochastic dynamics either via so-called `autoregressive' processes in AKF, or via a collection of oscillators in LKKFB. Second, we use dynamics of AKF to define a Quantised Kalman filter (QKF) with a non-linear, quantised measurement model such that the filter can act directly on binary qubit outcomes. We provide details in sub-sections below. 
 
\begin{figure} [h]
    \includegraphics[scale=1]{Predive_control_Fig_overview_17_three}
    \caption{\label{Predive_control_Fig_overview_17_three} Apriori Structure for $\Phi$: All Kalman dynamical models, $\Phi$, are mean square approximations to qubit dynamics under arbitrary covariance stationary, non-Markovian, mean square ergodic $\state$. AKF/QKF: Kalman $\Phi$, implements a weighted sum of $q$ past measurements driven by process noise, $w$. We represent $\Phi$ using a lag operator, $L^r: \state_n \mapsto \state_{n-r}$, and coefficients, $ \{ \phi_{q' \leq q} \}$ learned from LSF in \cite{mavadia2017}. This defines an autoregressive process of order $q$ and we use a high $q$ model to approximate any covariance stationary $\state$ [top]. LKFFB: Kalman $\Phi$ represents a collection of $J$ oscillators driven by process noise, $w$, where frequency of oscillators must span dephasing noise bandwidth. The instantaneous amplitude and phase of each basis oscillator can be derived from the Kalman state estimate $x_{j, n}$ at any $n$. Predictions combine learned amplitudes and phases for each basis oscillator and sum contributions over all $J$ [bottom].}
\end{figure}

\subsubsection{Autoregressive Kalman Filter (AKF)}

An AKF probes arbitrary, covariance stationary qubit dynamics such that the dynamic model is a weighted sum of $q$ past values driven by white noise i.e. an autoregressive process of order $q$, AR($q$). By Wold's decomposition, any zero mean covariance stationary process representing qubit dynamics has a representation in the mean square limit by an autoregressive process of finite order $q_c$, AR($q_c$), where the ability to approximate arbitrary power spectral density for a covariance stationary process typically falls with model complexity, $c$ \cite{west1996bayesian}. We design the Kalman dynamical model, $\Phi$, such that the true Kalman state is AR($q$) process that approximately tracks the qubit state. The study of AR($q$) processes falls under the study of a general class of techniques based on autoregressive moving average (ARMA) models in classical control engineering. For high $q$ models in a typical time-series  analysis, it is possible to decompose an AR($q$) into an ARMA model with fewer parameters \cite{brockwell1996introduction, salzmann1991detection}. However, we retain high $q$ model to probe arbitrary power spectral densities. Further, literature suggests a high $q$ approach is relatively easier than a full ARMA estimation problem and enables lower prediction errors \cite{wahlberg1989estimation,brockwell1996introduction}.
\\
\\
We write the Kalman dynamical operator $\Phi$ in terms of a lag operator, $L$, where each application of the lag operator delays a true state by one time step:
\begin{align}
L^r: f_n &\mapsto f_{n-r} \quad \forall r \leq n \\
\Phi(L) & \equiv  1 - \phi_1 L - \phi_2 L^2 - ... - \phi_q L^q 
\end{align}
Here, the set of $q$ coefficients $\{ \phi_q \}$ are the set of autoregressive coefficients which specify the dynamical model. Hence, the true stochastic Kalman state dynamics are:
\begin{align}
\Phi(L) \state_n & = w_n \\ 
\implies \state_n & = \phi_1 L \state_n + \phi_2 L^2 \state_n + ... + \phi_q L^q \state_n + w_n \\
 \state_n &= \phi_1 \state_{n-1} + \phi_2 \state_{n-2} + ... + \phi_q \state_{n-q} + w_n \label{eqn:main:ARprocess}
\end{align}
Formally, \cref{eqn:main:ARprocess} is an AR($q$) process. For small $q < 3$, it is possible to extract simple conditions on the coefficients, $\{ \phi_q \}$, that guarantee properties of $f$, for example, that $f$ is covariance stationary and mean square ergodic. In our application, we freely run arbitrary $q$ models via machine learning in order to improve our approximation of an arbitrary $f$. Any AR($q$) process can be recast (non-uniquely) into state space form, and we define the AKF by the following substitutions into Kalman equations:
\begin{align}
x_n & \equiv  \begin{bmatrix} f_{n} \hdots f_{n-q+1} \end{bmatrix}^T \\
\Gamma_n w_n & \equiv \begin{bmatrix} w_{n} 0 \hdots 0 \end{bmatrix}^T \\
\Phi_{AKF} & \equiv 
\begin{bmatrix}
\phi_1 & \phi_2 & \hdots & \phi_{q-1} & \phi_q \\ 
1 & 0 & \hdots & 0 & 0 \\  
0 & 1 & \ddots & \vdots & \vdots \\ 
0 & 0 & \ddots & 0 & 0 \\ 
0 & 0 & \hdots & 1 & 0 
\end{bmatrix} \quad \forall n \label{eqn:akf_Phi} \\
H & \equiv \begin{bmatrix} 1 0 \hdots 0 \end{bmatrix} \quad \forall n  
\end{align}
The matrix $\Phi_{AKF}$ is the dynamical model used to recursively propagate the unknown state during state estimation in the AKF. In general, the ${\phi_i}$ in $\Phi_{AKF}$ must be learned through an optimisation procedure where the total number of parameters to be optimised are $\{\phi_1, \hdots, \phi_q, \sigma^2, R \}$. This procedure yields the optimal configuration of the autoregressive Kalman filter, but at the computational cost of a $q+2$ dimensional optimisation problem for arbitrarily large $q$.
\\
\\
The Least Squares Filter (LSF) in \cite{mavadia2017} considers a weighted sum of past measurements to predict the $m$-th step ahead measurement outcome. A gradient descent algorithm learns the weights, $\{\phi_{q' \leq q}\}, q' = 1, ... , q $ for the previous $q$ past measurements, and a constant offset value for non-zero mean processes, to calculate the $m$ step ahead prediction, $m \in [0, N_P]$. The set of $m$ LSF models, collectively, define the set of predicted qubit states under an LSF.
\\
\\
For $m=1$, we assert that learned $\{\phi_{q' \leq q}\}$ in LSF effectively implements an AR($q$) process and we test via numerical experiments comparing LSF and AKF. For $m=1$, and for zero mean $w_n$, LSF in \cite{mavadia2017} by definition searches for coefficients for the weighted linear sum of past $q$ measurements, as in \cref{eqn:main:ARprocess}. We test our assertion by using an LSF to reduce the computational tractability of $q+2$ optimisation problem for an AKF for high order $q$. Namely, we use  $\{\phi_{q' \leq q}\}$ from LSF to define $\Phi_{AKF}$. Since Kalman noise parameters ($\sigma^2, R$) are subsequently auto-tuned using a Bayes Risk optimisation procedure, we optimise over potentially remaining model errors and measurement noise.
\\
\\
The choice $q$ for AKF and LSF is set by LSF while training LSF models. In general, LSF performance improves as $q$ increases and a full characterisation of model selection decisions for LSF are given in \cite{mavadia2017}. An absolute value of $q$ is somewhat arbitrary as it is relative to the extent to which a true $f$ is oversampled. For all analysis, we fix the ratio $q \Delta t = 0.1 [a.u.]$, where the experimental sampling rate is $1/\Delta t$ and $\{\phi_{q' \leq q}\}$ are identical in AKF and LSF. For simplicity, we fix a high $q$ model for all numerical experiments considered in this manuscript such that they exhibit numerical convergence behaviour during LSF training. In particular, numerical convergence for LSF means an analysis of errors generated as a gradient descent optimiser is used to learn autoregressive coefficients. Our choice of high $q$ is such that (a) state estimation errors gradually reduce with the number of iterations during a gradient descent optimisation in LSF, and (b), we operate in regimes where net state estimation error at the \textit{end} of a gradient descent optimisation exhibits diminishing returns as $q$ is increased in the underlying LSF model. Implementation details of gradient descent optimisation for LSF are relegated to \cite{mavadia2017}. We numerically confirm that gains for finely tuning $q$ for both LSF and AKF are insignificant for comparisons made in this manuscript.
\\
\\
The structure of AKF above is well-studied in classical engineering and control applications (e.g. a recursive least squares algorithm for adaptive feedforward control \cite{moon2006real} ) and presents opportunities to leverage existing knowledge for quantum control strategies.

\subsubsection{Liska Kalman Filter with Fixed Basis (LKFFB)}
In LKFFB, we probe stochastic qubit dynamics using a collection of oscillators.  We project our measurement record on $J^B$ oscillators with fixed frequency $\{ \omega_0^B j, j = 1, \hdots, J^B\}$. The structure of this Kalman filter, referred to as the Liska Kalman Filter (LKF), was developed in \cite{livska2007}. We incorporate a fixed basis to enable our application, yielding a Liska Kalman Filter with a Fixed Basis (LKFFB).
\\
\\
We track instantaneous amplitudes and phases explicitly for each basis oscillator With explicit phase and amplitude tracking, we enable state predictions by combining learned amplitudes and phases and projecting forwards in time. The superscript $ ^B$ indicates Fourier domain information about an algorithmic basis, as opposed to information about the true (unknown) dephasing process, and we drop this superscript for convenience.
\\
\\
For our application, the true hidden Kalman state, $x$, is a collection of sub-states, $x^j$, for each $j^{th}$ oscillator. Each sub-state is labeled by a real and imaginary component:
\begin{align}
x_n & \equiv \begin{bmatrix} x^{1}_{n} \hdots x^{j}_{n} \hdots x^{J}_{n} \end{bmatrix} \\
x^{j,1}_{n} & \equiv \text{estimates real $f$ component for $\omega_j$} \\
x^{j,2}_{n} & \equiv \text{estimates imaginary $f$ component for $\omega_j$} \\
x^j_n &\equiv \begin{bmatrix} x^{j,1}_{n} \\ x^{j,2}_{n} \\ \end{bmatrix} \equiv \begin{bmatrix} A^j_{n} \\ B^j_{n}  \end{bmatrix}
\end{align} 
We track the real and imaginary parts of the Kalman sub-state  simultaneously in order calculate the instantaneous amplitudes ($\norm{x^j_n}$) and phases ($\theta_{x^j_n}$)  for each Fourier component:
\begin{align}
\norm{x^j_n} & \equiv \sqrt{(A^j_{n})^2 + (B^j_{n})^2} \\
\theta_{x^j_n} & \equiv \tan{\frac{B^j_{n}}{A^j_{n}}}
\end{align}
We may probe dephasing noise to an arbitrarily high resolution for tracking qubit dynamics by choosing an arbitrarily high value for the ratio $J/\omega_0$ when defining the computational basis.
\\
\\
The dynamical model for LKFFB is a stacked collection of independent oscillators. The sub-state dynamics match the formalism of a Markovian stochastic process defined on a circle for each basis frequency, $\omega_j$, as in \cite{karlin2012first}. We stack $\Phi(j \omega_0 \Delta t) $ for all $\omega_j$ along the diagonal to obtain the full dynamical matrix for $\Phi_n$:
\begin{align}
\Phi_{n} & \equiv \begin{bmatrix} 
\Phi(\omega_0 \Delta t)\hdots 0  \\ 
 \hdots \Phi(j\omega_0 \Delta t) \hdots \\
0 \hdots \Phi(J \omega_0 \Delta t)  \end{bmatrix}\\ 
\Phi(j \omega_0 \Delta t) &\equiv \begin{bmatrix} \cos(j \omega_0 \Delta t) & -\sin(j \omega_0 \Delta t) \\ \sin(j \omega_0 \Delta t) & \cos(j \omega_0 \Delta t) \\ \end{bmatrix} \label{eqn:ap_approxSP:LKFFB_Phi} 
\end{align}

We observe numerically that instantaneous amplitude and phase information for different basis components are resolved at different timescales while the filter is receiving an incoming stream of measurements (see Appendices). In \cite{livska2007}, a state dependent process noise shaping matrix is introduced to enable potentially non-stationary instantaneous amplitude tracking in LKKFB for each individual oscillator: 
\begin{align}
\Gamma_{n-1} &\equiv \Phi_{n-1}\frac{x_{n-1}}{\norm{x_{n-1}}}
\end{align}
For the scope of this manuscript, we retain the form of $\Gamma_{n}$ in our application even if true qubit dynamics are covariance stationary. As such, $\Gamma_{n}$ depends on state estimates $x$. For this choice of $\Gamma_{n}$, we deviate from classical Kalman filters because recursive equations for $P$ cannot be propagated in the absence of measurement data. Consequently, Kalman gains cannot be pre-computed prior to experimental data collection. Details of gain pre-computation in classical Kalman filtering can be found in standard textbooks (e.g. \cite{grewal2001theory}).
\\
\\
We obtain a single estimate of the true hidden state by defining the measurement model, $H$, by concatenating $J$ copies of the row vector $[1 0]$ :
\begin{align}
H & \equiv \begin{bmatrix} 1 0 \hdots 1 0 \hdots 1 0 \end{bmatrix}
\end{align}
Here, the unity values of $H$ pick out and sum the Kalman estimate for the real components of $\state$ while ignoring the imaginary components, namely, we sum $x^{j,1}_{n}$ for all $J$ basis oscillators.
\\
\\
There are two ways to conduct forward prediction for LKFFB and both are numerically equivalent for the choice of basis outlined in Appendices: namely, we set the Kalman gain to zero and recursively propagate using $\Phi$. Alternatively, we define a harmonic sum using the basis frequencies and learned $\{\norm{x^j_n}, \theta_{x^j_n} \}$.  This harmonic sum can be evaluated for all future time to yield forward predictions a single calculation. 

\subsubsection{QKF}

In QKF, we implement a Kalman filter that acts directly on `0' or `1' outcomes. To reiterate the discussion of  \cref{fig:main:Predive_control_Fig_overview_17_one}(a), this means that the measurement action in QKF must be (a) non-linear and (b) receive quantised measurement data. This holds true irrespective of our dynamical model, $\Phi$.  Since we wish to test the performance of a complex measurement action in QKF, we freeze the dynamical model in QKF to be identical to AKF. With unified notation across AKF and QKF, we define a measurement model $h(x)$ and its Jacobian, $H$ as:
\begin{align}
z_n &  \equiv h(f_n) \equiv 0.5\cos(\state_{n}) \\
& \equiv h(x_n[0]) \\
\implies H_n &\equiv \frac{d h(\state_n)}{d\state_n} =  -0.5\sin(\state_{n})
\end{align}
During filtering, QKF applies $h(x)$ to compute the residuals when updating the true Kalman state, $x$. The Jacobian of $h(x)$, $H_n$, is used to propagate the state variance estimate and to compute the Kalman gain. The linearisation of $h(x)$ by $H_n$ holds if errors during the filtering process, including model errors in dynamical propagation, remain small. 
\\
\\
The entity $z$ is associated with an abstract `signal' - a likelihood function for a single qubit measurement in \cref{eqn:main:likelihood}. We note that the bias of a coin flip, namely, $ P(d_n|f_n, \tau, t) \propto z_n$, cannot be measured directly but only inferred in the frequentist sense for a large number of parallel runs, or in the Bayesian sense, by deconstructing the problem further using Bayes rule. In our application, we track  the correlated phase sequence $f$ as our Kalman hidden state, $x$. Subsequently, we extract an estimate of the true bias, $z$, as an unnatural application of the Kalman measurement model.  
\\
\\
The sequence $z$ is not observable, but can only be inferred over a large number of experimental runs. To complete the measurement action, we implement a biased coin flip within the QKF filter given $y$.   While the qubit is naturally quantised, we require a theoretical model, $\mathcal{Q}$, to generate quantised measurement outcomes with statistics that are consistent with Born's rule. If $z$ was a real signal, one could use \cite{karlsson2005,widrow1996} to encode $z$ into a binary sequence. This is a classical linear transformation where one discretises the amplitude of any signal by discretising the probability distribution of the underlying Gaussian errors generated from quantisation of a continuous amplitude value to its nearest allowed level. We modify the procedure in \cite{karlsson2005} to encode $z$ using biased coin flips. Notationally, we represent a black-box quantiser, $\mathcal{Q}$, that gives only a $0$ or a $1$ outcome based on $y_n$:
\begin{align}
d_n &= \mathcal{Q}(y_n)\\
&=  \mathcal{Q}(h(\state_n) + v_n)
\end{align}
\\
\\ 
Our model for projective measurements are biased coin flips where the bias of the coin is stochastically drifting due to $\{ y_n\}$:
\begin{align}
P(d_n | y_n, \state_{n}, \tau) & \equiv \mathcal{B}(d_n=n=1;p= y_n + 0.5 ) \label{eqn:main:qkf:binomial}
\end{align}
We saturate values of $|y_n| \leq 0.5$ and \cref{eqn:main:qkf:binomial} defines a biased coin flip used in QKF. We comment briefly on the statistical description of the action of the coin-flip quantiser below such that the machinery outlined in \cite{karlsson2005} can be applied to the QKF for future analysis. In particular, a binomial distribution parameterised by a random variable $y_n$ means that $\mathcal{Q}$ defines the likelihood of getting a $0$ or a $1$ after marginalising over all possible values of $y_n$.  
\begin{align}
\mathcal{Q}: & P(d_n | \state_{n}, \tau), \quad |y_n| \leq b = 0.5\\
& \equiv  \int (P(y_n | \state_{n}, \tau) * \mathcal{U}(b) ) P(d_n | y_n, \state_{n}, \tau) dy_n \\
\mathcal{U}(b) & \equiv \mathcal{U}(-b, b)
\end{align}
The convolution with a uniform distribution arises from the need to saturate a Gaussian distributed  $y_n$ between allowed values $|y_n| \leq b = 0.5$ for our application such that the resulting probability distribution of $y_n$ retains positivity (see Appendices for details). 
\\
\\
The definitions of $\{ \mathcal{Q}, h(x_n), H_n \}$ in this subsection, and $\{x, \Phi, \Gamma\}$ from AKF completely specify the QKF algorithm for single shot measurement record depicted in \cref{fig:main:Predive_control_Fig_overview_17_one} (a).  
\\
\\
\subsection{Gaussian Process Regression (GPR)}

In GPR, dephasing noise correlations in the measurement record can be learned if one projects data on a distribution of Gaussian processes, $P(\state)$ with an appropriate encoding of their covariance relations via a kernel, $\Sigma_\state^{i,j}$. In a linear measurement regime, let $\state_n$ be the true random phase belonging to the process $\state$ at time step $n$. Our measurement record is corrupted by additive zero mean white Gaussian noise, $v_n$ with scalar covariance strength $R$, yielding scalar noisy observations $y_n$:
\begin{align}
y_n &= \state_n + v_n \\
v_n & \sim \mathcal{N}(0, R) \quad \forall n
\end{align}
Under linear operations, the distribution of measured outcomes, $y$, is also a Gaussian. The  mean and variance of $P(y)$  depends on the mean $\mu_\state$ and variance $\Sigma_\state$ of the prior $P(\state)$, and the mean $\mu_v \equiv 0$ and variance $R$ of the measurement noise, $v_n$: 
\begin{align}
\state & \sim P_\state(\mu_\state,\Sigma_\state ) \\
y & \sim P_y(\mu_\state,\Sigma_\state + R ) 
\end{align}
For covariance stationary $\state$, correlation relationships depend solely on the time lag, $v \equiv \Delta t|n_i - n_j|$ between any two random variables at $t_i, t_j$.  An element of the covariance matrix, $\Sigma_\state^{i,j}$, corresponds to one value of lag, $v$, and the correlation for any given $v$  is specified by the covariance function, $R(v)$:
\begin{align}
\Sigma_\state^{i,j} & \equiv R(v_{i,j}) 
\end{align}
Any unknown parameters in the encoding of correlation relations via $R(v)$ are learned by solving the optimisation problem in \cref{sec:main:Optimisation}. The optimised GPR model is then applied to new datasets corresponding to new realisations of the dephasing process. Let indices $n,m \in N_T \equiv [-N_T, 0]$ denote training points, and $n^*,m^* \in N^* \equiv [-N_T, N_P]$ denote testing (including prediction) points in machine learning language. We now define the joint distribution $P(y,\state^*)$, where $\state^*$ is our prediction for the true process at test points: 
\begin{align}
\begin{bmatrix} \state^* \\y \end{bmatrix} & \sim \mathcal{N} (\begin{bmatrix} \mu_{\state^*} \\ \mu_y
\end{bmatrix} , \begin{bmatrix}   K(N^*,N^*)&K(N_T,N^*) \\ K(N^*,N_T) & K(N_T,N_T) + R \end{bmatrix} )
\end{align}
The additional `kernel' notation $\Sigma_\state  \equiv K(N_T, N_T)$ is ubitiquous in GPR. $K(N_T, N_T)$ depicts an $N_T$ by $N_T$ matrix where the diagonals correspond to $v=0$ and $i, j$-th off-diagonal element correspond to $|i-j|$ lag values. The kernel is calculated for each value of $v$ in the matrix.  we include it to help provide visibility of the time domain set of points over which the covariance function is being calculated. Following \cite{rasmussen2005gaussian}, the moments of the conditional predictive distribution $P(\state^*|y)$ can be derived from the joint distribution $P(y,\state^*)$ via standard Gaussian identities:
\begin{align}
\mu_{\state^*|y} &= \mu_\state + K(N^*,N_T)(K(N_T,N_T) + R )^{-1} (y - \mu_y) \\
\Sigma_{\state^*|y} &= K(N^*,N^*) \nonumber \\
& - K(N^*,N_T)(K(N_T, N_T) + R)^{-1}K(N_T,N^*) 
\end{align}
The above prediction procedure holds true for any choice kernel, $R(v)$. In any GPR implementation, the dataset, $y$, constrains the prior model yielding an aposteriori predictive distribution. The mean of this predictive distribution, $\mu_{\state^*|y}$, are the state predictions for the qubit under dephasing at test points $\in N^*$.
\\
\\
Our choice of a `periodic kernel' in this manuscript encodes a covariance function which is theoretically guaranteed to approximate any zero mean covariance stationary process, $f$, in the mean square limit, namely, by having the same structure as a covariance function for trigonometric polynomials with infinite harmonic terms \cite{solin2014explicit, karlin2012first}. The sine squared exponential kernel represents an infinite basis of oscillators and can be summarised as:
\begin{align}
R(v) &\equiv \sigma^2 \exp (- \frac{2\sin^2(\frac{\omega_0 v}{2})}{l^2}) 
% R(v) &=  \sigma^2 \exp (- \frac{1}{l^2}) \sum_{n = 0}^{\infty} \frac{1}{n!} \frac{\cos^n(\omega_0 v)}{l^{2n}} \\
\end{align} A derivation is provided in Appendices using commentary in \cite{solin2014explicit}.
The sine-squared kernel is summarised by two key hyper-parameters: the frequency comb spacing for our infinite basis of oscillators, $\omega_0$, and a dimensionless length scale, $l$. We use physical sampling considerations to approximate their initial conditions prior to an optimisation procedure, namely, that the longest correlation length encoded in the data, $N \Delta t $, sets the frequency resolution of the comb, and the scale at which changes in $f$ are resolved is of order  $\Delta t$:
\begin{align}
\frac{\omega_0}{2\pi} & \sim  \frac{1}{\Delta t N} \\
l & \sim \Delta t
\end{align} 
We exclude popular kernel choices from analysis. They include the Gaussian kernel (RBF); a scale mixture of Gaussian kernels (RQ); the Matern family of kernels; and a spectral mixture of Gaussian kernels \cite{rasmussen2005gaussian, tobar2015learning}. These exclusions are based on kernel properties as follows. An arbitrary scale mixture of zero mean Gaussian kernels will probe an arbitrary area around zero in the Fourier domain, as schematically depicted in \cref{fig:main:Predive_control_Fig_overview_17_two}(a). While such kernels capture the continuity assumption ubitiquous in machine learning, they are structurally inappropriate in probing a dephasing noise process of an arbitrary power spectral density (e.g. ohmic noise).  Matern kernels of order $q + 1/2$ correspond to a certain class of random process, known as autoregressive processes of order $q$, which are naturally considered under AKF in this manuscript. We do not duplicate our investigations under GPR. A class of GPR methods, namely, spectral mixture kernels and sparse spectrum approximation using GPR have been explored in \cite{wilson2013, quia2010}. However, these techniques require efficient optimisation procedures to learn many unknown kernel parameters, whereas the sine-squared exponential is parameterised only by two hyper-parameters.  Some literature suggests the use of kernel which is the product of the periodic kernel with kernels representing white noise \cite{klenske2016gaussian}. A detailed investigation of the application of spectral mixture and kernel product methods for forward prediction beyond pattern recognition and with limited computational resources, is beyond the scope of this manuscript. 

\section{Noise Engineering and Simulated Measurements\label{sec:main:NoiseEngineering}}

We engineer true environmental dephasing through the procedure described in \cite{soare2014} to enable future experimental verification of simulations reported in this manuscript. We generate $ N = N_T + N_P$ number of points in one sequence of true dephasing noise spaced $\Delta t $ apart in time. We define the discretised process, $\state$, as:
\begin{align}
\state_n &= \alpha \omega_0 \sum_{j=1}^{J} j F(j)\cos(\omega_j n \Delta t + \psi_j) \\
F(j) & = j^{\frac{p}{2}-1} 
\end{align}

Using the notation of \cite{soare2014}, $\alpha$ is an arbitrary scaling factor, $\omega_0$ is the fundamental spacing between true adjacent discrete frequencies, such that $\omega_j = 2 \pi f_0 j =\omega_0 j, j = 1, 2, ...J$. For each frequency component, there exists a uniformly distributed random phase, $\psi_j \in [0, \pi]$. The free parameter $p$ allows one to specify an arbitrary shape of the true power spectral density of $\state$. In particular, the free parameters $\alpha, J, \omega_0, p$ are true dephasing noise parameters which any prediction algorithm cannot know beforehand. With uniformly distributed phase information, it is straightforward to show that $f$ is mean square ergodic and covariance stationary \cite{gelb1974applied}. However, each $n^{th}$ member of the sequence $f$ is Gaussian distributed only by the central limit theorum for large $J$ (in simulations, $J \approx 20$ satisfies Gaussianity, see Appendices). For results in this manuscript, we choose $p=0$ flat top spectrum - this choice of a power spectral density theoretically favors no particular choice of algorithm. For linear regimes, we choose $\alpha$ arbitrarily high relative to machine precision for recursive Kalman calculations, and in non-linear regimes, we use $\alpha$ to rescale $f \in [0, \pi]$ before taking projective measurements. 
\\
\\
While $\{ \alpha, J, \omega_0, p \}$ represents true, unknown environmental dephasing, the choice of $\{N, \Delta t\} $ represents a sampling rate and Fourier resolution set by the experimental protocol. We choose regimes where Nyquist $r \gg 2$.
\\
\\
In generating noisy simulated datasets, we corrupt a noiseless measurement by additive Gaussian white noise. Since $\state$ is Gaussian, the measurement noise level, $NL$ is defined as a ratio between the standard deviation of additive Gaussian measurement noise, $\sqrt{R}$ and the maximal spread of random variables in any realisation $\state$. We approximate this computationally as three sample standard deviations, $\hat{\sigma}_\state$ of one realisation of true $\state$:
\begin{align}
NL = \frac{\sqrt{R}}{3\hat{\sigma}_\state}
\end{align}
To establish a link to GPR notation, $\hat{\sigma}_\state \equiv \sqrt{\hat{\Sigma}_f^{i,i}}$, where the superscript $\hat{}$ denotes sample statistics. This computational procedure enables a consistent application of measurement noise for $f$ from arbitrary, non-Markovian power spectral densities. For the case where binary outcomes are required, we apply a biased coin flip using \cref{eqn:main:qkf:binomial}.

\section{Algorithmic Optimisation \label{sec:main:Optimisation}}

All algorithms in this manuscript employ machine learning principles to auto-tune unknown design parameters. The physical intuition associated with optimising our filters is that we are cycling through a large class of general models for environmental dephasing. This allows each filter to track stochastic qubit dynamics under arbitrary covariance stationary, non-Markovian dephasing. 
\\
\\
In borrowing machine learning principles, we design and solve an optimisation problem to discover unknown KF and GPR design parameters using simulated training datasets. Our approach represents the simplest form of tuning algorithm parameters based on data. Sophisticated, data driven model selection schemes are described for both KF and kernel learning machines (such as GPR) in literature (e.g. \cite{arlot2009data, vu2015understanding}). We chose minimal computational complexity to enable nimble deployment of KF and GPR algorithms in realistic laboratory settings, particularly since LSF optimisation is extremely rapid for our application \cite{mavadia2017}.
\\
\\
For arbitrary power spectral densities of dephasing noise, an optimisation problem posed for Kalman Filters is extremely difficult to solve with standard local optimisers. There are no theoretical bounds on the values of ($\sigma, R$) and consequently, large, flat regions are generated by the Bayes Risk function. Further, the recursive structure of the Kalman filter means that no analytical gradients are accessible for optimising a choice of cost function and a large computational burden is incurred for any optimisation procedure. Beyond standard local gradient and simplex optimisers, we consider coordinate ascent \cite{abbeel2005} and particle swarm optimisation techniques \cite{robertson2017particle} as promising, nascent candidates and their application remains an open research question. 
\\  
\\
For optimisation of KF filters in this manuscript, we randomly distribute $\{(\sigma_{k}, R_{k}), k=1, \hdots K \}$ pairs over several orders of magnitudes in two dimensions:
\begin{align}
\sigma_k, R_k &\equiv \alpha_0 10^{\alpha_1} \\
\alpha_0 & \sim U[0, 1]\\
\alpha_1 & \sim U[\{ -p_{max}, -p_{max} + 1,  \hdots,  p_{min}\}]
\end{align}
Scale magnitudes are set by $\alpha_1$, a random integer chosen with uniform probability over $\{ -p_{max}, \hdots, p_{min} \}$ where we set $p_{min} = 3, p_{max} = 8$  such that $p_{max}$ is higher than machine floating point precision $\approx 10^{-11}$. Uniformly distributed floating points for $\sigma_k, R_k $ in each order of magnitude is set by $\alpha_0$. 
\\
\\
We generate a sequence of loss values $\{L(\sigma_k, R_k), k = 1, \hdots K\}$:
\begin{align}
L(\sigma_k, R_k) \equiv  \sum_{n=1}^{N'} L_{BR}(n | I= \{\sigma_k, R_k \})
\end{align}
$L_{BR}(n | I= \{\sigma_k, R_k \})$ is given by \cref{eqn:main:sec:ap_opt_LossBR}. Time horizons for state estimation ($N' = |N_{SE}| , N_{SE} \in  [-N_{T}, 0]$) or prediction ($N' = N_{PR}, N_{PR}  \in [0, N_{P}]$) are chosen such that the sequence $\{L(\sigma_k, R_k) \}$ defines sensible shapes of the loss function over parameter space and the numerical experiments in this manuscript. As illustrative guidelines, a choice of small $N_{SE}$ values ensures that we assess state estimates only once Kalman Filters are approaching convergence. Meanwhile, large $N_{PR}$ values will flatten the true prediction loss function as long term prediction errors will dominate and obscure low loss values for short term prediction horizons of interest, namely, small $n>0$. One may incorporate a different prior over $I= \{\sigma_k, R_k \}$ and/or construct optimisation problem over different choices of cost function (e.g. maximum likelihood). Our approach is computationally efficient given the recursive nature of the Kalman filter and a quantitative review of Bayesian hyper-parameter optimisation procedures is beyond the scope of this manuscript. 
\\
\\
We accept an optimal candidate ($\sigma^*, R^*$) which minimises the Bayes state estimation risk over $K$ trials by comparing the low loss regions for state estimation and prediction. We define a low loss regions for state estimation and prediction as being the set $ \{ (\sigma_k, R_k) : L(\sigma_k, R_k) < 0.1 L_0 \}$ where the $10 \%$ low loss threshold is defined relative to the median risk, $L_0 \equiv \text{median}\{  L(\sigma_k, R_k) \}$, incurred during the optimisation procedure over $K$ trials. If the low loss region in state estimation has an overlap with low loss regions during prediction in parameter space, and optimal ($\sigma^*, R^*$) candidate falls within this overlap region, then we accept that the KF filter is sensibly tuned. If an overlap of low loss regions for state estimation and prediction does not exist, or if the optimal candidate does not reside in the overlap region, then the optimisation problem is deemed `broken' as training is uncorrelated with prediction performance. 
\\
\\
The GPR optimisation procedure is not the same as for Kalman Filtering. In GPR, no recursion exists and analytic gradients are accessible to simplify the overall optimisation problem. Instead of minimising Bayes state estimation risk, we follow a popular practice of maximising the Bayesian likelihood. The set of parameters in GPR, $I = \{\sigma, R, p, l \}$ require optimisation. We use physical arguments in \cref{sec:main:OverviewofPredictive Methodologies} to provide initial conditions and/or to constrain the optimisation of $\{ p, l\}$. 

\section{Algorithm Performance Characterisation \label{sec:main:Performance}}

In the results to follow, our metric for characterising performance of optimally tuned algorithms will be the normalised Bayes prediction risk:
\begin{align}
\normpr \equiv \frac{L_{BR}(n|I)}{\langle \state_n^2 \rangle_{f, \mathcal{D}}} 
\end{align}
A desirable forward prediction horizon corresponds to maximal $n \in [0, N_P]$ for which normalised Bayes prediction risk at all time steps $n' \leq n$ is less than unity. We compare the difference in maximal forward prediction horizons between algorithms in context of realistic operating scenarios.
\subsection{KF (Linear Measurement)}
\begin{figure}
    \includegraphics[scale=1.0]{fig_data_all}
    \caption{\label{fig:main:fig_data_all} We plot state predictions against time steps $n > -50$ obtained from optimised AKF, LKFFB and LSF algorithms for $K=75$ trials. We plot true $f$ [black] and measurement data [grey dots], where measurements for $n \in [-N_T, -50]$ are omitted [(left)]. A single run contributes to Bayes prediction risk over an ensemble of $M=50$ runs normalised against predicting the mean, $\mu_\state$, of dephasing noise [right]. A normalised risk $<1$ for $n > 0$ defines a desirable forward prediction horizon. A single run phase sequence $f$ is drawn from a flat top spectrum with $J$ true Fourier components spaced $\omega_0$ apart and uniformly randomised phases $\in [0, 2\pi]$. A trained LKFFB is implemented with comb spacing $\omega_0^B / 2\pi = 0.5$ Hz and $J^B =100$ oscillators; while trained AKF / LSF models correspond to high $q = 100$. Relative to LKFFB,  (a) and (b) correspond to perfect projection $\omega_0 / \omega_0^B  \in Z $ for $J= 40, \omega_0 / 2\pi = 0.5$ Hz. In (c) and (d), we simulate realistic noise with $\omega_0 / \omega_0^B  \notin Z$, $J = 45000$, $\omega_0 / 2\pi = \frac{8}{9} \times 10^{-3}$ Hz such that $>500$ number of true components fall between adjacent LKFFB oscillators. For (a)-(d), $N_T = 2000, N_P = 100$ steps, $\Delta t = 0.001s$ such that we fulfill $r_{Nqy} \gg 2$, $N_T / \Delta t < \omega_0/2\pi$. Measurement noise level $ NL= 10\%$.}
\end{figure} 

\subsubsection{General predictive performance}

\cref{fig:main:fig_data_all} depicts predictive performance of LSF, AKF and LKFFB algorithms using a linear measurement record. In (a), we depict a single time domain run. We engineer a case when perfect projection of the true state on the LKFFB basis is \textit{theoretically} achievable. In (b), we plot $\normpr$ prediction risk such that one run in the ensemble is depicted in (a).  In (a) and (b), we find that LKFFB learns all information about the dephasing noise and qubit state dynamics are nearly perfectly predictable when perfect projection is theoretically enabled. Meanwhile, AKF and LSF share autoregressive coefficients and therefore, both algorithms have nearly identical $\normpr$ prediction risk trajectories. Both AKF and LSF cannot extract all information from dephasing noise.
\\
\\
We relax perfect projection relative to LKFFB basis in \cref{fig:main:fig_data_all}  (c) and (d). We engineer $f$ such any true spectral component in $f$ can never be perfectly projected on an LKFFB basis oscillator. Further, we enforce that  computational resolution for any practical application will be limited, namely, $J \gg J^B$ number of LKFFB basis oscillators. Meanwhile, no explicit basis considerations apply to AKF/LSF. A single run is plotted in (c), and $\normpr$ prediction risk is plotted in (d). We compare results from LKFFB and AKF and observe that autoregressive dynamics of LSF / AKF enable a larger forward prediction horizon than LKFFB. 
\\
\\
\begin{figure}
    \includegraphics[scale=1.0]{fig_data_maxfwdpred}
    \caption{\label{fig:main:fig_data_maxfwdpred} From (a)-(c), we plot $\normpr$ against forward time $n \in [0, N_P]$ for LSF, AKF and LKFFB. In each panel, we vary true $f$ cutoff relative to an apriori noise bandwidth assumption $f_B$ such that $\omega_0 / 2\pi = 0.5$ Hz, $J = 20, 40, 60, 80, 200$. We depict the maximal forward prediction horizon for each case using vertical lines at approximately $ n_{max} \mid  \normpr \lesssim 0.8 < 1$, where a threshold less than unity is chosen to reduce artifacts arising from Bayes risk oscillations around mean behaviour. For LKFFB, $\omega_0^B / 2\pi = 0.497$ Hz for $j \in J^B = 100$ oscillators. For LSF and AKF, $q = 100$. In all cases,  $N_T = 2000, N_P = 50$ steps, $\Delta t = 0.001s, r_{Nqy}=20$, with optimisation performed for $M=50$ runs, $K=75$ trials and measurement noise level $NL = 1\%$.} 
\end{figure} 
In \cref{fig:main:fig_data_maxfwdpred}, we plot $\normpr$ prediction risk against an increasing ratio of $Jf_0 / f_B$ where $f_B$ is the true dephasing noise bandwidth assumption in specifying the basis for LKFFB. We engineer $f$ with a power spectral density comparable to \cref{fig:main:fig_data_all} (a)-(b) but mildly detune the LKFFB basis to engineer imperfect projection. We confirm that as oversampling is reduced, the absolute forward prediction horizon shrinks, namely, $\normpr > 1 $ for increasing small $n>0$. The forward prediction horizon is approximately quantified using vertical lines. We confirm that absolute prediction horizons for any algorithm are arbitrary in the sense that they can be increased via increased oversampling. We restrict our analysis to comparative statements between algorithms for future results. 

\subsubsection{Power spectral density extraction}

We plot learned Fourier domain information associated with \cref{fig:main:fig_data_maxfwdpred} in \cref{fig:main:fig_data_specrecon}. For LKFFB, we plot the learned instantaneous amplitudes from a single run [blue dots] against the true dephasing noise power spectral density [black]. 
\\
\\
For AKF/LSF, we extract optimised algorithm parameters to calculate the spectrum using \cref{eqn:main:ap_ssp_ar_spectden} [red dots]. Under the assertion that LSF implements an AR($q$) process, the set of trained parameters, $\{  \{\phi_{q' \leq q}\}, \sigma^2\}$ from LSF and AKF allows us to derive experimentally measurable quantities, including the power spectral density of the dephasing process \cite{brockwell1996introduction}:
\begin{align}
S(\omega) & = \frac{\sigma^2}{2 \pi }\frac{1}{|\Phi(e^{-i\omega})|^2} \label{eqn:main:ap_ssp_ar_spectden} 
\end{align}
Here, we use the same summarised notation, $\Phi(L)$, but  $L$ is no longer the time domain lag operator and has been redefined as $L \equiv e^{-i\omega}$ in the Fourier domain. In all cases except LKFFB in \cref{fig:main:fig_data_specrecon} (d), all algorithms correctly discern the cut-off frequency of true dephasing. The reconstruction enabled by \cref{eqn:main:ap_ssp_ar_spectden} provides additional numerical evidence to validate our assertion. 
\\
\\
\begin{figure}
    \includegraphics[scale=1.0]{fig_data_specrecon}
    \caption{\label{fig:main:fig_data_specrecon} We compare the true power spectrum for $f$ with derived spectral estimates from LKFFB and AKF. From (a)-(d), we vary true $f$ cutoff relative to an apriori noise bandwidth assumption $f_B$ such that $\omega_0 / 2\pi = 0.5$ Hz, $J = 20, 40, 80, 200$. For LKFFB, we use learned amplitude information from a single run ($\propto ||x^j_n||^2 $) with $\omega_0^B / 2\pi = 0.497$ Hz for $j \in J^B = 100$ oscillators. For AKF, we plot \cref{eqn:main:ap_ssp_ar_spectden} using optimally trained $\{\phi_{q' \leq q}\}$ and $\sigma^2$, with order $q = 100$. The zeroth Fourier component and its estimates are omitted to allow for log scaling; and $N_T = 2000, N_P = 50$ steps, $\Delta t = 0.001s, r_{Nqy}=20$, with optimisation performed for $M=50$ runs, $K=75$ trials and measurement noise level $NL = 1\%$.} 
\end{figure} 
Further, we compare LKFFB and AKF/LSF in extracting power spectral density information for true dephasing. We find that spectrum reconstruction from LKFFB is of higher fidelity by several orders of magnitude compared to AKF/LSF, even when learning environments are imperfect. This is true for \cref{fig:main:fig_data_specrecon} (a)-(c) but fails if the true noise bandwidth assumption underpinning all analysis is relaxed, as in \cref{fig:main:fig_data_specrecon} (d). The discrepancy between AKF/LSF spectrum reconstruction and the truth depends on the accuracy of scaling factor given by optimally tuned $\sigma$ in AKF and the spectral content learned via $\{\phi_{q' \leq q}\}$ in LSF in \cref{eqn:main:ap_ssp_ar_spectden}. In contrast, instantaneous amplitudes are tracked in one run of LKFFB and are less susceptible to optimisation over model parameters.

    \begin{figure*} 
    \includegraphics[scale=1.0]{figure_lkffb_path}
    \caption{\label{fig:main:figure_lkffb_path} 
    We compare LKFFB and AKF performance when a true phase sequence $f$ is generated from a flat top spectrum in (a)-(d) by varying  $\omega_0 / 2\pi = 0.5, 0.499, \frac{8}{9} \times 10^{-3}, \frac{8}{9} \times 10^{-3}$ Hz and $J = 80, 80, 45000, 80000$ respectively. For (a)-(d), we depict normalised Bayes prediction risk for LKFFB, AKF, and LSF against time steps $n>0$. For LKFFB, these regimes correspond to perfect learning in (a); imperfect projection on basis in (b); finite computational Fourier resolution in (c); and a relaxed bandwidth assumption ($f_B < \omega_0 / 2\pi$) in (d). In the panels (e)-(l), we depict optimisation of Kalman noise parameters ($\sigma^2, R$) for LKFFB [top row] and AKF [bottom row] for the four regimes in (a)-(d). Low loss regions represent risk values $< 10\%$ of $L_0$, the median risk incurred during Kalman hyperparameter optimisation for $K=75$ trials of of randomised ($\sigma^2, R$) pairs. Optimal ($\sigma^*, R^*$) minimise state estimation risk. For each trial, a risk point is an expectation over $M=50$ runs of true $f$ and noisy datasets during state estimation ($n \in  [-N_{SE}, 0]$) or prediction ($n \in  [0, N_{PR}]$). We choose $ N_{PR}=N_{SE}=50$ such that the shape of total loss over time steps form sensible optimsation problems and a scan of $N_{PR}, N_{SE}$ values do not appear to simplify our Kalman optimisation problem. We plot optimisation results for LKFFB in (e)-(h) and AKF in (i)-(l). A KF filter is `tuned' if optimal ($\sigma^*, R^*$) lies in the overlap of low loss regions for state estimation and prediction. This condition is violated in (h). KF algorithms are set up with $q = 100$ for AKF; $J^B = 100, \omega_0^B / 2\pi = 0.5$ Hz for LKFFB, with $N_T = 2000, N_P = 100$ steps, $\Delta t = 0.001s, r_{Nqy}=20$ and applied measurement noise level $ NL = 1\%$.}  
    \end{figure*} 


\subsubsection{Model robustness}
We compare the model robustness of LKFFB and AKF in realistic operating environments. In \cref{fig:main:figure_lkffb_path}, these experiments correspond to (a) perfect learning in LKFFB; (b) imperfect projection relative to LKFFB basis; (c) imperfect projection in (b) combined with finite algorithm resolution; and (d), where case (c)  is extended to an ill-specified basis relative to true noise bandwidth. We find that LKFFB performance deteriorates relative to AKF / LSF as pathologies are introduced in \cref{fig:main:figure_lkffb_path} (a)-(d). 
\\
\\
We expose the underlying optimisation results for choosing an optimal $(\sigma^*, R^*)$ for LKFFB in \cref{fig:main:figure_lkffb_path} (e)-(h) and for AKF in \cref{fig:main:figure_lkffb_path} (i)-(l). The overlap area of low loss choices between state estimation (blue) and prediction (purple) Bayes Risk shrinks for LKFFB in \cref{fig:main:figure_lkffb_path} (e)-(g), and regions are disjoint in (h), indicating that training has diminishing returns for LKFFB predictive performance as the algorithm breaks. In contrast, overlap of low loss Bayes Risk regions do not change for AKF across \cref{fig:main:figure_lkffb_path} (i)-(l).

\begin{figure}
    \includegraphics[scale=1.]{fig_data_akfvlsf}
    \caption{\label{fig:main:fig_data_akfvlsf} (a) We plot the ratio of $\normpr$ prediction risk from AKF to LSF against time steps $n>0$.  AKF and LSF share identical $\{ \phi_q \}$ and  a value below $<1$ indicates AKF outperforms LSF. In (i)-(iv), applied measurement noise level is increased from $0.1 - 25 \%$. (b) We plot normalised Bayes Risk against time steps $n>0$ for AKF and LKFFB corresponding to cases (i) -(iv) and confirm a desirable forward prediction horizon underpins ratios in (a). True $f$ is drawn from a flat top spectrum with $\omega_0 / 2\pi = \frac{8}{9} \times 10^{-3}$ Hz, $J = 45000$, $N_T = 2000, N_P = 100$ steps, $\Delta t = 0.001s, r_{Nqy}=20$ such that \cref{fig:main:figure_lkffb_path}(c) corresponds to case (ii) in this figure. Optimisation is performed for $M=50$ runs, $K=75$ trials.}
\end{figure}

\subsubsection{Measurement noise filtering}
Since the AKF algorithm recasts an AR($q$) process from LSF into Kalman form, we confirm that the KF framework enables additional measurement noise filtering for qubit dynamics than LSF alone. In \cref{fig:main:fig_data_akfvlsf} (a), we plot $\normpr$ prediction risk for AKF and LSF as a ratio such that a value greater than unity implies LSF outperforms AKF:
\begin{align}
AKF / LSF \equiv \frac{\normpr ^{AKF}}{\normpr^{LSF}}, \quad n \in [0, N_P]
\end{align}
In cases (i)-(iv), we increase the applied measurement noise level to our noisy datasets $\{ y_n \}$. For the low measurement noise $NL = 0.1\%$ in (i), the ratio $AKF/LSF > 1$ and LSF outperforms AKF. For applied measurement noise level $NL > 1\%$ in (ii)-(iv), we find that $AKF/LSF <1 $ and AKF outperforms LSF in numerical simulations. In \cref{fig:main:fig_data_akfvlsf} (b), we plot $\normpr$ prediction risk for each measurement noise level (i)-(iv). The plots in (b) confirm that all ratios reported in (a) correspond to a desirable forward prediction horizon where both AKF and LSF outperform predicting the mean value of dephasing. 

\subsection{KF (Non Linear, Quantised Measurements)}
We use QKF to test if a Kalman Framework can incorporate single shot qubit outcomes for predictive estimation. To re-iterate, QKF estimates and tracks hidden phase information, $f$, using the Kalman true state $x$, and the associated probability for a projective qubit measurement outcome, $\propto z$ is not inferred or measured directly but given deterministically by Born's rule encoded in the non-linear measurement model, $z = h(x)$. The measurement action is completed by performing a biased coin flip, where $z$ determines the bias of the coin.  The $\text{N.} \langle (z_n - \hat{z}_n)^2 \rangle_{f, \mathcal{D}} $ risk in  \cref{fig:main:fig_data_qkf2} is calculated with respect to  $z$, instead of the stochastic phase sequence $f$, as the relevant quantity parametering qubit state evolution. We investigate if $\text{N.} \langle (z_n - \hat{z}_n)^2 \rangle_{f, \mathcal{D}} < 1, n\in [0, N_P] $ can be achieved for numerical experiments considered previously in the linear regime. In particular, we generate true $f$ defined in numerical experiments in \cref{fig:main:fig_data_maxfwdpred,fig:main:fig_data_specrecon} for $q=100$ such that $f_0 J / f^B = 0.2, 0.4, 0.6, 0.8$.  
\\
\\
\begin{figure}[h!]
    \includegraphics[scale=1.]{fig_data_qkf}
    \caption{\label{fig:main:fig_data_qkf2} We plot Bayes prediction risk for QKF against time steps $n>0$. In (a)-(b), we vary true $f$ cutoff relative to an apriori noise bandwidth assumption such that $J f_0 / f_B = 0.2, 0.4, 0.6, 0.8$ for an initially generated true $f$ in \cref{fig:main:fig_data_specrecon} with $\omega_0/ 2\pi = 0.497 $ Hz, $J = 20, 40, 60, 80$. Measurement noise is incurred on $f$ at $NL = 1 \%$ for the linear measurement record and on $z$ at $NL = 1\%$ level corresponding to the non-linear measurement record. In (a), we obtain $\{\phi_{q' \leq q}\}, q=100$ coefficients from AKF/LSF acting on a linear measurement record generated from true $f$. We re-generate a new truth, $f'$, from an autoregressive process by setting $\{\phi_{q'\leq q}\}, q=100$ as true coefficients and by defining a known, true $\sigma$. We generate quantised measurements from $f'$ and data is corrupted by measurement noise of a true, known strength $R$. Hence, QKF in (a) incorporates true dynamics and noise parameters $\{\{\phi_{q' \leq q} \}, \sigma, R\}$ but acts on single shot qubit measurements. In (b), we use $\{\phi_{q' \leq q} \}, q=100$ coefficients from (a) but we generate quantised measurements from the original, true $f$. We auto-tune QKF noise design parameters in a focused region ($\sigma_{AKF}^* \leq \sigma_{QKF}$, $R_{AKF}^* \leq R_{QKF}$) with with $M=50$ runs, $K=75$ trials. For (a)-(b), forward prediction horizons are shown with $N_T = 2000, N_P = 50$ steps, $\Delta t = 0.001s, r_{Nqy}\gg 2$.}
\end{figure}
\cref{fig:main:fig_data_qkf2} (a) isolates the performance of the measurement action by specifying a true dynamical model and true Kalman noise parameters. To specify true dynamics, we approximate $f$ by $f'$, where $f'$ is generated from a sequence of $\{ \phi_{q'\leq q}\}$ obtained from LSF acting on $f$ in the linear regime considered previously. By using $f'$, QKF incorporates a high $q$ true autoregressive dynamical model $\{ \phi_{q'\leq q}\}$. We generate single shot qubit measurements based on $f'$ and we input true noise parameters $(\sigma, R)$. \cref{fig:main:fig_data_qkf2} (a) depicts that a desirable forward prediction horizon $\text{N.} \langle (z_n - \hat{z}_n)^2 \rangle_{f, \mathcal{D}} < 1, n\in [0, N_P] $ is achieved for sufficiently oversampled regime, and the forward prediction horizon shrinks in $n$ when oversampling is reduced. As in the linear case, the absolute forward prediction horizon is arbitrary relative to $f_0 J / f^B$ and implicitly, an optimisation over the choice of $q$ in our application. 
\\
\\
\cref{fig:main:fig_data_qkf2} (b) depicts QKF performance for a realistic learning procedure. We generate single shot qubit measurements based on the true dephasing $f$. QKF incorporates a learned dynamical model from AKF in the linear regime and we tune $(\sigma, R)$ for QKF. In particular, we explore $\sigma \geq \sigma_{AKF}^*$ to incorporate model errors as $\{\phi_{q' \leq q}\}$ were learned in the linear regime.  We explore $R \geq R_{AKF}^*$ to incorporate increased measurement noise as QKF receives raw data that has not been pre-processed or low pass filtered. The underlying optimisation problems are well behaved for all cases in \cref{fig:main:fig_data_qkf2}(b) [not shown]. As oversampling is reduced, the QKF forward prediction horizon disappears rapidly i.e $\text{N.} \langle (z_n - \hat{z}_n)^2 \rangle_{f, \mathcal{D}} > 1 $ prediction risk for all $n>0$.  However, we confirm that in a highly oversampled regime, it is possible for the QKF to achieve a forward prediction horizon that is slightly better than predicting the mean value for dephasing noise. 

\subsection{GPR} 
In this section, we use a trained GPR model to track a deterministic sine curve with a single Fourier component as our `true' state trajectory, instead of a stochastic qubit dynamics described earlier. 

\begin{figure}
    \includegraphics[scale=1.]{fig_data_gpr}. 
    \caption{\label{fig:main:fig_data_gpr} In (a)-(d), prediction points $\mu_{\state^*|y}$ [purple] are plotted against time steps, $n$. We plot the true phase sequence,  $f$, [black] and  $f$ at the beginning of the run [red dotted]. Predictions are generated in a single run by a trained GPR model with a periodic kernel corresponding to a Fourier domain basis comb spacing, $\omega_0^B$. Data collection of $N_T$ measurements [not shown] ceases at $n=0$. For simplicity, the true $f$ is a deterministic sine with frequency, $\omega_0$. (a) Perfection projection is possible $\omega_0 / \omega_0^B \in Z$ natural numbers, $\omega_0 = 3$ Hz. Kernel resolution is exactly the longest time domain correlation in dataset, $2 \pi / \omega_0^B \equiv \Delta t N_T \implies \kappa = 0$.   (b) Imperfect projection, with $\omega_0 / \omega_0^B \notin Z$, $\omega_0 / 2 \pi = 3 \frac{1}{3}$ Hz, $\kappa=0$. (c) We increase kernel resolution to be arbitrarily high, $\kappa \gg 0 $, such that $\omega_0 / \omega_0^B \gg 0 \notin Z $ for original $ \omega_0 / 2 \pi = 3$ Hz. (d) We test (b) and (c) for $\kappa \gg0$, $ \omega_0 / \omega_0^B \notin Z$, $\omega_0 / 2 \pi = 3 \frac{1}{3}$ Hz. For all (a)-(d), $N_T = 2000, N_P = 150$ steps, $\Delta t = 0.001s$ and applied measurement noise level $1\%$.} 
\end{figure}
For this simple example, the periodic kernel learns Fourier information in the measurement record enabling interpolation using test-points $n^* \in [-N_T, 0]$ for all cases (a)-(d) in \cref{fig:main:fig_data_gpr}. Time domain predictions $n^* >0$  in (a) appear sensible when perfect learning is possible given the theoretical structure of the simulation. When learning is imperfect in (b)-(d), GPR predictions for the region  $n^* \in [0, N_P]$ show a pronounced discontinuity at a deterministic quantity, $\kappa$.  We increase the kernel resolution in \cref{fig:main:fig_data_gpr} (c) and (d) and test whether prediction artifacts in (b) can be reduced by using a fine Fourier comb in the periodic kernel. This is not the case in \cref{fig:main:fig_data_gpr} (c) and (d) and the algorithm sinks to zero for long regions before reviving discontinuously at $\kappa$. For all cases, we compare GPR predictions for $n^*>0$ with the true state dynamics at the start of the training run [red dotted] and we find good agreement. 

\section{Discussion} \label{sec:main:discussion}


Our studies in  \cref{fig:main:fig_data_all,fig:main:fig_data_specrecon,fig:main:figure_lkffb_path} revealed that autoregressive approaches to modeling stochastic dynamics via joint LSF / AKF implementation led to model robust forward predictions of the qubit state under dephasing. In contrast, oscillator based approaches in LKFFB and GPR (with a periodic kernel) were outperformed for yielding time domain predictions in realistic (imperfect) learning scenarios. We discuss the loss in performance for LKFFB and GPR below. 
\\
\\
In investigating the loss of performance for LKFFB, we find that the efficacy of our LKFFB approach depends on a careful choice of a \textit{probe} (i.e. a fixed computational basis) for the dephasing noise. \cref{fig:main:figure_lkffb_path} reveals that the largest loss in time domain predictive performance for LKFFB arises from imperfect projection onto its basis of oscillators in \cref{fig:main:figure_lkffb_path}(b). In the imperfect projection regime of \cref{fig:main:fig_data_maxfwdpred} and identically, \cref{fig:main:fig_data_specrecon}, LKFFB reconstructs Fourier domain information to a high fidelity across a range of sampling regimes in \cref{fig:main:fig_data_specrecon} (a)-(c) but LKFFB is outperformed by AKF in the time domain in \cref{fig:main:fig_data_maxfwdpred}. Since LKFFB tracks instantaneous amplitude and phase information explicitly for each basis oscillator, the loss of LKFFB time domain predictive performance must accrue from difficulty in tracking instantaneous phase, not amplitude, information. 
\\
\\
One could anticipate improved LKFFB performance on real dephasing noise in a laboratory, as opposed to simulations, where some projection on the LKFFB basis is guaranteed as a real noise power spectrum is continuous in the Fourier domain.  In simulated noise traces, dephasing noise is necessarily discrete in the Fourier domain and the imperfect projection regime is unnecessarily severe.
\\
\\
While difficulty of instantaneous phase estimation is likely to disadvantage time domain predictive performance of LKFFB, we note that an oscillator approach yielded high fidelity reconstructions of true noise power spectral density. These reconstructions are robust against imperfect projection on the LKFFB oscillator basis even as oversampling is reduced. This suggests that an application of LKFFB outside of predictive estimation could be tested against standard spectral estimation techniques in future work.
\\
\\
In GPR, we wanted to exploit the equivalence of amplitude and phase information in an infinite collection of oscillators summarised by a \textit{periodic} kernel to enable qubit prediction. Our investigations reveal that predictions with a periodic kernel are useful for interpolation but have limited meaning for forward predictions for time steps $n >0$.  We find that a fundamental period is set by the comb spacing in the kernel and we expect that learned Fourier information will repeat in the time domain deterministically at the fundamental period, namely, $\kappa$ in all cases depicted in \cref{fig:main:fig_data_gpr}. When learning is perfect, a repeated pattern can be interpreted as qubit state predictions and no discontinuities are seen in forward predictions. When learning is imperfect, GPR  with a periodic kernel is able to learn Fourier amplitudes to provide good state estimates for $n<0$ but one cannot interpret state predictions for $n>0$ without a formal procedure for actively tracking and correcting phase information for each individual basis oscillator at $n= \kappa$. Since phase information can be recast as amplitude information for any oscillator, one expects that forward predictions can be improved by reducing the comb spacing for an infinite basis of oscillators in the periodic kernel.  We find that this is not the case - an increase in kernel resolution means that we are probing time domain correlations longer than the physical time spanned by the measurement record. As such, the GPR algorithm predicts zero for $n \in [0, \kappa], \kappa > 0$, before reviving at $\kappa$.  In fact, if prediction test points were not specified beyond $\kappa$, then a flat region in (c) or (d) may be misinterpreted as predicting zero mean noise rather than a numerical artifact.
\\
\\
Having discussed oscillator based approaches, we return to autoregressive models and examine the performance of AKF and LSF in high measurement noise regimes. We expect that a Kalman framework enables increased measurement noise filtering compared to LSF alone if dynamics are identically specified.  Our study reveals that the Kalman procedure outperforms LSF alone for applied measurement noise levels $NL \geq 1\%$. There are two theoretical reasons for why this is the case: measurement noise filtering is enabled in the Kalman framework through the optimisation procedure for $R$ and has a regularising (smoothening) effect. Secondly, an imperfectly learned dynamical model $\Phi$ is optimised through the tuning of $\sigma$. The joint optimisation procedure over $(\sigma, R)$ ensures that the relative strength of noise parameters is also optimised.
\\
\\
In QKF, we wish to use single shot qubit data while enabling model-robust qubit state tracking and increased measurement noise filtering via AKF. \cref{fig:main:fig_data_qkf2} reveals that the QKF is vulnerable to the build of errors for arbitrary applications and we provide three explanatory remarks from a theoretical perspective. Firstly, the Kalman gains are recursively calculated using a set of \text{linear} equations of motion which incorporate the Jacobian $H_n$ of $h(x_n)$ at each $n$. All non-linear Kalman filters perform well if errors during filtering remain small such that the linearisation assumption holds at all time steps. Secondly, measurements are quantised and hence residuals must be one of $\{-1, 0, 1 \}$ rather than floating point numbers.  In our case, the Kalman update to $x_n$ at $n$, mediated by the Kalman gain cannot benefit from a gradual reduction in residuals. A third effect incorporates consequences of both quantised residuals and a non-linear measurement action. In simple, linear Kalman filtering, Kalman gains can be pre-calculated in advance of any measurement data. Namely, the recursion of Kalman state variances $P$, can be decoupled from the recursion of Kalman state means, $x$ \cite{grewal2001theory}. The former governs computation of the gains, and the latter depends residuals computed from measurement records. In our application, quantised residuals affect the Kalman update of $x$, and further, they affect the recursion for the Kalman gain via the state dependent Jacobian, $H_n$. These three effects means that QKF is extremely sensitive to a rapid build of errors during the filtering process such that meaningful predictive estimation is no longer possible.
\\
\\
In this context, we demonstrate numerically that the QKF achieves a desirable forward prediction horizon when build of errors during filtering are minimised, for example, by specifying Kalman state dynamics and noise strengths perfectly; and/or by severely oversampling relative to the true $f$.   In \cref{fig:main:fig_data_qkf2}(a) we demonstrate that the forward prediction horizon can be tuned relative to the oversampling ratio $f_0J / f^B$ if true state dynamics and noise strengths are incorporated into QKF, suggesting robustness of the measurement model design.  If dynamics and noise strengths are unknown but learned by machine learning procedures, then QKF achieves a forward prediction horizon only in a highly oversampled regime. It is possible that QKF forward prediction horizons in realistic learning environments can be improved by solving the full $q+2$ optimisation problem for $\{\{ \phi_{q' \leq q}\}, \sigma, R\}$, rather than the approach taken in this manuscript. However, a solution to the full optimisation problem is beyond the scope of our analysis. At present, we interpret QKF as demonstration that one may track stochastic qubit dynamics using single shot measurements under a Kalman framework.

\section{Conclusion \label{sec:main:Conclusion}}

We considered a sequence of projective measurements obtained from a single qubit under non-Markovian environmental dephasing. Predictive estimation algorithms in this manuscript learn noise correlations in the data and forecast the qubit state beyond the measurement record. To address the absence of a theoretical model describing stochastic qubit dynamics, we use high order autoregressive processes and a collection of oscillators to learn qubit state dynamics under arbitrary dephasing. 
\\
\\
To accommodate stochastic dynamics under arbitrary dephasing, we choose two Bayesian learning protocols - Gaussian Process Regression (GPR) and Kalman Filtering (KF).  All Kalman algorithms predict the qubit state in forward time better than predicting mean qubit behaviour under dephasing.  Forward prediction horizons can be arbitrarily increased for all Kalman algorithms by oversampling true dephasing noise.  In contrast, under GPR, we encode dynamics using an infinite basis of oscillators and we find numerical evidence that this approach enables interpolation but not forward predictions beyond the measurement record.  
\\
\\
We analyse two measurement models under the Kalman framework - a linear regime, where qubit outcomes and pre-processed, and a non-linear regime, where algorithms act on raw projective qubit measurements. In the linear regime, a key insight is that Kalman-based autoregressive approaches exhibit model-robust dynamical tracking of qubits compared to Kalman-based oscillator approaches. We confirm autoregressive Kalman filters enable increased measurement noise filtering compared to autoregressive, least squares procedures in \cite{mavadia2017} alone. Subsequently, we implement a non-linear, coin-flip measurement model in an autoregressive Kalman filter to demonstrate qubit state tracking and prediction can occur using only 0 or 1 qubit measurements. 
\\
\\
There are exciting opportunities for machine learning algorithms to increase our understanding of dynamically evolving quantum systems in real time using  projective measurement records. Quantum systems coupled to classical spatially or temporally varying fields provide opportunities for classical algorithms to analyse correlation information and enable predictive control of qubits. Moving beyond a single qubit, we anticipate that measurement records will grow in complexity allowing us to exploit the natural scalability offered by machine learning for mining large datasets. In realistic laboratory environments, the success of algorithmic approaches will be contingent on robust and computationally efficient algorithmic optimisation procedures. The pursuit of these opportunities is a subject of ongoing research.

\section{Acknowledgments}
 The LSF filter is written by V. Frey and S. Mavadia \cite{mavadia2017}. The GPR framework is implemented and optimised using standard protocols in GPy \cite{gpy2014}. Authors thank C. Grenade, K. Das, V. Frey, S. Mavadia, H. Ball, C. Ferrie and T. Scholten for useful comments. 
 
\section{Version Control \label{sec:main:versioncontrol}}
Notes: NOTES-v0-1 | Notes-2017-Main-v7
\\
Data: Fig v5 (resized, with maxpredfwd)
