\section{Kernel-Learning and Spectral Methods in GPR} \label{sec:ap_gpr_kernels}

A standard review of kernel techniques in GPR is provided in \cite{rasmussen2006gaussian}. Here, we provide analytical and pragmatic grounds for excluding certain choices of kernels from our detailed study. We use kernel definitions in \cite{rasmussen2006gaussian}, \cite{wilson2013gaussian}, and the correction published by the authors for \cite{wilson2013gaussian}, and recast them in our notation as closely as possible:
\\
\begin{align} 
R(v)_{RBF} & = \sigma^2 \exp \left[ -\frac{v^2}{2l^2} \right], \quad v \equiv |n-m|, n, m \in N  \label{eqn:ap_gpr_kernels:RBF_} \\
R(v)_{RQ} & = \sigma^2 \left[1 + \frac{v^2}{2 \alpha l^2}\right]^{-\alpha} , \quad v \equiv |n-m|, n, m \in N  \label{eqn:ap_gpr_kernels:RQ} \\
R(v)_{SM} & = \sum_j^J c_j \cos (2 \pi \Omega^T V) \prod_p^P e^{-2\pi^2 v_p \sigma_p^2} \\
\text{where: } \Omega & \equiv [\omega_j^{(0)}, ..., \omega_j^{(P)}], V \equiv |n-m|, n, m \in N \in \Re^P \label{eqn:ap_gpr_kernels:SM} \\
R(v)_{SM}|_{P=1} & = \sum_j^J c_j \cos (2 \pi \omega_j v) e^{-2\pi^2 v \sigma^2}, \quad v \equiv |n-m|, n, m \in N \label{eqn:ap_gpr_kernels:SM_univariate} \\
R(v)_{SSGPR} & = \frac{\sigma^2}{J} \sum_j^{J} \cos(2\pi\omega_j v), \quad v \equiv |n-m|, n, m \in N \label{eqn:ap_gpr_kernels:SSPG} \\
R(v)_{MAT}|_{q = p + 1/2} & =  \exp \left[   -\frac{\sqrt{2q} v}{l} \right]  
\frac{\Gamma(p+1)}{\Gamma(2p+1)} 
\sum_{i=0}^{p} \frac{(p+i)!}{i!(p-i)!}\left[ \frac{\sqrt{8q}v}{l}\right]^{p-i}  \label{eqn:ap_gpr_kernels:MAT_}\\
\text{where: }  v & \equiv |n-m|, n, m \in N, \Gamma(n) \equiv (n-1)!
\end{align}
\\
\textbf{$R(v)_{RBF}$}: The \textit{radial basis kernel} (RBF) represents a covariance function described mathematically as a Gaussian. This form of a covariance function captures the continuity of the underlying true covariance stationary process - that outputs separated by large input distances are weakly correlated than those with closely spaced inputs. The rate of decay of correlations with increasing input separations is controlled by a parameter called the length scale, $l$. Since a Gaussian retains its form under a Fourier transformation, we are implicitly assuming that we are learning a single Fourier component centered on the mean of this Gaussian. Hence,  RBF is expected to underperform for covariance stationary processes with complex power spectral densities. 
\\
\\
\textbf{$R(v)_{RQ}$}: The \textit{rational quadratic kernel} (RQ) is one way to capture correlation strengths as it consists of a set of Gaussians with different length scales. An RQ Kernel may work as a smoothening approximation, where large length scales allow us to grant a non-zero weight to Fourier components far from the mean. However, the RQ kernel summarises a set of basis functions which are still Gaussians centered in the same place in the Fourier domain. We choose to consider kernels where the spectral interpretation is clear and literature suggests that alternative methods, such as the spectral mixture kernel, outperforms RQ.  
\\
\\
\textbf{$R(v)_{SM}$}: As introduced in \cite{wilson2013gaussian}, a \textit{spectral mixture kernel} (SM) provides a basis of Gaussian functions with differing means, covariance strengths, and length scales. Theoretically, this kernel enables us to learn multiple Fourier components. 
\\
\\
However, the optimisation problem associated with estimating the parameters for the spectral mixture kernel is prohibitive for complex true power spectral densities relevant to our application. We can choose to fix the basis, and then difference between the periodic kernel and the SM kernel for zero mean univariate processes will be only in the scaling factors (since each term in the SM kernel represents a convolution in the Fourier domain with a Lorenztian and dirac-delta functions). 
\\
\\
In \cite{wilson2013gaussian}, a detailed comparison for interpolation and prediction is conducted, comparing RBF, RQ, Periodic and SM kernels. The SM kernel appeared most apt in learning Fourier domain correlations. However, the prediction problem in \cite{wilson2013gaussian} consisted of a periodic data with a linear drift - the linear drift being pathological for a periodic kernel alone. Our application has no such drift term. We expect that using the Periodic Kernel allows us to learn correlations for processes with complex power spectral densities without incurring a complex optimisation problem defined over a large number of hyperparameters. 
\\
\\
\textbf{$R(v)_{SSGPR}$}: An interesting approach that mimics the Livska Kalman Filter in the GPR regime is the \textit{Sparse Spectrum Gaussian Process Regression} (SSGPR) algorithm presented in \cite{quia2010sparse}. In both algorithms, a basis of osscillators is provided, where oscillator frequencies are assumed to be known or estimated via an optimisation procedure. Both algorithms utilise a trignometric polynomial approximation of a covariance stationary process. 
\\
\\
Notably, both algoriths employ a deliberate approach to increase the accuracy of phase estimation. Whereas the Livska Kalman Filter estimates the real and imaginary parts of a Hilbert transform of the original signal simultaneously such that amplitude and phase information can be reconstructed, SSGPR estimates the coefficients of cosine and sine terms in the trigometric polynomial reconstruction of $f(n)$  directly. As the authors note in \cite{quia2010sparse}, the estimation of both sine and cosine terms separately enables better prediction power even though it would be mathematically equivalent to consider a single cosine with an amplitude and some arbitrary phase. 
\\
\\
There are three ways in which Livska Kalman and SSGPR approaches differ. Firstly, the the authors in \cite{quia2010sparse} recommend Monte Carlo sampling procedure to estimate and optimise over basis frequencies, whereas our Livska Kalman Filter uses a fixed basis. Secondly, the stacked osscilators of Livska Kalman and the SSGPR algorithm do not appear to share the same mathematical structure for the covariance matrix, $\hat{P}$ (in our notation). Lastly, the Livska Kalman Filter is recursive in nature, whereas SSGPR appears to be designed for batch training. 
\\
\\
Given the apparent success of autoregressive methods for our applications and (imaginably?) resource intensive nature of Monte Carlo methods for complex spectral densities (?), a detailed comparison of the Livska Kalman Filter using a Fixed Basis and the SSPG algorithm has not been conducted.
\\
\\
\textbf{$R(v)_{MAT}$}: Lastly, we note that the \textit{Matern kernel} is equivalent to an autoregressive AR($p$) process, for a natural number $p$, such that $p = q - \frac{1}{2}$. In the autoregressive approaches considered in this paper, $p= 100$ and a prohibitive increase in computational load is associated with computing a kernel of the Matern form.



