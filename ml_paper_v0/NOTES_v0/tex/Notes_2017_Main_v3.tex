
In this study, we investigate numerical techniques to reconstruct stochastic state dynamics and enable prediction. Through a specific measurement protocol performed on an ensemble of qubits interacting with a noisy classical field, one can generate a measurement record which encodes true noise correlations and these correlations are used to predict noise evolution beyond the measurement record \cite{mavadia2017}. The true dynamics describing slowly varying laboratory noise process are, of course, unknown. However, in the regime of classical covariance stationary processes satisfying specific properties, a variety of different representations can be used to approximately reconstruct a true, but unknown, covariance stationary process. The accuracy of these representations are often discussed in the mean square limit - that the average difference between a true stochastic process and its estimator approaches zero given enough measurements. However, the interpretation of the mean square limit in context of generating sufficiently accurate predictions for experimental control applications is limited. Hence, this study presents a numerical analysis of predictive power for a number of different choices of representations for approximately reconstructing covariance stationary processes. 
\\
\\
Our measurement protocol effectively discretises a continuous time slowly varying laboratory noise process. This is the key simplifying assumption about our measurement model. Further, we operate at timescales where our measurement action is significantly faster than the true noise dynamics. In this regime, we consider the true laboratory noise dynamics as a `true' discrete time covariance stationary process. Our state estimation problem is to track true noise dynamics. In addition to the true noise dynamics, we assume that our measurements are further corrupted by Gaussian measurement noise and this defines our filtering problem. Our prediction problem is to predict noise evolution once our measurement record ends. In standard machine learning approaches, prediction can often mean interpolation or smoothening in region where observations (measurements) are available. However, our prediction problem is that of univariate time-series forecasting, and hence, the accuracy of our predictions depends on the efficacy of our chosen representation in capturing stochastic dynamics. 
\\
\\
From a computational perspective, our choice of an approximate representation of a true covariance stationary processes naturally dictates the structure of an algorithm. Additionally, for many state estimation and prediction problems, learning from data assumes that hyper-parameters associated with a chosen representation are known or define a tractable optimisation problem over an arbitrary choice of an objective (cost) function. In many standard engineering and econometric applications, it is possible to use a priori information or theory to define or reduce the search space for these hyper-parameters. As we intend to track stochastic dynamics arising from complex power spectral densities commonly experienced in realistic laboratory settings, it is difficult to simplify the hyper-parameter optimisation problem without increasing our apriori assumptions of true noise dynamics. 
\\
\\
In investigating approximations to covariance stationary processes, we are effectively studying the types of stochastic processes generated by different types of covariance functions. Hence, the study naturally touches both frequentist and Bayesian approaches to state estimation and prediction. The following general structural representations are considered in this study: (a) finite order autoregressive processes; (b) an infinite sum of trigometric polynomials with a fixed frequency spacing; and (c) a collection of stationary processes defined on a circle for a collection of fixed basis frequencies. We briefly consider wavelets as an alternative basis for reconstructing covariance stationary processes in Appendix [PLACEHOLDER]. For each representation, we use either the mean square error or the maximum likelihood as our cost function to define an associated optimisation problem over hyper-parameters. In Baysesian approaches, hyper-parameters are assigned a probability distribution that incorporates apriori assumptions. Where possible, we use a physical argument to constrain search space for hyper-parameters; in all other cases, we are equivalently assuming uniformly distributed hyper-parameters. (I haven't analysed the measurement model through a Bayesian framework to see if the apriori probability distribution on hyper-parameters (i.e. our noise variance strengths) could be anything else other than uniform.)  
\\
\\
This document is structured as follows. In Section I, we provide details about the experimental framework under which our measurement record would be created, though we rely solely on simulations to characterise our algorithms in this study. We refer to \cite{soare} to show that projective measurements performed on a qubit interacting with a classical field with phase noise encodes information about the slowly varying phase noise process. We detail the simplifying assumptions about our measurement model that ensure that our (Ramsey) measurement outcomes, in the absence of measurement noise, can be interpreted as a discretised covariance stationary process that represents the a linearised derivative of the true phase noise field. Relevant sub-sections of \cite{soare} have been re-derived in an equivalent way in Appendix \ref{sec:ap_setup}.
\\
\\
In Section II, we select representations of covariance stationary processes that are well suited for tracking realisations from complex power spectral densities, and we link these representations to a choice of algorithm. In modelling true noise dynamics to enable enhanced experimental control protocols, we are severely limited from using any theoretical reasoning to inform model design, and/or to reduce the search space for hyper-parameters associated with a particular choice of representation. For uniformly distributed hyper-parameters, our hyperparameter optimisation becomes prohibitively resource intensive or intractable for our applications. We formulate a hyperparameter optimisation problem for each algorithm, and we highlight pathologies or simplifications enabled by our choice of representation. 
\\
\\
In Section III, we discuss the predictive capabilities and state estimation results for algorithms under severely imperfect learning scenarios. Here, the measurement noise level is low, and imperfections in learning conditions come from reducing our experimental over-sampling rate until our bandwidth assumption is violated; and limiting the number of true spectral components that an algorithm can discern within any given spectral range.
\\
\\
In Section IV, we gradually add measurement noise to algorithms operating in imperfect learning conditions. The representation of true stochastic dynamics is sufficiently problematic for our application even in the low measurement noise case, and our filter design and automated tuning problems are further complicated when non-trivial levels of measurement noise is introduced. In this context, we focus on state space representations of stochastic dynamics and we investigate whether state space models offer advantages in supporting the easy deployment of classical Kalman filtering techniques.
\\
\\
\section{Measurement Model [PLACEHOLDER]}
TBC.
\section{Approximating Stochastic Dynamics for State Estimation} \label{sec:main_A}

In the absence of a theoretical model to characterise true stochastic dynamics for state estimation, we choose approximate generalised representations of covariance stationary stochastic processes. The choice of a particular learning algorithm is associated with a different mechanism by which stochastic dynamics are approximated. We state these representations below, and we state the corresponding algorithm implemented in this study. 
\\
\\
Covariance stationary processes have an exact representation as a moving average process of infinite order by Wold's decomposition, as defined in Appendix \ref{sec:ap_approxSP}. Additionally, each moving average process has an approximation as an autoregressive process of finite order, $q$. For zero mean Gaussian noise $w_n$, $n$ denoting time steps for $N$ total measurements, the true state representation is:
\begin{align}
f_n^{AR} & = \sum_{r=1}^{q} \phi_r f_{n-r}  + w_n\label{eqn:main:f_true_AR}  \\
\ex{w_n} &\equiv 0 \\
\ex{w_n^2} &\equiv \sigma^2 < \infty \\
\ex{w_n w_m} &\equiv Q_n < \infty \forall n, m \in N
\end{align}
The Least Squares Filter (LSF) is a modified autoregressive approach of finite order, as introduced in \cite{mavadia2017}. In the absence of additional Gaussian measurement noise, i.e $\hat{y}_n \equiv \hat{f}_n$, the order $q$ of the autoregressive AR(q) process would represent the number of past measurements used for enabling prediction in \cite{mavadia2017}. The estimator for the true $f_n$ in \cite{mavadia2017} is constructed as:
\begin{align}
\hat{f}_n^{LSF} & = \sum_{r=1}^{q} \hat{\phi}_r y_{n-r} \label{eqn:main:f_hat_LSF} 
\end{align}
The $\{\phi_r\}$  are directly learned via gradient descent in \cite{mavadia2017}.
\\
We note that an AR(1) process is a random walk .... [TBC: there should be an easily summarised link between AR(1) processes to noise correlations analysis in QCL's RB work; and (maybe) to quantisation error in one bit-sampling of CT signals]. ... 
\\
A simple extension of the LSF is to recast this filter into a state space model, and then enable the use of Kalman Filtering (KF) techniques.
For a known linearised dynamical model ($\Phi_n$) describing the evolution of the state $x_n$ with process noise injection, $w_n$ as defined for LSF,  and a known measurement action ($H_n$) on $x_n$ corrupted by measurement noise $v_n$, the true state space model is:
\begin{align}
x_n & = \Phi_{n-1} x_{n-1} + w_{n-1} \\
f_n^{KF} & \equiv H_n x_n \\ \label{eqn:main:f_true_KF}
y_n & = f_n^{KF} + v_n \\
\ex{v_n} &\equiv 0 \\
\ex{v_n v_m} &\equiv R_n < \infty  \forall n,m  \in N 
\end{align}
We omit the Ricatti equation for propagating the covariance matrix of $x$, found in any textbook [REFS]. Let $(-), (+)$ denote a priori and a posteriori state estimates (i.e. before and after a Kalman update is performed in the same time step). Then, the Kalman states estimates $\hat{x}_n$, and its covariance $\hat{P}_n$, for a incoming measurement record $y_n$ are:
\begin{align}
\amp{n}&= \Phi_{n-1} \app{n-1} \Phi_{n-1}^T + Q_{n-1} \\
\gamma_n &\equiv \amp{n} H_n^T(H_n\amp{n}H_n^T + R_n)^{-1} \\
\hat{y}_n(-) & = H_n \amx{n} \\
\apx{n} &= \amx{n} + \gamma_n (y_n - \hat{y}_n(-)) \\
\app{n} &= \left[1  - \gamma_n H_n \right] \amp{n}
\end{align}
The matrix $Q_n$ defines process noise features weighted by the noise strength $\sigma^2$, and $R$ defines measurement noise features. These `known' matrices are part of a priori filter design. Full details can be found in the derivation of the Kalman Filter for a non-standard implementation in Appendix \ref{sec:ap_liska_deriv}.
\\
\\
Any autoregressive process can be easily recast into a state space model. We recast $\{\phi_r\}$ into the Kalman state transition matrix $\Phi$ for a state-space model. Once in this form, the deployment of classical Kalman filtering techniques is straightforward and we call this approach an autoregressive Kalman filter (AKF). In particular, $\Phi$ and $Q$ for the AKF are defined in \eqref{eqn:akf_Phi} and \eqref{eqn:akf_Q} in Appendix \ref{sec:ap_approxcov_lsakf}.
\\
\\
In a pure AKF approach, the autoregressive weights, $\{\phi_r\}$, are learned as part of hyper-parameter optimisation. Since our application can represent large order numbers ($q>100$) compared to typical engineering or time series modelling applications ($ q \approx 1 - 15$), the pure AKF is associated with an extremely difficult optimisation problem. We side-step this problem by using the learned information in LSF to define the state transition matrix for AKF. This means that state estimation procedures across these two algorithms are nearly theoretically equivalent. The additional hypothesis we test by enabling AKF is increased functionality for measurement noise filtering, for equivalent autoregressive weights in LSF.
\\
\\
Standard Kalman Filtering equations are equivalent to a particular branch of machine learning algorithms described by Gaussian Process Regression \cite{solin2014}. More generally, Bochner's Theorum ensures that any positive finite measure can generate a covariance function representing a complex covariance stationary process in the mean square limit, according to \eqref{eqn:bochnerthm}. This fact is heavily exploited in Gaussian Process Regression (GPR) to design covariance functions (kernels) upon which data is projected; enabling spectral learning (pattern recognition) and prediction (commonly, interpolation). Maximising prediction outside the zone of the measurement record is our primarily goal in the this paper, and this goal is less commonly discussed in standard GPR literature. We consider GPR with the Periodic Kernel (GPRP) as one approach, and exclude other choices of kernels on pragmatic grounds as outlined in Appendix \ref{sec:ap_gpr_kernels}. The Periodic Kernel represents a covariance function of an infinite sum of trignometric polynomials. 
\\
\\
The true system definitions ($y_n, f_n, w_n, v_n$) for GPRP are identically defined in previous sections. The predictions, $\hat{f}_n$, for $n \in N^*$ test points, are derived in Appendix \ref{sec:ap_prediction:GPR}, listed here in vector form with $\mu_f \equiv \mu_y \equiv 0$:
\begin{align}
\hat{f}_n &\equiv \ex{f*|y} \\
& = K(N*,N)(K(N,N) + R )^{-1} y\\
\text{cov}[f*|y] &= K(N*,N*) \\
& - K(N*,N)(K(N,N) + R)^{-1}K(N,N*) 
\end{align}
Here, the elements of the matrix $K$ are defined by a choice of a covariance function $R(n-m)\equiv R(v), v = n-m$, where we have chosen the Periodic Kernel:
\begin{align}
K(n,m) & \equiv K_{n,m} \equiv R(n-m) \\
& = \sigma^2 \exp (- \frac{2\sin^2(\frac{\omega_0 v}{2})}{l^2}), \quad v \equiv n-m  \label{eqn:sec:main:GPRP_l} \\
&\approx \sigma^2 (p_{0,J} + \sum_{j=0}^{J} p_{j,J} \cos(j\omega_0 v)) \\
\omega_0 &\equiv 2 \pi \frac{1}{p} \equiv  \frac{\omega_j}{j}, j \in \{0, 1,..., J\}  \label{eqn:sec:main:GPRP_p}
\end{align}
The approximate equivalence of a Periodic Kernel to trignometric polynomials, including  the definition of $p_{0,J}, p_{j,J}$ is known in literature \cite{solin2014}, and we show this in Appendix \ref{sec:ap_approxSP:GPRPKernel} for completeness. Here, the period $p$ in \eqref{eqn:sec:main:GPRP_p} and the length scale $l$ in \eqref{eqn:sec:main:GPRP_l}  are two hyper-parameters of the Periodic Kernel that must be learned via apriori knowledge or via optimisation. We discuss their experimental interpretation in subsequent sections.
\\
\\
In our fourth and final approach, we consider using a fixed basis of oscillators to probe an arbitrary noise field - if true spectral components can be projected on this basis, then these components are learned, else the closest approximate basis frequency is given a stronger weight. This method estimates the amplitude and phase of each basis osscillator during Kalman filtering according to the procedure outlined in \cite{livska}. Predictions outside the zone of measurement data can be generated in the standard way by setting the Kalman gain equal to zero, and recursively propagating forwards in time; or equivalently, using learned instantaneous amplitude and phase information for each basis osscillator to predict an arbitrary number of time-steps ahead. Standard Kalman filtering equations can still be used but with $\Phi_n \equiv \Phi \forall n \in N$ given in Appendix \ref{sec:ap_approxSP:LKFFB}. Further, $Q$ is no longer a known matrix but describes adaptive noise features which depend on state estimates, as  in \cite{livska}: 
\begin{align}
\Gamma_{n-1} &\equiv \Phi_{n-1}\frac{\apx{n-1}}{\norm{\apx{n-1}}} \\
Q_{n-1} & \equiv  \sigma^2 \Gamma_{n-1}\Gamma_{n-1}^T
\end{align}
Here, $\sigma^2$ retains its interpretation as process noise covariance strength, as defined for LSF, AKF and GPRP but $Q$ is not longer $\idn$ and instead adds in structural noise features (such features are incorporated into standard derivations of the Ricatti equation). 
The original Livska Kalman Filter (LKF) is found in \cite{livska}; that LKF satisfies standard derivations is explicitly shown for completeness in Appendix \ref{sec:ap_liska_deriv}; and our additional modifications to LKF by using a fixed basis of osscillators (LKFFB) are detailed in Appendix \ref{sec:ap_liska_fixedbasis}.
\\
\\
In summary, a pure autoregressive approach is presented via LSF; we test measurement noise filtering for autoregressive representations via AKF; and implement Gaussian Process Regression using trignometric polynomials in GPRP. Lastly, we use a fixed basis of osccillators with instantaneous amplitude and phase estimation to probe noise using LKFFB.
\\
\\
Let $\beta_z (t)$ be a slowly varying continuous true noise process of Appendix \ref{sec:ap_setup}, and let $f(n)$ be the discretisation of $\beta_z (t)$ via projective qubit measurements i.e.  $f(n)$ is the 'true' state for a state estimation algorithm in the absence of measurement noise. As in Fig. \ref{fig:sec:main_setup}, we consider a single realisation of the true noise, and we collect $n_{train}$ number of noisy measurements spaced $\Delta t$ apart in time. We stop collecting measurements and predict forward for all time steps until $n_{predict}$. The Fourier resolution set by experimental parameters, $\Delta t$ and $n_{train}$, effectively ballparks the computational frequency spacing for fixed osscillators in LKFFB and the spacing between trignometric polynomials in the infinite sum in GPRP i.e $\Delta s \equiv \frac{1}{\Delta t n_{train}}$. In a real implementation where future noise predictions tested against actual (future) measurements, the actual Fourier resolution over the total measurement record would be $\Delta s' > \Delta s$, and where the difference between Fourier and computational grid spacing inherent in an algorithmic approach increases for larger non-zero prediction horizons. We restate the obvious that our computational resolution is limited by $\Delta s$ (and computational considerations); and we operate near maximal physical Fourier resolution if we test a real implementation with short prediction horizons. 

%  \begin{figure}[h!]
% 	\centering
% 	\caption{Set Up [PLACEHOLDER]}\label{fig:sec:main_setup}
% 	\includegraphics[width=0.45\textwidth]{schematic_2.png}
% \end{figure}

Lastly, the true noise in our simulations is bandlimited by $2\pi f_0J$, where $J$ are true spectral components spaced $f_0$ apart. The shape of the power spectral density is given analytically in Appendix \ref{sec:ap_randomprocess}. All algorithms are blind to true noise parameters. In order to use classical covariance stationary process theory and linear prediction tools, we impose properties on true noise as detailed in Appendix  \ref{sec:ap_randomprocess}. In particular, we assume that our experimental sampling rate corressponds to a bandwidth assumption, $B$, which is much greater than true noise bandwidth.


\subsection{Filter Tuning}
Prior to presenting any result, all algorithms must be tuned, namely, one must use apriori knowledge or one must optimise hyper-parameters. 
\\
\\
For LSF, the hyper-parameter for optimisation is the order of magnitude at which the initial condition for the gradient descent step-size is chosen, as in \cite{mavadia2017}. Once an order of magnitude is determined, gradient descent in LSF adaptively adjusts the step-size while training over a single training dataset. The objective function for training is root mean square error; the final loss value at the end of a single training run is used to estimate initial order of magnitude for the step-size. 
\\
\\
For GPRP, we use GPy's implementation of a well-known optimiser, L-BFGS-B,to optimise over hyperparameters by maxmising the log likelihood function. We encounter pathologies in our application (refer Appendix \ref{sec:ap_opt_diagnostics}). Analytical simplifications to this optimisation procedure are detailed in Appendix \ref{sec:ap_opt}, and results in this paper reflect our best attempt at tuning GPRP implmentations. The lack of a sufficiently effective procedure to tune GPRP has led to performance results that are below theoretical expectations. 
\\
\\
The recursive structure of Kalman filtering is problematic for standard gradient-based optimisers. Both LSF and GPRP reflect batch training schemes, where the lack of a recursive structure allows analytical (numerical) access to gradients (gradient estimates) to hasten optimisation. Our Kalman optimisation procedure is a naive random sampling of this parameter space for a mean square error objective function, using true errors between one step ahead state estimates and $f(n)$. For AKF and LKFFB, the hyper-parameters are two scalar Kalman design parameters, process ($\sigma$) and measurement ($R$) noise covariance strengths. An alternative would be to use a maximum likelihood function for Gaussian noise processes, but we simplify our optimisation problem by exploiting our access to the true $f(n)$ in simulations. 
\\
\\
We explore the shape of our Kalman objective function for a number of cases. For sufficiently high oversampling rates, the objective functions for Kalman filtering are relatively flat. This has two implications. Firstly, without a theoretical bound on $\sigma, R$, there is a large parameter search space - with a flat cost function, standard simplex optimisers fail to traverse this large region or are unable to converge on shallow features. Secondly, we adopt a standard procedure to include one-step ahead state estimates from the last $2.5\%$ of training time to define our objective function, namely, the Bayes Risk calculation given by \eqref{eqn:sec:ap_opt_LossBR}. We exclude Bayes Risk over the forecasting region (i.e. prediction at $n>n_{train}$) as the Kalman gain over this region is equivalently zero, thereby defining a separate objective function. A linear combination of objective functions for both state estimation and forecasting periods could be considered but it is beyond the scope of this study. 

\subsection{Prediction and Spectrum Estimation for a Simple Sum of Sinuoids}
 
We first introduce these representations in context of a simple demonstration: tracking a periodic true state. We take realisations $f(n)$ drawn from a true power spectral density with equally weighted $J=40$ spectral components and uniformly distributed random phases. While autoregressive approaches, LSF and AKF, in this paper require no explicit computational basis, both GPRP and LKFFB have a structure where a basis of osscillators is inherently specified. Hence, we construct our demonstration for two sub-cases (A) when perfect projection on the osccillator basis is possible, and (B) when perfect projection is impossible. Over the number of training points considered, $f(n)$ is periodic in Case (A) and insignificantly aperiodic in Case (B) such that the fundamental period exceeds the total training time by $\approx 0.002\%$. The true highest frequency component defines the shortest time scale for periodicity, and this timescale is seen during training. We add measurement noise to $f(n)$ defined by [EQREF] where the measurement noise strength is less than $1\%$ of 3 standard deviations of members in a given $f(n)$ realisation. 
\\
\\
Fig. \ref{fig:sec:main_fig1} (A) and (B) panels depict results for perfect and imperfect projection regimes. Each panel reports low loss regions over Kalman design parameters for AKF and LKFFB respectively. We highlight the lowest 20 Bayes Risk values over 75 randomised initial pairs of $(\sigma, R)$. When the lowest Bayes Risk regions during state estimation (tan dots in Fig. \ref{fig:sec:main_fig1}) show overlap with lowest Bayes risk regions during prediction (blue dots in Fig. \ref{fig:sec:main_fig1}), the optimisation problem over $(\sigma, R)$ is physically sensible i.e. tuning the filter during state estimation can lead to low-loss outcomes during forecasting. When our Bayes Loss minimising choice of $(\sigma^*, R^*)$ lies within this area of overlap, we deem our filter to be `tuned'. 
\\
\\
To fully characterise the change in the optimisation problem from perfect to imperfect projection regimes, we depict all Bayes Risk trajectories over training and prediction time steps. We find that the optimisation problem shows increasingly small areas where low training and prediction loss regions overlap when the fixed basis in LKFFB is poorly specified (Case B) relative to the true Fourier components in $f(n)$.  In constrast, the optimisation problem for AKF is found to be well defined for most of the cases considered.
 \begin{figure}[h!]
	\centering
	\caption{} \label{fig:sec:main_fig1}
	\includegraphics{fig1.pdf}
\end{figure}  

Fig. \ref{fig:sec:main_fig2} compares predictive capabilities of all tuned algorithms for perfect and imperfect projection in columns (A) and (B) respectively, with $1\%$ measurement noise level. In (i), we repeat single predictions for different $f(n)$ and different realisations of measurement noise to calculate the Bayes Risk for trials $= 75$.  We normalise the Bayes Risk against risk obtained by predicting the (zero) mean of $f(n)$. In perfect learning conditions, the LKFFB learns all information about $f(n)$ enabling perfect prediction for all future time steps. We note that the approximate reconstructions, LSF and AKF, reach parity with predicting $\mu_{f_n}$ for approximately 20-25 time steps forward. Our inability to tune GPRP is reflected in that it is the worse performing algorithm, reaching parity within 5 steps forward. For all low-noise cases, we expect equivalent performance between LSF and AKF since the learned weights of LSF define stochastic dynamical model in AKF. In column (B), we perturb away from the perfect project scenario and LKFFB performance drops to AKF and LSF for imperfect projection. The insets in (i) depict normed Bayes Risk for forecasting on a linear scale; and an instance of a single run in this averaging process is depicted in (ii).
\\
\\
In addition to time domain predictions, the information in AKF/LSF and LKFFB allow us to form a estimate of the true noise spectrum, and we report this in Fig. \ref{fig:sec:main_fig2} (iii). To conduct spectrum estimation via LKFFB, we extract the learned amplitudes for each basis osscillator at the end of the training period. The square of these amplitudes forms our spectrum estimate for a covariance stationary process. Instantaneous amplitudes estimated by LKFFB in a single run are extracted via an optimal training procedure, such that $n_{train}$ is set using \eqref{eqn:sec:ap_liska_fixedbasis_nC}. In AKF/ LSF, we observe the state space representation for the autoregressive Kalman filter in \eqref{eqn:akf_Q} and the spectral equation for finite order autoregressive processes in \eqref{eqn:sec:ap_ssp_ar_spectden}. We construct a spectrum estimate by combining the weights learned from LSF in \eqref{eqn:sec:ap_ssp_ar_spectden}. We require an additional constant scaling factor: this is approximated by using the optimal Kalman process noise covariance strength learned via hyperparameter optimisation for the AKF, namely, $\sigma^*$. We have not established an equivalent procedure to extract amplitudes from the Periodic Kernel in GPRP, and the implied arbitrary truncation of the infinite basis of oscillators to get to \eqref{eqn:pjJ} suggests that a similar extraction may not be possible. 
\\ 
\\
In Fig. \ref{fig:sec:main_fig2} (iii), we depict the one-sided power spectral density estimate from LKFFB, and LSF/AKF relative to the truth. The difference between AKF and the truth largely depends on the effectiveness of the optimisation procedure to estimate process noise scaling factor. For LKFFB, instantaneous amplitude estimation appears to be more robust than time domain predictions under imperfect learning, the latter of which are vulnerable to choice of fixed basis and learned phase information.  We restate that spectrum estimation in a tuned LKFFB are based on learned state estimates after a single run; whereas spectrum estimation for LSF/AKF combines information obtained via hyperparameter optimisation, namely, weights in LSF and $\sigma^*$ in AKF.
% \begin{figure}[h!]
% 	\centering
% 	\def\svgwidth{\columnwidth}
% 	\caption{} \label{fig:sec:main_fig2}
% 	\import{./img/}{compiled_fig_2_ver_0.pdf_tex}
% \end{figure}  
\section{Performance under Low Oversampling Ratios and Limited Spectral Resolution} \label{sec:main_C}
We take the imperfect projection regime (B) of Section \ref{sec:main_A} (with $1\%$ SNR) and we extend to a scenario where a  large number of true frequency components exist in our true PSD which, given our experimental parameters and model specifications, our algorithms do not have a sufficient resolution to detect in any single realisation of $f(n)$. Further, we vary the ratio between the true bandwidth of $f(n)$ realisations, namely $f_0J$, and assumed bandwidth, $B$, used to set experimental and computational parameters. 
\\
\\
In this section, we omit GPRP results as it is difficult to assign an interpretation to numerics without a sufficiently robust and tractable hyper-parameter optimisation procedure. Further, given an idealised optimisation procedure, increasing $f_0J$ for a basis of an infinite number of osscillators in the Periodic Kernel should make no theoretical difference to GPRP's  predictive capabilities. 
\\
\\
We consider normed Bayes Risk of algorithms as $\frac{f_0J}{B}$ is reduced, for two regimes: (B) an imperfect learning regime, as defined by inability to project perfectly on a fixed computational basis in Section \ref{sec:main_A} and (C) an imperfect projection regime with limited spectral resolution, where $\approx$ 500 true frequency components exist between two adajcent basis frequencies. In simulations Fig. \ref{fig:sec:main_fig3} (i)-(ii), we depict $B > f_0J$ but gradually reduce the level of oversampling by [NUMBER]. In  simulation (iii), we relax our bandwidth assumption such that our computational basis in LKFFB is ill-specified $B < f_0J$. (However, our Nqyquist multiplier is still high enough to avoid physical aliasing, $r = 20 >> 2$.)
\\
\\
In Fig. \ref{fig:sec:main_fig3} (top two rows), we see prediction horizon over which algorithms do better than predicting $\mu_{f_n}$ is greatest when $B > f_0J$ for any algorithm, simply confirming that over-sampling yields better predictions. However, the basis of osscillators approach embodied in LKFFB is outperformed by the autoregressive representation in LSF and AKF. In particular, the failure of LKFFB is worse than predicting $\mu_{f_n}$, whereas autoregressive approaches simply approach $\mu_{f_n} \equiv 0$. 
\\
\\
Fig. \ref{fig:sec:main_fig3} (bottom two rows) depict spectrum estimation results for a single run in the normed Bayes Risk calculation for simulations (i-iii). All spectrum estimates find the band-edge for true noise. Spectrum estimates generated  (collectively) via LSK and AKF diverge from truth by a constant scaling factor, which likely reflects the difficulty of solving the underlying AKF optimisation problem for $\sigma^*$. 
\newpage
\onecolumngrid
 \begin{figure}[h!]
	\centering
	\caption{} \label{fig:sec:main_fig3}
	\includegraphics{fig3.pdf}
\end{figure} 
\FloatBarrier
\twocolumngrid
Spectrum estimation in (C) for LKFFB has additional information that has not been included for Cases (A) and (B). Namely, with severe undersampling, the LKKFB algorithm assigns an extremely large weight to basis frequencies as it cannot discern all true spectral components within a spectral range. As a result, we scale the reported spectral estimates by the level of undersampling to check that recaled spectral estimates are comparable with the truth, hence confirm the actual operation of the algorithm. The rescaling is a constant factor that does not change the shape of the spectral estimate from LKFFB in a single run. Since the level of undersampling is constant across (i-iii), the application of the rescaling factor is also constant for all results within Case (C). 
\\
\\
The underlying LKFFB optimisation problem also changes drastically when we violate our bandwidth assumption. We depict low loss regions for extreme Cases C(i) and C(iii) in Fig. \ref{fig:sec:main_fig8}, where the ill-specification of the computational basis in LKFFB is characterised by a fundamental breakdown of the optimisation problem. Namely, the low loss regions in state estimation show no overlap with low loss regions for prediction. Hence, tuning LKFFB based on losses incurred during training is irrelevant for forecasting in Case C(iii).

%  \begin{figure}[h!]
% 	\centering
% 	\def\svgwidth{\columnwidth}
% 	\caption{} \label{fig:sec:main_fig8}
% 	\import{./img/}{compiled_fig_8_ver_0.pdf_tex}
% \end{figure} 

\subsection{GPRP Optimisation and Performance}
In this subsection, we consider predictions generated by GPRP for 50 different optimisation runs conduct by GPy's L-BFGFS-B Algorithm. We provide initial conditions and bounds for hyper-parameters, and GPy optimises over hyperparameters for the same realisation of true noise, but different datasets corrupted by measurement noise. We reconsider Case (A) but for a smaller number of true spectral components ($J=4$) denoted by \textbf{Case A*} in Fig. \ref{fig:sec:main_fig9}, and we consider the severely undersampled imperfect projection case, namely \textbf{Case C(ii)} in the second column of Fig. \ref{fig:sec:main_fig9}. Even in the current set-up, we have simplified the optimisation problem for the Periodic Kernel by considering the longest and shortest timescales of the system sampled experimentally to provide a sensible initial value for $l$ and $p$ to an optimiser (refer \eqref{eqn:sec:main:GPRP_l} and \eqref{eqn:sec:main:GPRP_p}), namely:
\begin{align}
\frac{p_0}{\Delta t} & \approx n_{train} \\
l_0 & \propto \Delta t 
\end{align} 
As with the Kalman case, the optimisation over $\sigma, R$ is unconstrained. By specifying no prior, we assume uniformly distributed hyperparameteres. We repeat L-BFGS-B and generate predictions, and the results are in the first row Fig. \ref{fig:sec:main_fig9} (i).  GPRP predictions show moderate agreement in Case A*, and the procedure breaks down in Case C(ii) - where the breakdown is not anticipated theoretically by examining the form of the underlying covariance function. We attribute this to a difficult optimisation over remaining parameters - $l, \sigma, R$. Note that $p_0$ corressponds to the end of the training period (gray region), i.e. $y$ axis of Fig. \ref{fig:sec:main_fig9}. 
\\
\\
In particular, we test the hypothesis that if an optimised value for $p^* \approx p_0$, then we know that our optimisation procedure has resulted in a physically interpretable value.  Our training data forbids us to see any correlation lengths outside the set ${v: 1, ..., n_{train} -1 }$, and the covariance matrix generated by $R(v)$ repeats with period $p$. For $p^* <<< n_{train}$, we fail to see all possible correlation lengths and predictions worsen. This is depicted in row (ii) of \ref{fig:sec:main_fig9}. For $p^* >>> n_{train}$, we see predict zero for testing points outside the measurement record until the learned pattern begins to repeat itself at $p^*$ (brown dotted). The latter case suggests that we are seeing spectral components at a computational resolution that exceeds our equivalent, experimentally determined Fourier domain resolution. In all cases where $p^*$ is far from $p_0$ and perfect learning is not possible, numeric artefacts arise in the form of sharp discontinuities (pattern repeats with imperfect phase or amplitude information), or we mis-interpret the zero region in row (iii) as predicting the mean of zero-mean covariance stationary process. 

% \begin{figure}[h!]
% 	\centering 
% 	\def\svgwidth{\columnwidth}
% 	\caption{} \label{fig:sec:main_fig9} 
% 	\import{./img/}{compiled_fig_9_ver_0.pdf_tex}
% \end{figure} 

Subsequent sections do not report GPRP results since our methodology is not robust enough to generate meaningful predictions. 
\section{Measurement Noise Filtering} \label{sec:main_D}
A key success of Kalman Filtering in literature has been its ability to filter out uncorrelated measurement noise. Since LKFFB departs from classical Kalman Filtering by the use of adaptive noise features matrix $Q$, but the formalism of measurement noise filtering is unmodified, the measurement noise filtering capabilities should be theoretically equivalent between GPRP, LKFFB and AKF algorithms. Further, we expect AKF to have increased measurement noise filtering capabilities with respect to LSF even when they share identical autoregressive weights. 
\\
\\
We return to a comparable scenario to Case (A) of Section \ref{sec:main_A} such that all algorithms outperform predicting the mean for a certain forecasting period. We increase our measurement noise levels from $1\%$ to $10\%$ and $25\%$.
As a check of our procedure for a simple example, we note that the underlying optimisation problem for LKFFB and AKF responds by favouring larger noise covariances strengths as measurement noise levels are increased from 10\% to 25\%, namely, the optimal hyper-parameter pair in Fig. \ref{fig:sec:main_fig10} shifts towards the upper-right. This relationship breaks down as we move towards realistic learning scenarios. However, numerical investigations show that even with high levels of measurement noise, the optimisation problem for LKFFB does not become ill-specified unless there is physical aliasing (not shown) or a poor bandwidth assumption in the choice of basis (second column of Fig. \ref{fig:sec:main_fig8}).
% \begin{figure}[h!]
% 	\centering 
% 	\def\svgwidth{\columnwidth}
% 	\caption{} \label{fig:sec:main_fig10}
% 	\import{./img/}{compiled_fig_10_ver_0.pdf_tex} 
% \end{figure}
\FloatBarrier
In Fig. \ref{fig:sec:main_fig4} A, AKF outperforms LSF by enabling measurement noise filtering. While it may be possible to configure LSF for improved performance, the comparative statement in this study is that increased measurement noise filtering is enabled by AKF for an identical choice of autoregressive weights. Since Case (A) corressponds to perfect projection, LKKFB filters measurement noise and learns maximal information about state dynamics such that predictions outperform all algorithms for all future time steps. 
\\
\\
We perturb from Fig. \ref{fig:sec:main_fig4} A to imperfect projection with severe undersampling, namely, Case (C)(ii) of Fig. \ref{fig:sec:main_fig3} is comparable to column (i) in Fig. \ref{fig:sec:main_fig4} C. The normed Bayes Risk suggests that autoregressive AKF improves measurement noise filtering given an equivalent choice of weights in AKF and LSF. LKFFB time domain predictions deteriorate compared to Fig.\ref{fig:sec:main_fig4} A. In spectrum estimation, re-scaled instantaneous amplitudes learned during a single run in LKFFB capture band edges of true PSD. Autoregressive spectral estimates are vulnerable to accurate estimation of $\sigma^*$, and the band edge is captured less clearly as measurement noise increases.
\\
\\
\onecolumngrid
% \begin{figure}[h!]
% 	\centering 
% 	\def\svgwidth{0.9\textwidth}
% 	\caption{} \label{fig:sec:main_fig4}
% 	\import{./img/}{compiled_fig_4Com_ver_0.pdf_tex} 
% \end{figure} 
\FloatBarrier
\twocolumngrid

\section{Pathologies in Tuning Filters via Optimisation Procedures}
In tuning our algorithms thus far, we find that our Bayes Risk function has features pathological for standard gradient and simplex based optimisers. Brute force investigations over large hyper parameter regimes reveal that ensemble average Bayes Risk is relatively flat, and for some simple cases, shows periodic shallow features.  Traversing a flat space is difficult for local optimisers. The Rosenbrook function is a standard shape  to demonstrate efficacy of  gradient and simplex optimisers in traversing a local valley and finding a true global optimum. We consider an alternative 2D shape to highlight pathologies relevant to our application: a large flat plane with a narrow Gaussian dip, where the depth of the dip is a parameter we vary to break local optimisers. With stochasticity, a large flat parameter search space with numerous shallow features becomes an intractable problem for many standard Scipy optimisers. 
\\
\\
In addition to the shape of the objective function, Kalman optimisations are difficult as the recursive structure of the filter forbids access to analytical gradients, or gradient estimates. Coordinate ascent was suggested in \cite{abbeel2005discriminative} as method to discriminatively train Kalman filters in engineering (without the use of manual effort). It has a further attractive property that in 2D, the step size of coordinate ascent can both increase or decrease, unlike a continually decreasing step-size or simplex area of gradient and simplex based approaches, suggesting that coordinate ascent may explore a larger region in a flat stochastic plane.
\\
\\
 While we have not yet devised an implementation that works reliably on filtering problems considered in this study, Fig. \ref{fig:main:coordinateascent} benchmarks coordinate descent against top Scipy performers for traversing a flat plane with a narrow 2D Gaussian dip. We make our Gaussian feature shallow by decreasing the depth of the Gaussian dip from 25\% of total base crossectional area to less than 1\% in panel A; and we introduce stochasticity in panel B. All optimisers are initialised by a random sprinkling of initial conditions. A single true global optimum exists, namely, the bottom of our Gaussian dip at $(x_T^*=2, y_T^*=4)$.
 \onecolumngrid
% \begin{figure}[h!]
% \centering
% \caption{placeholder figure} \label{fig:main:coordinateascent}
% \includegraphics[width=\textwidth]{temp_coordinate_ascent_comparison.png}
% \end{figure}
\twocolumngrid
\FloatBarrier
 In each test case for Fig. \ref{fig:main:coordinateascent} A, the output of random initialisations are $(x^*, y^*)$ chosen by the optimiser. We consider a circular area with its origin at the true solution ($(x_T^*, y_T^*)$), and we count the number of $(x^*, y^*)$  points (wins) encountered as the radius of a circle under consideration increases (win radius). This is plotted against counting points dictated by the random distribution of initial conditions for $(x,y)$ (black line). An ideal optimiser would maintain a win ratio of 1 for all values of the win radius across all  choices of the depth of our Gaussian feature (colored lines). In constrast, the black line represents the distribution of randomised initial conditions unaffected by the action of the optimiser. While the full range of Scipy optimisers has been omitted except for the two top-performers, we see coordinate ascent performs equivalently to top-performer COBYLA and appears to outperform L-BFGS-B implementations. (The initial scope of optimisers included standard implementations of Powell, Nelder Mead, TNC and others in scipy.optimise library. Further details on diagnostics can be found in Appendix \ref{sec:ap_opt_diagnostics}.) 
 \\
 \\
 In Fig. \ref{fig:main:coordinateascent} B, we keep the Gaussian depth the same but increase stochasticity from 0\%, 1\%, 10\% and 25\% relative to amplitude of the Gaussian. Equivalent curves to Fig. \ref{fig:main:coordinateascent} A are omitted in leiu of a normalised heatmap that characterises robustness of coordinate ascent against SciPy's top-performer, COBLYA. For an ideal optimiser, all optimisation runs would converge on truth (red `x') visually creating a homogenously dark blue map with a single yellow point at the true known solution. While both algorithms struggle with increasing stochasticity, coordinate ascent is able to discern a global optimum with 10\% noise level whereas no such discernement occurs for COBLYA.  While these appear to be promising diagnostics, coordinate ascent moves  unreliably in the parameter space for $(\sigma, R)$ for applications considered in this study, and alternative techniques are under development.
 \FloatBarrier

\section{Conclusion}
We review techniques for approximately reconstructing covariance stationary stochastic processes. We assess the efficacy of techniques in enabling state estimation, prediction and filtering in the context of tracking slow noise processes encountered in realistic laboratory settings. In particular, we find that autoregressive representations allow for time domain predictions and spectrum estimation in non-ideal, noisy learning environments. We review challenges in tuning filters in context of limited apriori information available for filter design, and we provided high level diagnostics on the performance of standard local optimisers for pathological objective functions. 