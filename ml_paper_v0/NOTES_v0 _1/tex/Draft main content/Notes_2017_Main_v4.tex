We use quantum sensing protocols to enable predictive control of single-qubit sensors under environmental dephasing. Quantum sensing techniques support the realisation of quantum-enabled devices in realistic operational environments. In sensing applications, the susceptibility of quantum systems to external disturbances is harnessed to measure a physical quantity or enhance measurement precision \cite{degen2017}. Sensing demonstrations have found diverse applications in characterising both the intrinsic quantum object (sensor) and its external environment, as examples, through the detection of small time-varying field signals; Hamiltonian learning via adaptive measurement protocols; and noise spectroscopy where decoherence is analysed under different experimental control sequences \cite{degen2017, biercuk2011, matsuzaki2017, wiebe2015, lazariev2017}.
\\
\\
The realisation of predictive control for single qubit sensors depends on the measurement protocol. We focus exclusively on a sequence of projective measurements. A key challenge is that a projective measurement resets the quantum state of the sensor. Hence, one aim of predictive techniques is to enable future control strategies while reducing the need for projective measurements, for example, interleaving periods of projective measurements with periods of unsupervised control. A key performance metric for predictive techniques with projective measurements in this paper is the number of time-steps for which sensible predictions are obtained once experimental data collection ceases. The use of predictive control in context of a standard sensing protocol was experimentally demonstrated for a system of trapped ions in \cite{mavadia2017} for a pre-processed measurement record. In \cite{mavadia2017}, a quantum sensor is coupled to a continuous, stochastically drifting, non Markovian external noise field. A  standard sensing measurement protocol (see \cite{mavadia2017, degen2017}), namely, a series of Ramsey experiments, are applied to obtain a measurement record. Noise correlations are learned from the measurement record and enable time domain predictions about the quantum state of the sensor once data collection ceases.
\\
\\
 \begin{figure}[h] \label{fig:vfrey}
 	\caption{State prediction in \cite{mavadia2017} using pre-processed data for a sequence of Ramsey measurements performed on freely evolving qubits under constant Ramsey wait times ($\tau$) and a stochastic, time dependent detuning between qubit frequency and local oscillator (reference) manifesting as a dephasing process. A learning algorithm is trained using past measurement data and forward predictions commence at $t_k=0$. Algorithmic predictions seek to minimise prediction error relative to predicting the noise mean for maximal steps $t_k>0$.}
	\includegraphics[scale=0.9]{poster_2017_07_08_LSF.pdf}  
\end{figure}
In this paper, we build on \cite{mavadia2017} and seek model robust techniques to reconstruct arbitrary environmental dephasing processes and enable predictive control. In a simple approximation where our measurement procedure is linear, we draw on linear Bayesian filtering and prediction techniques as natural extensions of Least Squares Filter (LSF) implemented in \cite{mavadia2017}. The key challenge is to approximately reconstruct true environmental dephasing by designing a dynamical model, namely, a map from Gaussian white noise to a non-Markovian, Gaussian random process. We define the class of random processes representing environmental dephasing and we study autoregressive and harmonic reconstructions in the mean square limit. The choice of framework - a state space Kalman or a machine learning Gaussian Process Regression approach - is informed by the desire to test these accuracy of these reconstructions and enable additional measurement noise filtering for enhanced predictive performance and minimal computational burden. Where relevant, we describe well known regimes across our different numerical approaches where theoretically equivalent performance is anticipated. We extend our analysis to a treating a measurement record consisting purely of binary outcomes, namely, we extend state space Kalman models to non-linear and quantised measurement actions with non-Markovian state dynamics. 
\\
\\
In \cref{sec:main:part1}, we describe our non linear, quantised measurement model for sensing and we define approximations for analysis to be considered in a simpler, linear regime where quantised measurement outcomes are preprocessed. In \cref{sec:main:part2}, we detail assumptions about environmental dephasing processes which underpin our analysis. In \cref{sec:main:part3}, we reframe the absence of apriori information of the dephasing noise field as a dynamical model design problem in classical control engineering. We compare a selection of suitably modified approaches for predictive performance - drawn from the study of autoregressive processes, Kalman filtering and machine learning. We deploy these tools to reconstruct dephasing noise fields in realistic, non-ideal learning environments and we additionally corrupt experimental data with measurement noise. All predictive techniques require tuning when applied to specific applications, and we achieve this through an optimisation procedure detailed in \cref{sec:main:part4}. The optimisation procedure yields supporting evidence to compare model robustness of different predictive techniques. We conclude with \cref{sec:main:part5}, where we return to the full non linear quantised measurement model of \cref{sec:main:part1} and analyse the predictive performance of the Kalman frameworks for quantised measurements. 
\\
\\
By transforming to a specific interaction picture, it is seen that the behaviour of a qubit freely evolving under environmental dephasing is effectively indistinguishable from the behaviour of a stable qubit relative to a noisy local oscillator \cite{ball2016, soare2014}. This insight enables experimental verification of predictive control techniques \cite{mavadia2017}. We present numerical results while experimental verification is out of scope for our work.

\subsection{Measurement Model} \label{sec:main:part1}

We consider a freely evolving single qubit under environmental dephasing. By transforming to an appropriate interaction picture, the effect of an environmental dephasing process manifests as time-dependent stochastic detuning between a stable local oscillator (reference) and the qubit frequency.. Under dephasing, free qubit evolution is modeled as $\omega_A \to \omega_A + \delta \omega_A \dd$. Meaningful state estimation and prediction is contingent on effectively tracking dynamical phase evolution due to $ \delta \omega_A \dd$ from a sequence of projective measurements. 
\\
\\
Below, we define `measurement' for non-linear and approximately linear Bayesian analysis respectively.

\subsubsection{Non Linear Bayesian Model for Single Shot Outcomes}

As detailed in Supplementary Information, the probability of obtaining a measurement outcome, $d \in [0,1]$ corresponding to $(\ket{\p{z}-}, \ket{\p{z}+})$ respectively in the transformed basis is:

\begin{align}
P(d_t | \state_t, \tau, t) &= \begin{cases} \cos(\frac{\state(t,\tau)}{2})^2 \quad \text{for $d=1$} \\   \sin(\frac{\state(t,\tau)}{2})^2  \quad \text{for $ d=0$} \end{cases}
\end{align}

where  $ \state(t,\tau) \equiv \int_{t}^{t+\tau} \delta \omega_A(t') dt'$ at time $t$ for short Ramsey times, $\tau$.
\\
\\
The key assumption being made is that the measurement action over $\tau$ timescales is much faster than the slow time dependence of stochastic detuning,  $\delta \omega_A(t)$. The procedure is repeated to obtain the next measurement, at $t + \Delta t$ for $\Delta t >> \tau$, thereby discretising the continuous time dephasing noise process in equal time-steps $\Delta t$. Since $\tau$ is fixed by experiment, and $t = n \Delta t$ for the $n^{th}$ measurement, we use shorthand $\state(t,\tau) \equiv \state_n$, where $n$ indexes time, $t$.
% For the discussion below, it is assumed that $\state_n$ is zero mean, Gaussian distributed.
\\
\\
We note that $P(d_n | \state_n, \tau, \Delta t, n)$ defines the Bayes likelihood for each single shot measurement outcome and one can consider the statistical inverse problem of estimating $\state_n$. Dropping $\Delta t, n$ in the conditional notation, it follows from Bayes Rule for the full measurement record, $\mathcal{D} \equiv \{d_n\}$ that:

\begin{align}
P(\state_n| \mathcal{D}_n, \tau)  \propto P(d_n | \state_n, \tau) P(\state_n | \mathcal{D}_{n-1}, \tau)
\end{align}

In most cases, the above problem is generally intractable and numerical approaches to building the aposteriori distribution apply. In standard numerical approaches, Markovianity is often assumed. If $\state$ was Markovian, then it would be straightforward to write a `dynamical model' to enable sequential approximations to non Bayesian filtering, by resampling according to $P(\state_{n+1} | \state_n)$ at each time step:
\begin{align}
P(\state_{n+1} | \mathcal{D}_{n}, \tau) = \int P(\state_{n+1} | \state_n) P(\state_n | \mathcal{D}_{n}, \tau) d\state_n
\end{align}

An example of tracking a Markovian stochastic time dependent detuning with a non linear measurement model identical to ours is considered in \cite{wiebe2015bayesian}. An example of Bayesian inference via particle filtering for non Markovian state space models have been considered in engineering applications (e.g. \cite{jacob2016}). Modifications to particle filtering approaches for non-Markovian dynamics in non-linear systems is beyond the scope of this work.  
\\
\\
Instead, we will introduce a classical state space approach for handling for non linear, quantised measurements for non-Markovian systems in the last section of this paper. 
% Experimentally, the pre-processed regime can be achieved in two ways:
% % Gaussian process regression (batch) and Kalman filtering (recursive) are examples of linear Bayesian techniques and can be theoretically equivalent for a certain choice of regimes. We use suitably modified implementations of these techniques to learn stochastic dynamics in our problem.
% % \\
% % \\
% % The pre-processed data refers to a measurement record as the set of estimates of $\{\hat{\state}_n\}$, potentially corrupted by additional Gaussian white measurement noise, and linear Bayesian filtering schemes commence subsequently.

\subsubsection{Approximately Linear Measurement Model for Pre-processed Data}
To yield an approximately linear Bayesian regime,  experimental data is pre-processed such that the measurement record is the a set of estimates of $\{\hat{\state}_n\}$, not $\{d_n\}$.  Statistical inference from single shots (or cloud averaged) outcomes is not incorporated for filtering and prediction algorithm design. This yields a linear measurement model such that we may focus on the reconstruction of non Markovian $\{\state_n\}$. 

% Experimentally, this means:
% \begin{itemize}
% \item Average single shots over many parallel runs for different wait times to obtain $\hat{P}(d | \state_n, \tau, t)$ v. $\tau$. One may deduce $\state_n$ from a Fourier transform of $P(d | \state_n, \tau, t)$ v. $\tau$; or condense all pre-processing into Bayesian treatment of single shots as in CITE, with the assumption that $\state_n$ is constant over $M\tau$ measurements performed, $M\tau << \Delta t$.
% \item Perform $M$ single shots with the same wait time, $\tau$,  and $\Delta t$ apart, such that drifts in probability are due to $\state_n$, not $\tau$. This binary signal can be decimation filtered to yield non binary $\hat{P}(d | \state_n, \tau, t)$ v $t$, and the inversion $\hat{\state}_n = \cos^{-1}(1- 2\hat{P}(d | \state_n, \tau, t))$ holds as long as $\state_n  < \pi $.
% \end{itemize}

% Here, we simplify the measurement model to be approximately linear, and we focus on the design of the dynamical model, $\Phi$, in enabling state estimation and prediction.  The interpretation of dynamics is a deterministic coloring process for white, Gaussian noise inputs. In this approximately linear Bayesian regime, we refer to Gaussian Process Regression and Kalman frameworks subsequently. The Kalman frameworks employ (a) autoregressive processes (AR) and (b) a collection of oscillators to reconstruct $\Phi$. 



% Here, classical statistical theory for amplitude quantisation is adapted to yield a `quantised' Kalman Filter. 

% \subsubsection{State Space Non Linear, Quantised Measurement Model}

% In state space view, we consider $P(\state_n| d, \tau, t)$ a continuous time, continuous amplitude signal. In addition to time domain discretisation of $P(\state_n| d, \tau, t)$ via sequential measurements, we may also quantise the amplitude of such a signal according to classical quantisation theory in \cite{widrow1996}. While a general theory of quantised amplitudes allows an arbitrary quantisation of $m$ levels, we consider the case where $m=1$, namely, a one bit classical quantisation of the true $P(d | \state_n, \tau, t)$. 
% \\
% \\
% Let $ \state_n$ be a true unobserved state with a dynamical model $\Phi_n$ and white Gaussian process noise $w_n$; $h(\state_n) \equiv P(d | \state_n, \tau, t) $ be a non linear measurement action corrupted by white Gaussian noise $v_n$, and $\mathcal{Q}$ be a one bit quantiser. Here, $\Phi_n$ is a deterministic transformation which colors the originally white noise input, $w_n$, and we investigate the design of $\Phi_n$ subsequently. 
% \\
% \\
% Then the state space equations are:
% \begin{align}
% \state_n &= \Phi_{n-1} \state_{n-1} + \Gamma_{n-1} w_{n-1} \\
% y_n &= h(\state_n) + v_n 
% \end{align}
% And quantisation yields: 
% \begin{align}
% d_n &= \mathcal{Q}(y_n) = \mathcal{Q}(h(\state_n) + v_n)
% \end{align}

% For the purposes of simulating a data record $\{ d_n, n = 0, 1, ... N \}$, the quantiser $\mathcal{Q}$ is effectively drawing from a binomial distribution where the bias of coin flip at $t$ is given by a true (engineered) $P(d | \state_n, \tau, t)$. In experiment, $\mathcal{Q}$ is a naturally quantised physical sensor, namely, a qubit. 
% \\
% \\
% The discussion above constitutes as a non-linear, quantised measurement model for state space methods considered in this paper. Particle filtering performed significantly better than extended Kalman filtering frameworks for one bit quantisers defined over non linear Markovian models, as compared in \cite{karlsson2005}. In this paper, we will use methodology in  \cite{karlsson2005} to apply the one bit quantiser for our problem.  

\subsection{Stochastic Properties of an Arbitrary Dephasing Process} \label{sec:main:part2}

In the absence of a theoretical dynamical model for $\state$, we impose properties on $\{ \state_n \}$ that enables a reconstruction in a mean square sense. The purpose of our numerical analysis in this section is to test whether our mean square approximate reconstructions of $\{ \state_n \}$ enable experimentally sensible time domain predictions. 
\\
\\
We assume $\{ \state_n \}$  is non Markovian, covariance stationary and mean square ergodic, that is, a single realisation of the process $\{ \state_n, t = \Delta t n, n = 0, 1, ... N\}$ is drawn from a band-limited power spectral density of arbitrary non-Markovian shape. 
% The indistingusihablity of environmental dephasing and engineered dephasing noise is derived in \cite{ball2016, soare2014}, enabling experimental verification of numerical analysis considered in this paper. 
\\
\\
For numerical simulations and experimental noise engineering, let $\state$ take the specific form outlined in \cite{soare2014} for \cref{sec:main:part3,sec:main:part4}. In discrete time, this is a harmonic sum with random phase information and spectral density shape controlled by $p$:
\begin{align}
\state_n &\equiv \alpha \omega_0 \sum_{j=1}^{J} j F(j)\cos(\omega_j t + \psi_j) \\
F(j) & \equiv j^{\frac{p}{2} -1}\\
\omega_j & \equiv \omega_0 j, j = 1, 2, ..., J \\
t &= \Delta t n,  n = 0, 1, ...N  \\ 
\psi_j & \sim U(-\pi, \pi) 
\end{align}
It is straightforward to show that the analytical form of true (engineered) $\state_n$ is covariance stationary, mean square ergodic and satisfies Gaussianity for large but finite $J$ (numerically, $J > 15$). The free parameters, $\alpha, \omega_0, J, p$, are true noise parameters which any prediction algorithm cannot know beforehand. Where equivalent quantities are being designed as part of learning algorithms, the notation is retained but with a knowledge that algorithmic design and learning is blind to the true choice of parameters. 
\\
\\
Other noise engineering methods satisfying assumptions about $f$ are applicable, for example, a true autoregressive process is generated and studied in \cref{sec:main:part5}.

% The reconstruction of any covariance stationary process is enabled in the mean square limit by spectral decomposition using harmonic sums; and by Wold's decomposition, via autoregressive processes of finite order. Both approaches yield simple analytical formulae for the autocorrelation function and the power spectral density. The reverse is also true: under Bochner's theorum, an autocorrelation function (equivalently, a `kernel') can be arbitrary designed and, with certain conditions, will describe a valid covariance stationary process that we interpret as the approximate reconstruction of our true dephasing noise field. 

\subsection{Approximately Linear Bayesian Models for Filtering and Prediction}  \label{sec:main:part3}
In this section, we review effective methods for reconstructing an arbitrary, non-Markovian $\state$ in a simple linear measurement regime. 
\\
\\
For a true $\state_n$ corrupted by Gaussian measurement noise, $v_n$, the our model is:
\begin{align}
y_n &= \state_n + v_n \\
v_n & \sim \mathcal{N}(0, R) \quad \forall n
\end{align}
The above applies for all linear Bayesian models but incorporation of the dynamics of $\state_n$ varies between Kalman Filtering (KF) and Gaussian Process Regression (GPR) techniques. 
\\
\\
In KF framework, the state space model for $\state$ with a linear measurement action, $H_n$, process noise $w_n$, a noise features matrix $\Gamma_n$ and a linear dynamical model $\Phi_n$  is :
\begin{align}
\state_n & \equiv H_n x_n  \label{eqn:KF:lin_measurement}\\
x_n & = \Phi_n x_{n-1} + \Gamma_n w_n \label{eqn:KF:dynamics} \\
w_n & \sim \mathcal{N}(0, \sigma^2) \quad \forall n
\end{align}
Based on these definitions, the standard equations for state propagation and prediction in Kalman filtering are detailed in Supplementary Information. 
\\
\\
In GPR, $\state$ is interpreted as a Gaussian distribution over random processes, $P_\state$, where each element of the covariance matrix of this distribution, $\Sigma_\state$ is specified by the covariance function, $R(v)$, or equivalently a `kernel':
\begin{align}
\state & \sim P_\state(\mu_\state,\Sigma_\state ) \\
\Sigma_\state & \equiv K(N,N) \\
y & \sim P_y(\mu_y,\Sigma_\state + R ) \\
\Sigma_\state^{i,j} & \equiv K(n_i, n_j) \equiv R(\Delta t(n_i - n_j))
\end{align}
The approximate representations of $\state$ realisations informs the structure of learning algorithms, namely the design of $\{\Phi, R, \Gamma, H, \sigma \} $ in KF and choice of $R(v)$ and its hyper-parameters in GPR. The additional notation $K(N,N)$ is ubitiquous in Gaussian process regression and we include it to help provide visibility of the time domain set of points over which the covariance function is being calculated. 
\\
\\
In detailed algorithmic descriptions to follow, let $N = N_{T} + N_{P}$, where $N_{T}$ denotes the total number of measurements (for training) and $N_{P}$ denotes the total number of time steps where prediction outcomes beyond the measurement record are desired. Hence, state estimation refers to one-step ahead KF projections, or GPR projections at test points within the measurement record, namely $n \leq N_T$. `Prediction' refers to KF projections with zero gain, or GPR projections at test points beyond the measurement record, namely, $n \in [N_T, N_T + N_P], N_P > 1$. The key metric for comparing algorithmic performance is RMS prediction error, namely, Bayes Risk incurred during $n \in [N_T, N_T + N_P]$.


% For linear Bayesian filtering techniques, this means:

% \begin{itemize}
% \item In Gaussian process regression, our approximation is that aposteriori distribution remains Gaussian if the prior is Gaussian. In particular, our prior is a distribution over Gaussian processes (dynamical models) that best capture the true evolution of our stochastic process, $\bold{\state} \in \mathcal{N}(\bold{\mu}, \bold{\Sigma})$, with $\bold{\state}$ a vector whose elements are Gaussian random variables indexed by time. Hence, our approximation tests the design of the covariance function to specify elements of $\bold{\Sigma}$ which, if designed appropriately, captures all correlations in the measurement record. 
% \item For state space models, our approximation is that $h(\state_n) \to H\state_n$, where $H$ is a linear transformation, and no quantisation $\mathcal{Q}$ occurs. This approximation will test our design of stochastic dynamics, $\Phi_n$ in yielding non Markovian colored noise.
% \end{itemize}

% \subsection{Stochastic Dynamics with Linear Measurements} \label{sec:main:part2}
 

\subsubsection{Least Squares (LSF) and Autoregressive Kalman (AKF) Filters}

Our first set of algorithms test autoregressive processes to reconstruct unknown dynamics for $\{ \state_n \}$. Firstly, autoregressive dynamics are incorporated into a Least Squares filtering (LSF) algorithm in \cite{mavadia2017}. Secondly, we recast a trained LSF into a recursive Kalman filter (AKF) and theoretically, this should enable increased measurement noise filtering.
\\
\\
By Wold's Decomposition, any zero mean covariance stationary process has a representation as an Moving Average process of infinite order, MA($\infty$). Any MA($\infty$) (or their generalisation via autoregressive moving average models ARMA($q, p$)) can be approximated in the mean square limit by an autoregressive process of order $q_m$, AR($q_m$), where convergence to true process falls with $m$. If a true zero mean ${f_n}$ is well approximated as an AR($q$) process, $w$ a zero mean white noise process, then we define a dynamical model, $\Phi$ in terms of a lag operator $L$:
\begin{align}
\Phi(L) f_n & = w_n \\
L^r: f_n &\mapsto f_{n-r} \quad \forall r \leq n \quad \text{(lag operator)} \\
\Phi(L) & \equiv  1 - \phi_1 L - \phi_2 L^2 - ... - \phi_q L^q 
\end{align}
If the roots of polynomial equation defined by  $\Phi(L)$ lie outside the unit circle, then $\Phi(L)^{-1}$ exists such that $f_n = \Phi(L)^{-1} w_n$, and the weights $\{\phi_r: r = 1, ... q\}$ can be used to extract the spectral density and long run variance as:
\begin{align}
S(\omega) & = \frac{\sigma^2}{2 \pi }\frac{1}{|\Phi(e^{-i\omega})|^2} \label{eqn:sec:ap_ssp_ar_spectden}\\
\Sigma & = \frac{\sigma^2}{\Phi(1)^2}
\end{align}
The Least Squares Filter applies gradient descent algorithm to directly learn the weights, $\{\phi_j\}, j = 1, ... , q $, of the previous $q$ past measurements. For $n=1$, the LS Filter in \cite{mavadia2017} implements effectively an AR($q$) process. 
\\
\\
Any AR($q$) process can be recast (non-uniquely) into state space form easily by substituting the following definitions into \cref{eqn:KF:lin_measurement,eqn:KF:dynamics}:
\begin{align}
f_n & = (\phi_1 L +  ... + \phi_q L^q) f_n + w_n \\
x_n & \equiv  \begin{bmatrix} f_{n} \hdots f_{n-q+1} \end{bmatrix}^T \\
\Gamma_n w_n & \equiv \begin{bmatrix} w_{n} 0 \hdots 0 \end{bmatrix}^T \\
\Phi_{AKF} & \equiv 
\begin{bmatrix}
\phi_1 & \phi_2 & \hdots & \phi_{q-1} & \phi_q \\ 
1 & 0 & \hdots & 0 & 0 \\  
0 & 1 & \ddots & \vdots & \vdots \\ 
0 & 0 & \ddots & 0 & 0 \\ 
0 & 0 & \hdots & 1 & 0 
\end{bmatrix} \quad \forall n \label{eqn:akf_Phi} \\
H & \equiv \begin{bmatrix} 1 0 \hdots 0 \end{bmatrix} \quad \forall n 
\end{align}
The matrix $\Phi_{AKF}$ is the dynamical model used to recursively propagate the unknown state during state estimation in the AKF. The ${\phi_i}$ in $\Phi_{AKF}$ are ideally the output of a maximum likelihood optimisation problem. We side-step this optimisation problem by using ${\phi_i}$ from LSF to define $\Phi_{AKF}$. Kalman noise parameters ($\sigma^2, R$) were auto-tuned using a Bayes Risk optimisation procedure detailed in \cref{sec:main:part4}.
\\
\\
We test two theoretical implications: firstly, using identical autoregressive coefficients, we expect  AKF to outperform LSF in high noise regimes where the Kalman framework should enable stronger measurement noise filtering. These results are shown in \cref{fig:LSFvAKF}. Secondly, a learned spectrum $S(\omega)$ can be reconstructed using LSF coefficients and the tuned Kalman parameter, $\sigma^2$. These reconstructions are used subsequently in \cref{fig:LKFFB}.
% \begin{widetext}
 \begin{figure}[h] 
 	\caption{placeholder}
	\includegraphics[scale=0.6]{poster_2017_07_08_four_COMPILED.pdf}
	\label{fig:LSFvAKF}
\end{figure}
% \end{widetext}

\subsubsection{GPR with a Periodic Kernel}

Our second reconstruction uses GPR and encodes the covariance function for trignometric polynomials. As with AR($q$) processes, a sequence generated by trigonometric polynomials also enable an approximate reconstruction of any covariance stationary process in the mean square limit.
\\
\\
We detail the general GPR predictive framework as follows. Let indices $n,m \in N_T \equiv [0, N_T]$ denote training points, and $n^*,m^* \in N^*$ denote testing points in machine learning language. We now define the joint distribution $P(y,\state^*)$, where $\state^*$ is our prediction for the true process at test points: 
\begin{align}
\begin{bmatrix} \state^* \\y \end{bmatrix} & \sim \mathcal{N} (\begin{bmatrix} \mu_{\state^*} \\ \mu_y
\end{bmatrix} , \begin{bmatrix}   K(N^*,N^*)&K(N_T,N^*) \\ K(N^*,N_T) & K(N_T,N_T) + R \end{bmatrix} )
\end{align}
Following \cite{rasmussen2006}, the moments of the conditional distribution $P(\state^*|y)$ can be derived from the joint distribution $P(y,\state^*)$ via standard Gaussian identities:
\begin{align}
\ex{\state^*|y} &= \mu_\state + K(N^*,N_T)(K(N_T,N_T) + R )^{-1} (y - \mu_y) \\
\text{cov}[\state^*|y] &= K(N^*,N^*) \nonumber \\
& - K(N^*,N_T)(K(N_T, N_T) + R)^{-1}K(N_T,N^*) 
\end{align}
The aposteriori mean, $\ex{\state^*|y}$ is interpreted as a sequence of predictions of $f$ at test points $N^*$. Hence, the efficacy of GPR methods depends on appropriate apriori design of the kernel $K(x,y) \equiv R(x-y)$. 
\\
\\
The specific GPR reconstruction we would like to focus on in this paper corresponds to a trigonometric polynomial representation of a covariance stationary process. Any zero mean covariance stationary process can be represented as the mean square limit of trigonometric polynomials, and this reconstruction is defined as follows. Let $A_j, B_j$ be independent, identically distributed random variables with mean,$\mu_j$, and variance, $\sigma_j^2$. Let the abstract index be $j \in J, j = 0,1,2... $, and define $\state$ as:
\begin{align}
\state_n &= \sum_{j=0}^{J} A_j \cos(\omega_j n) + B_j \sin(\omega_j n) \label{eqn:E_f_nrigpolynomial} \\
\ex{A_j} & = \ex{B_j} =0 \forall j \in J \\
\ex{A_k A_j} &= \ex{B_k B_j} = \sigma_j^2 \delta_{j,k} \quad \forall k,j \in J\\
\ex{A_k B_j} &= 0 \quad  \forall k,j \in J \label{eqn:karlin_nrig_cross} \\
 \ex{\state_n} &= 0 \\
 \ex{\state_n \state_{n+v}} &= \sigma^2 \sum_j^{J}  p_j \cos(\omega_jv), p_j = \frac{\sigma_j^2}{\sum_j \sigma_j^2} = \frac{\sigma_j^2}{\sigma^2}\\
&\equiv R(v)
\end{align}
Consider $\omega_j =0$. For this frequency, \{$\state_n$\} behaves like a perfectly correlated random variable (DC term). Consider $J \to \infty$, with all non zero $\omega_j$ equally represented, hence $p_j \to 0$ . Then we expect $R(v)$ is nearly zero for all $v\neq 0$ and will $R(v)=\sigma^2$ for $v=0$, that is, \{$\state_n$\} is Markovian. For $J$ finite, we represent a colored noise process.
\\
\\
The sine squared exponential kernel in GPR methods is defined as:
\begin{align}
R(v) &\equiv \sigma^2 \exp (- \frac{2\sin^2(\frac{\omega_0 v}{2})}{l^2}) \\
 &=  \sigma^2 \exp (- \frac{1}{l^2}) \sum_{n = 0}^{\infty} \frac{1}{n!} \frac{\cos^n(\omega_0 v)}{l^{2n}} 
\end{align}
We follow \cite{solin2014} to  show that a sine squared exponential kernel represents an infinite basis of oscillators, and reduces to the covariance function describing trigonometric polynomials if spectral components are truncated to a finite number, $J$. We apply cosine power reduction formulas and recollect terms to recover the basic structure of a covariance function for trigonometric polynomials with $J$ terms:
\begin{align}
R(v) &- \sigma^2 p_{0,J}  = \sigma^2 \sum_{j=0}^{J} p_{j,J} \cos(j\omega_0 v)\\
p_{0,J} & \equiv \frac{1}{2} \exp (- \frac{1}{l^2}) \sum_{\alpha = 0}^{\alpha = \lfloor\frac{J}{2}\rfloor} \frac{1}{(2l^2)^{(2\alpha)}} \frac{1}{(2\alpha)!} \binom{2\alpha}{\alpha} \label{eqn:p0J}\\
p_{j,J} & \equiv \exp (- \frac{1}{l^2}) \sum_{\beta = 0}^{\beta = \lfloor\frac{J-j}{2}\rfloor} \frac{2}{(2l^2)^{(j + 2\beta)}} \frac{1}{(j + 2\beta)!} \binom{j + 2\beta}{\beta} \label{eqn:pjJ} \\
\omega_0 &\equiv \frac{\omega_j}{j}, j \in \{0, 1,..., J\} 
\end{align}
We note that the sine-squared kernel is summarised by two key hyper-parameters, $\omega_0, l$ and we use physical arguments about sampling to approximate their values.
 \begin{figure} 
 	\caption{We track a hidden true process for the case where observations can, theoretically, be perfectly projected onto the sine squared exponential kernel, enabling near perfect learning for low measurement noise. The longest correlation length encoded in the data, $N_T \Delta t$, corresponds to a fundamental period inside the sine-squared exponential kernel, $p = \frac{1}{\omega_0 }$, which enables exact learning of Fourier amplitudes if optimally tuned $p \approx N_t,  \Delta t \equiv 1$, and small deviations arise from low additive measurement noise. When perfect learning is theoretically enabled and when $p \approx N_T$, phase information is less relevant to predictive performance and GPR predictions show high fidelity [top left]. For $p = N_T + \kappa, \kappa >> 0$, the algorithm predicts zero for correlation lengths $> N_T$ not seen in the data [bottom left]. A `revival'  at $t=p$ suggests the learned Fourier pattern is repeated at the fundamental period. In low measurement noise regimes, this implies predictions will show high fidelity with the true process at the beginning of the measurement record [bottom right].}
	\includegraphics[scale=0.8]{poster_2017_07_08_five_COMPILED.pdf} 
	\label{fig:GPR}
\end{figure}
\\
\\
Without active phase tracking, \cref{fig:GPR} depicts that GPR with a sine-squared exponential kernel repeats a Fourier domain learned pattern with a time domain periodicity $p$ and predicts zero for long correlation lengths not physically encoded in the dataset. In cases where perfect projection is not possible; there is high measurement noise; and/or the specified prediction horizon is too short relative to $\kappa$ to observe revivals (refer \cref{fig:GPR}), these numerical artefacts may be misinterpreted as `best available' forward predictions. While interpolation and pattern recognition may be enhanced by the ability to project data onto an infinite Fourier domain comb, predictions beyond the measurement record have limited interpretation without active phase correction. In particular, while one can recover a theoretically equivalent stacked state space model from the Periodic kernel, the arbitrary choice of truncation at $J$ makes it unlikely that one could somehow extract amplitude and phase information from analytical expressions of coefficients $p_{j,J}$. 
\\
\\
We briefly summarise kernel choices excluded from our analysis, including popular kernel choices such as the Gaussian kernel, rational quadratic kernel, Matern family and Gaussian spectral mixtures. These exclusions are based on kernel properties: namely, the shape of a kernel and / or the number of unknown hyper-parameters which require automated tuning. For example, an arbitrary mixture of zero mean Gaussian Kernels will probe an arbitrary area around zero in the Fourier domain. While such kernels capture the continuity assumption ubitiquous in machine learning, they appear structurally disadvantaged in probing a dephasing noise process of an arbitrary power spectral density (e.g. ohmic noise). Matern kernels of order $q + 1/2$ correspond to AR($q$) processes and we do not duplicate our state space analysis here. A class of GPR methods, namely, spectral mixture kernels and sparse spectrum approximation using GPR have been explored in \cite{wilson2013, quia2010}. These require efficient optimisation procedures to learn a large number of unknown kernel parameters. A detailed analysis of the application of these spectral methods for forward prediction (not pattern learning) with limited computational resources is beyond the scope of this paper.  

\subsubsection{Liska Kalman Filter with Finite Oscillator Basis}

We build on the basis of oscillator approach in GPR (above) and we consider explicitly tracking phases and amplitudes for a finite basis of oscillators We return to the Kalman framework and we project our measurement record on a basis of $J$ oscillators with fixed $\{ \omega_0 j, j = 1, \hdots, J \}$, while learning the amplitude and phase of these oscillators using a Kalman filter developed in \cite{livska2007}. We refer to this implementation as the Liska Kalman Filter (LKF).
\\
\\
The Liska Kalman approach departs from classical Kalman filtering in two key ways.  Firstly, the real and imaginary parts of the Kalman state are estimated simultaneously in order to track instantaneous amplitudes ($\norm{x^j_n}$) and phases ($\theta_{x^j_n}$) explicitly. Secondly, the process noise feature matrix $\Gamma_{n}$ depends on state estimates and hence, state variance propagation via the Ricatti equation cannot be decoupled from state estimation, as in traditional Kalman implementations. We depart from LKF in \cite{livska2007} by specifying a fixed basis of oscillators (LKFFB) to enable an arbitrarily high reconstruction of $f$ by an arbitrarily high choice of the ratio $J/\omega_0$. The following substitutions into \cref{eqn:KF:dynamics,eqn:KF:lin_measurement}, and the choice of fixed basis specified in Supplementary information, defines the LKFFB algorithm:
\begin{align}
x_n & \equiv \begin{bmatrix} x^{1}_{n} \hdots x^{j}_{n} \hdots x^{J}_{n} \end{bmatrix} \\
x^{j,1}_{n} & \equiv \text{estimates real $f$ component for $\omega_j$} \\
x^{j,2}_{n} & \equiv \text{estimates imaginary $f$ component for $\omega_j$} \\
x^j_n &\equiv \begin{bmatrix} x^{j,1}_{n} \\ x^{j,2}_{n} \\ \end{bmatrix} \equiv \begin{bmatrix} A^j_{n} \\ B^j_{n}  \end{bmatrix}\\
\norm{x^j_n} & \equiv \sqrt{(A^j_{n})^2 + (B^j_{n})^2} \\
\theta_{x^j_n} & \equiv \tan{\frac{B^j_{n}}{A^j_{n}}} \\
\Gamma_{n-1} &\equiv \Phi_{n-1}\frac{x_{n-1}}{\norm{x_{n-1}}}  \\
H & \equiv \begin{bmatrix} 1 0 \hdots 1 0 \hdots 1 0 \end{bmatrix}
\end{align}
By using block diagonal matrices to stack $\Phi(j \omega_0 \Delta t) $ for all $\omega_j$, we obtain the full dynamical model $\Phi_n$:
\begin{align}
\Phi_{n} & \equiv \begin{bmatrix} 
\Phi(\omega_0 \Delta t)\hdots 0  \\ 
 \hdots \Phi(j\omega_0 \Delta t) \hdots \\
0 \hdots \Phi(J \omega_0 \Delta t)  \end{bmatrix}\\ 
\Phi(j \omega_0 \Delta t) &\equiv \begin{bmatrix} \cos(j \omega_0 \Delta t) & -\sin(j \omega_0 \Delta t) \\ \sin(j \omega_0 \Delta t) & \cos(j \omega_0 \Delta t) \\ \end{bmatrix} \label{eqn:ap_approxSP:LKFFB_Phi} 
\end{align}
The interpretation is that $\Phi_n$ deterministically correlates the white noise input $w_n$ at each time-step. In particular, one obtains the same mathematical structure if one defined a stack of Markovian stochastic processes on a circle, as in \cite{karlin2012}, if we assume that resonators are statistically independent and each resonator has the following temporal properties (dropping the $j$ below):
\begin{align}
\ex{w_n} &= 0 \\
\ex{w_n,w_m} &= \sigma^2 \delta_{n,m} \quad \text{(scalar Gaussian R.V)} \\
\ex{A_0} &=\ex{B_0} = 0 \\
\ex{A_n B_m} &= 0 \\
\ex{A_n A_m} &= \ex{B_n B_m} = \sigma_j^2 \delta_{n,m} \quad \text{given $\omega_j$} \\
\ex{w_n A_m} &= \ex{w_n B_m} \equiv 0 \quad  \forall n, m 
\end{align}
This yields state means and covariances for each resonator as:
\begin{align}
\ex{x_n}_j &= 0  \\
\ex{x_n x_m^T}_j & = \sigma^2_j \delta_{n,m} \begin{bmatrix} 
1 & 0 \\ 
0 & 1  \\
\end{bmatrix} \label{eqn:cov_kf_nerm4}
\end{align}
The cross-correlation terms for $\ex{x_n x_m^T}_j$ disappear under the temporal correlation functions for $A_i, B_j, w_k$ - namely, if we assume $n \geq m$, then states at $m-1$ at most have a $w_{n-2}$ term (for the case $n=m$) and cannot be correlated with a future noise term $w_{n-1}$.  Proofs are deferred to Supplementary Information. 
\\
\\
We confirm our intuition through extensive numerical analysis that Kalman predictions obtained by the following two methods are equivalent: (a) setting the Kalman gain to zero and recursively propagating forward in time, or (b) extracting learned amplitudes and phases for each basis oscillator and reconstructing $f$ in a single calculation using harmonic sums. These two prediction procedures yield state predictions in very close agreement to each other for all cases where number of recursion time steps are `short' such that associativity of numerical matrix multiplication remains within a few orders of floating point precision. Indeed, numerical discrepancies in state predictions arising from the precision of different computational operations are often extremely small relative to other sources of error (e.g. measurement noise) but the implication for favouring a single calculation in method (b) over a recursion in method (a) would depend on a specific application.
\\
\\
 \begin{figure}[h] 
	\caption{Panel A: Time Domain Predictions. A single realisation of true $\state$ and measurements for 10\% noise threshold yield predictions commencing at $t=0$ from LSF, AKF and LKFFB  [left column]. The Bayes Risk for 75 runs is normalised against predicting the mean (zero) [right column]. Two regimes are considered: perfect projection on LKFFB basis is theoretically possible [top] and in an imperfect learning regime [bottom]. LKFFB outperforms AKF / LSF in perfect learning [top right] where $f$ is learned near perfectly; AKF/LSF outperform LKFFB if perfect learning is perturbed even at $\leq 10\% \omega_0^B$ (LKFFB basis spacing). Panel B: Spectrum Reconstruction. True power spectral density of $\state$ is depicted alongside $S(w)$ in \cref{eqn:sec:ap_ssp_ar_spectden} overlayed using trained parameters from AKF/LSF. LKFFB $\norm{x^j_n}$ v. $\omega_j$ are extracted from a single run on an $f$ realisation.  Oversampling of true $\state$ is reduced (without physical aliasing) from top-right to bottom-left in an imperfect learning regime. LKFFB $\norm{x^j_n}$ outperforms AKF $S(w)$ in spectrum reconstruction of $f$ unless LKFFB basis is does not span $f$ bandwidth. Poor time domain LKFFB predictive performance in imperfect regimes suggests tracking instantaneous phase $\theta_{x^j_n}$ remains diffcult compared to tracking $\norm{x^j_n}$. }
	\includegraphics[scale=0.8]{poster_2017_07_08_two_v2_COMPILED.pdf} 
	\label{fig:LKFFB}
\end{figure} 
In \cref{fig:LKFFB}, LKFFB basis frequencies are chosen such that perfect projection of measurement record  is theoretically enabled and we are able to reconstruct dynamics for any realisation of dephasing noise perfectly [Panel A, top left]. LKFFB outperforms AKF for state predictions by achieving low Bayes Risk for a long prediction horizon [Panel A, top right]. In realistic operational scenarios, perfect learning is impossible. For all algorithms in imperfect learning regimes, the length of a prediction horizon for which algorithms outperform predicting the mean is an arbitrary number which scales with the oversampling ratio, namely, the ratio between experimental sampling rate and true noise cutoff: $2 \pi / \Delta t J^{TRUE} \omega_0^{TRUE}$. Nevertheless, predictive performance of LKFFB drops dramatically when perfect projection is not possible [Panel A, bottom row]. Meanwhile, LKFFB spectrum reconstruction is robust compared to AKF/LSF even as oversampling is reduced in imperfect learning scenarios. Since LKFFB predictive performance relies on both amplitude and phase information, \cref{fig:LKFFB} Panel B suggests that active instantaneous phase tracking is relatively more problematic that amplitude learning for LKFFB in yielding sensible time domain predictions.
\\
\\
In \cref{fig:optimisation} [top], we probe model robustness of Kalman techniques by comparing the following pathologies against a perfect learning regime: imperfect projection on LKFFB basis; imperfect learning with increased number of true noise components relative to the spectral resolution of the LKFFB basis spacing; and an LKFFB basis which does not sufficiently span the true noise bandwidth. In all such cases, AKF outperforms LKFFB in predictive performance and AKF has a model robust design for realistic operational scenarios. 

\FloatBarrier
\subsection{Optimisation Procedure for Autotuning Algorithms} \label{sec:main:part4}

In the absence of apriori knowledge of statistics of environmental dephasing, we require an optimisation procedure to tune Kalman design parameters ($\sigma^2, R$). For the simulations considered in this paper, we calculate the Bayes Risk - the mean square distance between truth and prediction sequences for different realisations of true $\state$ and different noisy datasets $\mathcal{D}$:
\begin{align}
L_{BR}(\state) & \equiv \sum^{N}\ex{(\state_n - H_n\hat{x_n})^2}_{\state,\mathcal{D}} \label{eqn:sec:ap_opt_LossBR}
\end{align}
State estimation risk is Bayes Risk incurred during $n \in [0, N_T]$; prediction risk is the Bayes Risk incurred during $n \in [N_T, N_T + N_P]$. If the lowest state estimation risk is incurred for low loss regions during prediction, then the KF filter is sensibly tuned. If an overlap of low loss regions does not exist, then the optimisation problem is deemed `broken' as algorithm training will not improve prediction performance. 
\\
\\
In \cref{fig:optimisation}, we add pathological features to our learning problem. The deterioration in performance of LKFFB with respect to AKF/LSF algorithms is mirrored by the shrinking area of low loss overlap regions for LKFFB. The optimisation problem breaks down entirely for the extremely pathological case where low loss state estimation and prediction regions are disjoint. In constrast, the optimisation problem for AKF does not change dramatically across all cases. 
\\
\\
For GPR, we consider the shortest and longest physical correlation information captured by our data ($\Delta t$, $\Delta t N_T$) to approximate initial conditions for the lengthscale and periodicity parameters in the Periodic Kernel. We use \cite{gpy2014} Python software to solve a variety of optimisation procedures over all kernel design parameters and noise covariance strengths; results omitted as interpretation of ensuing predictions are limited.  
\begin{widetext}
\begin{figure}[h] 
	\caption{placeholder}
	\includegraphics[scale=0.8]{poster_2017_07_08_three_v2_COMPILED.pdf} 
	\label{fig:optimisation}
\end{figure}
\end{widetext}
\FloatBarrier

\subsection{Stochastic Dynamics with Non-Linear, Quantised Measurements} \label{sec:main:part5}

In this section, we freeze the design of non-Markovian reconstruction using finite order AR($q$) processes, and we ask whether we can incorporate a non linear measurement model and quantised measurement outcomes in a KF state space model.  While it is not possible to construct a sensible comparison of predictive performance with prior work (AKF, LKFFB), we demonstrate that linearisation and quantisation errors significantly effect filtering and prediction performance even if true dynamics are well approximated by $\Phi$ for our application. With these introductory comments, we specify the structure of the system under consideration, and the resulting adaptations to AKF such that it yields the Quantised Kalman Filter (QKF) in this study.
\\
\\
We consider a state space (linearised Kalman) framework with a non linear measurement action. We retain $x, \Phi, \Gamma$ of AKF and we redefine the measurement action in state space form as:
\begin{align}
y_n &= h(x_n[0]) + v_n = h(\state_n) + v_n \\
z_n & \equiv  h(\state_n) \equiv  0.5\cos(\state_{n}) \\
H_n &\equiv \frac{d h(\state_n)}{d\state_n} =  -0.5\sin(\state_{n})
\end{align}
The likelihood function is shifted by $\frac{1}{2}$ to yield $z_n$ and this ensures that our state space system represents zero mean processes. Unlike AKF and LKFFB, QKF has a time varying measurement Jacobian matrix $H_n$ that is used for state variance propagation, thereby affecting the calculation of the Kalman gain and state estimates at each timestep.
\\
\\
Next, we introduce a true quantiser $\mathcal{Q}$ which yields $\{d_n\}$ binary measurement outcomes.  
\begin{align}
d_n &= \mathcal{Q}(y_n) = \mathcal{Q}(z_n + v_n) 
%\mathcal{Q}(y_n): &\quad  d_n \sim \mathcal{B}(d_n=1; n=1, p= z_n + 0.5 | z_n, t) 
\end{align}
Statistical theory for classical amplitude quantisation of a continuous signal into $m$ levels is considered in \cite{karlsson2005}. We focus our attention to $m=1$ (single bit quantiser). We depart from \cite{karlsson2005} by altering the quantisation to be a biased coin flip, where the bias of the coin is given by the un-quantised measurement $y_n$. This quantisation procedure is necessarily only in simulations as the qubit acts a naturally quantised sensor.
% Numerically, we incur additional saturation effects when calculating quantised residuals in the Kalman filter:
% \begin{align}
% \hat{d}_n &\sim \mathcal{B}(k=\hat{d}_n; n=1, p= p_n | y_n, z_n, t) \\
% p_n &\equiv \mathcal{S}(y_n) + b \\
% b  &\equiv   0.5 \\
% \mathcal{S} &\equiv \begin{cases} b, y_n > b \\  -b, y_n < b \end{cases}
% \end{align}
% % Here, $\mathcal{S}$ saturates the value of $y_n$  if measurement noise $v_n$ is too large for our interpretation of $y_n$ as a shifted, noisy probability trace. Saturation effects are well documented in amplitude quantisation theory (refer \cite{karlsson2005, widrow1996}), and have negligible impact for welll designed filter for the`true' dephasing regime presented in this paper.  
% \\
% \\
% We note that the measurement term refers to additive Gaussian white noise introduced during the experimental protocol before the final projective measurement is taken. It does not refer to measurement errors after a projective measurement is taken (e.g. detector efficiency). For our regime, the measurement noise $v_n$, has no physical interpretation but plays a critical role in smoothening and tracking the slow drift of a dephasing process. The process noise term retains its original rationale - that a dephasing processes is Markovian, and a deterministic transformation can be designed to approximate  its non-Markovian features.
In particular, for KF state $x$ with state variance $P$, the Cramer Rao Lower Bound (CRLB) is given in \cite{karlsson2005} as:
\begin{align}
& \ex{(\state_n - \hat{\state}_{n})(\state_n - \hat{\state}_{n})^T} \succeq P_{\state_n} 
\end{align}
$P_{n}$ is retrieved from the numerical recursion given in \cite{karlsson2005} for time-invariant linear $\Phi$ as:
\begin{align}
P_{n}^{-1} &= Q_{n-1}^{-1} + J_{m=1, n} - S_n^T(P_{n-1}^{-1} + V_n)^{-1}S_n \\
S_n &\equiv -\Phi_n^T Q_n^{-1} = -\Phi^T Q^{-1} \quad \forall n \\
V_n &\equiv -\Phi_n Q_n^{-1}\Phi_n^T  = -\Phi Q^{-1}\Phi^T \quad \forall n 
\end{align}
With linear time invariant $\Phi, Q$, the recursion of $P_n$ is affected only by one time varying quantity, $J(m,n)$, which is the Fisher information term arising from the amplitude quantisation $\mathcal{Q}$ for $m$ levels ($m=1$), as derived in \cite{karlsson2005}: 
\begin{align}
J_{m, n} &\equiv H_n^T J_{m} (z_n) H_n \\
J_{m=1} (z_n) & \equiv -\ex{ \frac{d^2}{dz_n^2} \log(P(d_n | z_n))}
% J_{m=1} (z_n) & \equiv \exp^{- \frac{z_n^2}{R} }\frac{1}{2 \pi R (1-\rho(z_n))\rho(z_n)}  \\
% \rho(z_n) &\equiv  1 - P(d_n = 1 |z_n, t) \\ 
% R & \equiv \ex{v_n^2}  \quad \forall t 
\end{align}
% One way to truncate probability distributions for amplitude quantisation is outlined in \cite{widrow1996, karlsson2005}, namely, suppose we add a uniformly distributed random variable $ u_n \sim U(a) \in [-a, a]$ such that $v_n' = y_n - z_n + u_n = v_n + u_n $. Addition of random variables requires a convolution (*) between uniform distributed noise and Gaussian distribution of $y_n$, namely, a validly `bandlimited' distribution for $v_n'$. 
% \begin{align}
% P(v_n' | z_n, t) & \equiv P(v_n | z_n, t) * U(a, b) = P(y_n | z_n, t) * U(a)
% \end{align}

The likelihood function $P(d_n|z_n)$ represents a coin toss experiment, with $d_n \in [0,1]$. In insisting on a coin-toss experiment to generate quantised measurement outcomes, we depart from quantisation approach in \cite{karlsson2005}, and we write the full likelihood as:
\begin{align}
P(d_n|z_n) & \equiv P(d_n=1|z_n)^{d_n}(1 - P(d_n=1|z_n))^{1-d_n} 
\end{align}
We assume that $P(d_n, y_n | z_n)$ exists and then the likehood for seeing $d_n=1$ requires us to marginalise over $y_n$:
\begin{align}
P(d_n=1|z_n) & = \int_{-\infty}^\infty P(d_n=1|y_n, z_n) P(y_n | z_n) dy_n 
\end{align}
$P(d_n=1|y_n, z_n)$ is merely the binomial distribution and $P(y_n | z_n)$ at $y_n$ is Gaussian distributed:
\begin{align}
P(d_n|y_n, z_n) &\equiv \mathcal{B}(k=d_n; n=1, p=y_n + 0.5| y_n, z_n) \\
P(y_n | z_n) &\equiv  P(z_n + v_n | z_n) = P(v_n | z_n) \\
 & = \frac{1}{2\pi R}\exp^{ -(y_n - z_n)^2 /R}  
\end{align}
Here, $y_n$ is a continuously distributed variable from $[-\infty, \infty]$. While a qubit is naturally quantised, numerical simulations of amplitude quantisation require us to saturate $ y_n \in[-b, b], b = 0.5$. Indeed, an optimisation of the design parameter $R$ for a given application incorporates the need to minimise saturation effects by narrowing the Gaussian distribution for $ P(y_n | z_n) $. A formal way to bandlimit a probability distribution is to introduce an additive uniformly distributed noise signal such that the probability distribution of $y_n$ are convolved with a band-limited uniform distribution. However, for analytical tractability, we omit this step and minimise numerical artefacts from the approximate truncation ($b$) through choosing low $R$ regimes:
\begin{align}
P(d_n=1|z_n) & = \int_{-\infty}^\infty P(d_n=1|y_n, z_n) (P(y_n | z_n) dy_n \\
p(1|z_n)_b & \approx \int_{-b}^{b} P(d_n=1|y_n, z_n) P(y_n | z_n) dy_n \\
&= \frac{1}{4\sqrt{\pi R}} (1 + z_n) \text{erf}(\frac{z_n + b}{\sqrt{R}})  \\
&+ \frac{1}{4\sqrt{\pi R}} (1 + z_n) \text{erf}(\frac{b - z_n }{\sqrt{R}})\\
&+ \frac{1}{4\pi} (\exp^{-\frac{(z_n + b)^2}{\sqrt{R}}} - \exp^{-\frac{(z_n - b)^2}{\sqrt{R}}})
\end{align}
While full calculation required to compute $J_{m=1} (z_n)$ would require a calculation of $ \frac{d^2}{dz_n^2} \log(P(d_n | z_n))$, we make the following approximation by using the Fisher information for a binomial distribution $(n=k=1)$, with success probability $ p(1|z_n)_b$:
\begin{align}
J_{m=1} (z_n) \approx \frac{1}{p(1|z_n)_b (1 - p(1|z_n)_b)}
\end{align}

\begin{figure}[h] 
	\caption{placeholder}
	\includegraphics[scale=0.45]{qif_rmse.pdf} 
	\label{fig:qif}
\end{figure}

We implement QKF for a true AR(2) process, where the AR(2) coefficients are chosen such the process satisfies covariance stationarity.  We provide the QKF the true dynamical model as well as a learned dynamical model from LSF. In our final figure, the RMSE (quantised) prediction error over 100 runs falls rapidly for true dynamics. The gap between QKF with true and learned LSF dynamics represents a fall in performance when dynamics are learned incorrectly for a simple $f$ if non-linear measurement models and quantisation errors are held constant. 
\\
\\
Additionally, we plot our numerical estimate of CRLB from our quantiser (biased coin flips) and we compare this with CRLB with sign quantisation (no coin flips) in \cite{karlsson2005} for our non linear measurement model. The difference between these two approximate CRLB trajectories captures truncation errors in the distribution of $y_n$ and the inclusion of a biased coin flip. Both CRLB trajectories accumulate linearisation errors in the covariance recursion equation from $H_n$ in the calculation of $J_{m=1} (z_n)$. 
\\
\\
The RMSE for QKF with true dynamics falls below numerically estimated CRLB trajectories. While such a violation may reflect our approach to modeling quantisation, both CRLB trajectories are violated irrespective of how measurement outcomes are quantised. A study of simpler single bit quantisers in \cite{karlsson2005} also appear to suggest that non-linear error accumulation are non trivial and alternative approaches outside of KF framework may be better suited to extracting noise correlations from binary measurement records.  

\subsection{Conclusion}
We presented algorithmic techniques to harness the susceptibility of single-qubit sensors to environmental dephasing processes to enable predictive control. A key design challenge is to develop approximate reconstructions of covariance stationary, non-Markovian environmental dephasing processes drawn from arbitrary power spectral densities. In approximately linear Bayesian regimes, we recast autoregressive and harmonic reconstructions of dephasing processes into Least Squares, Kalman and GPR frameworks, and we discussed their relative strengths in enabling forward prediction. Autoregressive reconstructions appeared to be model robust in a range of realistic operational scenarios. We moved away from a linear Bayesian regime with a desire to work directly with binary measurement records. We introduced a classical state space KF approach incorporating non-linear measurement actions and quantised measurement outcomes while incorporating non-Markovian dephasing through autoregressive dynamics. We compared the performance of a quantised Kalman filter with true and learned dynamics with a numerically estimated Cramer Rao Lower Bound.  Early indications are that QKF will not remain model robust in realistic operating environments. It is likely that one will need to move beyond the state-space KF frameworks presented in this paper to attain model robust predictive control for single qubits with projective measurements. 
% We are now in a position to calculate $p(d_n=1| z_n, t)$ as:
% \begin{align}
% P(1 |z_n, t) &= P(d_n=1| y_n) P(y_n | z_n, t)_Q  \\
% & = \begin{cases} \mathcal{B}(1; n=1, p= p_n | y_n, t) P(y_n | z_n, t), |y_n| \leq 0.5  \\ 0, \quad \text{otherwise}\end{cases}
% \end{align}


% \begin{align}
% P(1 |z_n, t) &= P(d_n=1| y_n') P(y_n' | z_n, t)_Q  \\
% & = \begin{cases} \mathcal{B}(1; n=1, p= p_n | y_n, t) P(y_n | z_n, t), |y_n| \leq 0.5  \\ 0, \quad \text{otherwise}\end{cases}
% \end{align}

% We make qualitative remarks to justify the form of $\rho(z_n)$, i.e. $P(d_n =1 |z_n, t)$,  for our quantiser remains Gaussian distributed despite the additional step of drawing from a binomial distribution. We assume that $P(d_n, y_n | z_n, t)$ exists such that:
% \begin{align}
% % P(d_n|z_n, t) = P(d_n = 1 |z_n, t) \delta(d_n-1) + P(d_n = 0 |z_n, t) \delta(d_n) \\
% % P(d_n |z_n, t) & \equiv \int  P(d_n, y_n | z_n, t) dy_n \\
% % & = -\sqrt(\pi)(z^2  + 1/4) \text{not a probability}\\
% %  P(d_n, y_n | z_n, t) &\equiv  P(d_n| y_n, z_n, t) P(y_n | z_n, t) \\
% P(d_n | y_n, z_n, t) &\equiv \mathcal{B}(k=d_n; n=1, p= p_n | y_n, z_n, t) \\
% P(d_n = 1 | z_n, t) & = \int (\mathcal{S}(y_n) + 0.5) P(y_n | z_n, t)  dy_n \\
% P(y_n | z_n, t) &\equiv  P(z_n + v_n | z_n, t) = P(v_n | z_n, t) \\
% & = \frac{1}{2\pi R}\exp^{ -(y_n - z_n)^2 /R}
% \end{align}



