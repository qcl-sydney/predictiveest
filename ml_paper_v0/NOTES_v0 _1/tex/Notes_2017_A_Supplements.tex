% ##############################################################################
\section{Physical Setting \label{sec:app:setup}}
% ##############################################################################


We consider a qubit under environmental dephasing.  For any two level system, a quantum mechanical description of physical quantities of interest can be provided in terms of the Pauli spin operators $\{ \p{x}, \p{y}, \p{z}\}$. If $\hbar \omega_A$ corresponds to an energy difference separating these two qubit states, then the Hamiltonian for a single qubit in free evolution can be written in the Pauli representation. We consider a qubit states in the $\p{z}$ basis, $\ket{0}$ or $\ket{1}$ with energies $E_0, E_1$ in our notation, corresponding to a 0 or 1 outcome upon measurement. This yields a Hamiltonian for a single qubit as:

\begin{align}
%\op{\mathcal{H}}_0 &= E_0\ket{0}\bra{0} + E_1\ket{1}\bra{1} \\
\p{z} &\equiv \ket{1}\bra{1} - \ket{0}\bra{0} \\
\op{\mathcal{I}} & \equiv \ket{0}\bra{0} + \ket{1}\bra{1} \\
\op{\mathcal{H}}_0 & = \frac{1}{2} (E_0\ket{0}\bra{0}+ E_1\ket{1}\bra{1}) \\
& + \frac{1}{2} [(E_1 - E_0)\p{z} + E_0 \ket{1}\bra{1} + E_1 \ket{1}\bra{1}]\\
& = \op{\mathcal{I}} \left( \frac{E_0 + E_1}{2} \right) + \p{z}(\frac{E_1 - E_0}{2}) \\
\quad E_{0,1} &\equiv \mp \frac{1}{2} \hbar \omega_A \\
\op{\mathcal{H}}_0 &= \frac{1}{2} \hbar \omega_A \p{z}
\end{align}

In this representation, the effect of dephasing noise on a free qubit system is that any initially prepared qubit superposition of $\ket{0}$ and $\ket{1}$ states will decohere over time in the presence of dephasing noise. This physical effect is modelled as a stochastically fluctuating process $\delta\omega(t)$ that couples with the $\p{z}$ operator. The noise Hamiltonian is described as:
\begin{align} 
\op{\mathcal{H}}_{N}(t) & \equiv \frac{\hbar}{2}\delta\omega(t)\p{z}
\end{align}
In the formula above, $\delta\omega(t)$ is a classical, stochastically fluctuating parameter that models environmental dephasing and $\hbar/2$ appears as a convenient scaling factor. The total Hamiltonian for a single qubit under dephasing is:
\begin{align} 
\op{\mathcal{H}}(t) &\equiv \op{\mathcal{H}}_0 + \op{\mathcal{H}}_{N}(t)
\end{align}

Since $\op{\mathcal{H}}_{N}(t)$ commutes with $\op{\mathcal{H}}_0$, we can transform away $\op{\mathcal{H}}_0$ by moving to a rotating frame with respect to $H_0$. Let $\ket{\psi (t)}$ be a state in the lab frame, let $\op{U}$ define a transformation to a rotating frame, and let $\ket{\tilde{\psi} (t)}$ be the state in the rotating frame. The notation, $\tilde{}$, indicates operators and states in the transformed frame. In this simple case, the transformed Hamiltonian governing the evolution of $\ket{\tilde{\psi} (t)}$ will just be $\op{\mathcal{H}}_{N}(t)$:

\begin{align}
\op{U} &\equiv e^{-i\op{\mathcal{H}}_0 t / \hbar } \\
\ket{\tilde{\psi} (t)} &\equiv \op{U}^\dagger \ket{\psi (t)} \\
i \hbar \frac{d}{dt} \ket{\tilde{\psi} (t)} & \equiv i \hbar \frac{d}{dt} \op{U}^\dagger \ket{\psi (t)} \\
&= -\op{\mathcal{H}}_0 \op{U}^\dagger \ket{\psi (t)} + i\hbar \op{U}^\dagger \frac{d}{dt} \ket{\psi (t)} \\
&= -\op{\mathcal{H}}_0 \op{U}^\dagger \ket{\psi (t)} + \op{U}^\dagger \mathcal{H}  \ket{\psi (t)} \\
&= -\op{\mathcal{H}}_0 \op{U}^\dagger \ket{\psi (t)} + \op{U}^\dagger \mathcal{H} \op{U} \op{U}^\dagger \ket{\psi (t)}, \quad \op{U}\op{U}^\dagger \equiv 1 \\
&= -\op{\mathcal{H}}_0 \op{U}^\dagger \ket{\psi (t)} + \op{U}^\dagger \mathcal{H} \op{U} \op{U}^\dagger \ket{\psi (t)}, \quad \op{U}\op{U}^\dagger \equiv 1 \\
 &=  ( \op{U}^\dagger \mathcal{H} \op{U} -\op{\mathcal{H}}_0  ) \op{U}^\dagger \ket{\psi (t)} \\
& =  ( \op{U}^\dagger \mathcal{H} \op{U} -\op{\mathcal{H}}_0  ) \ket{\tilde{\psi} (t)}
\end{align}

\begin{align}
\implies \op{\tilde{\mathcal{H}}} &\equiv \op{U}^\dagger \mathcal{H} \op{U} -\op{\mathcal{H}}_0 \\
& = \op{U}^\dagger \op{\mathcal{H}}_0\op{U}  + \op{U}^\dagger \op{\mathcal{H}}_{N}(t) \op{U} -\op{\mathcal{H}}_0 \\
& = \op{U}^\dagger \op{U} \op{\mathcal{H}}_0  + \op{U}^\dagger \op{U} \op{\mathcal{H}}_{N}(t)  -\op{\mathcal{H}}_0, \quad [\op{U}, \op{\mathcal{H}}_0 ] = [\op{U}, \op{\mathcal{H}}_{N}(t) ] = 0 \\
& = \op{\mathcal{H}}_{N}(t)
\end{align}
In the semiclassical approximation,  $\op{\mathcal{H}}_{N}(t)$ commutes with itself at different $t$, and hence we can write a unitary time evolution operator in the rotating frame as:
\begin{align}
\op{\tilde{U}}(t, t + \tau) &\equiv  e^{-\frac{i}{\hbar}  \int_{t}^{t + \tau} \op{\mathcal{H}}_{N}(t') dt'  } \\
& = e^{-\frac{i}{2} \state(t, t + \tau) \p{z} } \\
\state(t, t + \tau) & \equiv  \int_{t}^{t + \tau} \delta \omega (t') dt' \label{eqn:app:phases}
\end{align}
In the rotating frame, we prepare an initial state that is a superposition of $\ket{0}$ and $\ket{1}$ states. This state evolves under $\op{\mathcal{H}}_{N} (t)$ during a Ramsey experiment for duration $\tau$.  Subsequently, the qubit state is rotated before a projective measurement is performed with respect to the $\p{z}$ axis i.e. the measurement action resets the qubit. 

Without loss of generality, define the initial state as  $\ket{\tilde{\psi} (0)} \equiv \frac{1}{\sqrt{2}} \ket{0} + \frac{1}{\sqrt{2}} \ket{1}$ in the rotating frame. Then, the probability of measuring the same state after time $\tau$ in a single shot measurement, $d \in \{0, 1\}$, is:
\begin{align} 
Pr(d | f(0, \tau), \tau) & = |\bra{\tilde{\psi} (0)} \op{\tilde{U}}(0, \tau) \ket{\tilde{\psi} (0)}|^2 \\
& = |\frac{1}{2} \bra{\tilde{\psi} (0)} e^{-\frac{i}{2} \state(0, \tau) \p{z} } \left(  \ket{0} + \ket{1} \right)|^2 \\
& = |\frac{1}{2}  \bra{\tilde{\psi} (0)} \left( e^{\frac{i}{2} \state(0, \tau) }\ket{0} + e^{-\frac{i}{2} \state(0, \tau)}\ket{1} \right)|^2\\
& = |\frac{1}{2}  ( e^{\frac{i}{2} \state(0, \tau)} + e^{-\frac{i}{2} \state(0, \tau)} )|^2\\
& = \cos(\frac{\state(0, \tau)}{2})^2 \label{eqn:app:likelihood}
\end{align}
The second $\pi/2$ control pulse rotates the state vector such that a measurement in $\p{z}$ basis is possible, and the probabilities correspond to observing the qubit in the   $\ket{1}$ state. Hence, \cref{eqn:app:likelihood} defines the likelihood for single shot qubit measurement. Further, \cref{eqn:app:likelihood} defines the non linear measurement action on phase noise jitter, $\state(0, \tau)$.  We impose a condition that $\state(0, \tau)/2 \leq \pi$  such that accumulated phase over $\tau$ can be inferred from a projective measurement on the $\p{z}$ axis. 



\newpage
% ##############################################################################
\section{Experimentally Controlled Discretisation of Dephasing Noise \label{sec:app:exptres}} 
% ##############################################################################
 In this section, we consider a sequence of Ramsey measurements. At time $t$, \cref{sec:app:setup} describes the qubit measurement likelihood at one instant under dephasing noise. We assume that the dephasing noise is slowly drifting with respect to a fast measurement action on timescales of order $\tau$. In this regime, \cref{eqn:app:phases} discretises the continuous time process $\delta\omega(t)$, at time $t$, for a number of $n= 0, 1, ..., N$ equally spaced measurements with $t = n \Delta t$. Performing the integral for $\tau \ll \Delta t$ and slowly drifting noise such that we substitute the following terms in \cref{eqn:app:phases}:
\begin{align}
\delta\bar{\omega}_n &\equiv \delta\omega(t')|_{t'=n \Delta t } \\
\state_n &\equiv \state(n\Delta t, n\Delta t + \tau) \\
& \equiv \frac{\hbar}{2}  \int_{n\Delta t}^{n\Delta t + \tau} \delta\bar{\omega}_n dt' \\
& \equiv \frac{\hbar}{2}  \delta\bar{\omega}_n \int_{n\Delta t}^{n\Delta t + \tau}  dt' \\
& = \frac{\hbar}{2}\p{z}\delta\bar{\omega}_n \tau \label{eqn:app:phases_constantdetuning}
\end{align}
In this notation, $\delta\bar{\omega}_n $ is a random variable realised at time, $t = n \Delta t$, and it remains constant over short duration of the measurement action, $\tau$.  We use the shorthand $\state_n \equiv \state(n\Delta t, n\Delta t + \tau)$ to label a sequence of stochastic, temporally correlated qubit phases $ \state \equiv \{\state_n \}$. 

Since the qubit is reset by each projective measurement at $n$, the unitary operator governing qubit evolution is also reset such that $\op{\tilde{U}}_n \equiv \op{\tilde{U}}(n\Delta t, n\Delta t + \tau)$ are a collection of $N$ unitary operators describing qubit evolution for each new Ramsey experiment. They are not to be interpreted, for example, as describing qubit free evolution without re-initialising the system. Hence, for each stochastic qubit phase $\state_n$, the true probability for observing the $\ket{1}$ in a single shot is given by substituting $\state_n $ for $ \state(0,1)$ in \cref{eqn:app:likelihood}.
\begin{align}
Pr(d_n | \state_n, \tau, n \Delta t) &= \begin{cases} \cos(\frac{\state_{n}}{2})^2 \quad \text{for $d=1$} \\   \sin(\frac{\state_{n}}{2})^2  \quad \text{for $ d=0$} \end{cases} 
\end{align}
The last line follows from the fact that total probability of the qubit occupying either state must add to unity. This yields \cref{eqn:main:likelihood} in the main text.

\iffalse
 \clearpage
 The discretisation of $\delta\omega(t)$ into  a discrete time stochastic phase sequence, $\state$, governs the physical Fourier resolution at which dephasing noise is sampled.
 If we take a sequence, $\state$, to be $N$ samples long, then we define the resulting physical Fourier domain resolution :
 \begin{align}
 \Delta f_{EXPT} & \equiv \frac{1}{\Delta t N} \\
 N & \equiv N_T + N_P
 \end{align} By specifying  $\Delta t$, training points, $ N_T$, and forward prediction time steps, $ N_P$, an experiment fully defines the physical Fourier resolution. In particular, an implicit assumption about the bandwidth of dephasing noise, $f_B$ is captured in $\Delta t$, namely:
\begin{align}
\Delta t \equiv \frac{1}{f_s} \equiv \frac{1}{r_{Nqy}f_B}
 \end{align}
In numerical simulations, we choose  Nyquist multipler $r_{Nqy} \gg 2$ and fix $f_B, \Delta t, N_T, N_P$. For true noise engineering, we compare true noise spacing $\omega_0/ 2\pi$ relative to the physical resolution of the entire experimental run, i.e. $\Delta f_{EXPT}$. Further, if the oversampling regime for any algorithm is varied, it can interpreted as relaxing $f_B$ assumption or equivalently, reducing $r_{Nqy}$, however, we ensure there is no physical aliasing and $r_{Nqy}>2$ in all cases.
\\
\\
The computational resolution in algorithms has a natural interpretation such that  $\omega_0^B / 2\pi \geq \frac{1}{\Delta t N_T} > \Delta f_{EXPT}$. This is necessarily the case since an algorithm ceases to receive measurement data but we continue to `sample' true noise to confirm accuracy of predictions in our study. Numerically, this is significant in optimising spacing between adjacent oscillators for  LKFFB and GPR algorithms and linking their performance to a physical interpretation of sampling rates.  
\fi




\newpage
% ##############################################################################
\section{True Dephasing Noise Engineering \label{sec:app:truenoise}} 
% ##############################################################################
In the absence of an apriori model for describing qubit dynamics under dephasing noise, we impose the following properties on a sequence of stochastic phases, $\state  \equiv \{ \state_n \}$ such that we can design meaningful predictors of qubit state dynamics. We assert that a stochastic process, $\state_n$, indexed by a set of values, $ n = 0, 1, \hdots N $ satisfies: 

\begin{align}
\ex{\state_n} &= \mu \quad \forall n \label{eqn:app:f_mean} \\
\ex{\state_n^2} & < \infty \quad \forall n \label{eqn:app:f_var} \\
\ex{(\state_n - \mu)(\state_m - \mu)} &= R(\nu), \quad  \nu = |n-m|, \quad \forall n, m \in N  \label{eqn:app:f_covar} \\
R(\nu) & \neq \sigma^2  \delta(\nu) \label{eqn:app:f_Markovian} 
\end{align}
Covariance stationarity of $\state$ is established by satisfying \cref{eqn:app:f_mean,eqn:app:f_var,eqn:app:f_covar}, namely that the mean is independent of $n$, the second moments are finite, and the covariance of any two stochastic phases at arbitrary time-steps, $n, m$, do not depend on time steps but only on the separation distance, $\nu$. The $\delta(\nu)$ in the last condition,  $ \cref{eqn:app:f_Markovian}$, is the Dirac-delta function and establishes that $\state$ is not delta-correlated (white). This condition captures the slowly drifting assumption for environmental dephasing noise. 


We also require that correlations in $\state$ eventually die off as $\nu \to \infty$ otherwise any sample statistics inferred from noise-corrupted measurements are not theoretically guaranteed to converge to the true moments. The mean square ergodicity is defined below:
\begin{align}
 \lim_{K \to \infty} \frac{1}{K} \sum_{v=0}^{K-1} R(\nu) & = 0  \iff  \lim_{K \to \infty} \ex{(\bar{\state_K} - \mu)^2} = 0 \nonumber \\
\text{for} \quad v &= |n_k - n_j|, \quad \forall k,j \in K, n_k, n_j \in N  \nonumber \\
\text{with} \quad \bar{f_K} &= \frac{1}{K} \sum_{k=0}^K f_{n_k} \quad \text{(sample mean of $\state$)}  \label{eqn:app:f_msergodic}  
\end{align}
The statement above means that a true $R(\nu)$ associated with $\state$ is bandlimited for sufficiently large (but unknown) $K$. If correlations never `die out', then any designed predictors for one realisation of dephasing noise will fail for a different realisation of the same true dephasing. For the purposes of experimental noise engineering, we satisfy the assumptions above by engineering discretised process, $\state$, as:
\begin{align}
\state_n &= \alpha \omega_0 \sum_{j=1}^{J} j F(j)\cos(\omega_j n \Delta t + \psi_j) \label{eqn:app:noiseengineering} \\
F(j) & = j^{\frac{p}{2}-1}  
\end{align}

Using the notation of \cite{soare2014}, $\alpha$ is an arbitrary scaling factor, $\omega_0$ is the fundamental spacing between true adjacent discrete frequencies, such that $\omega_j = 2 \pi f_0 j =\omega_0 j, j = 1, 2, ...J$. For each frequency component, there exists a uniformly distributed random phase, $\psi_j \in [0, \pi]$. The free parameter $p$ allows one to specify an arbitrary shape of the true power spectral density of $\state$. In particular, the free parameters $\alpha, J, \omega_0, p$ are true dephasing noise parameters which any prediction algorithm cannot know beforehand.

It is straightforward to show that $\state$ is covariance stationary. To show mean square ergodicity of $\state$, one requires phases are randomly uniformly distributed over one cycle for each harmonic component of $\state$ \cite{gelb1974applied}. Subsequently, one shows that an ensemble average and a long time average of multi-component engineered $\state$ are equal. 

\subsection{Proof  $\state$ is mean square ergodic and Gaussian}

Consider one term in the harmonic sum $f^j_n(\psi_j)$, $j$ denoting the $j$-th true spectral component (not a power) of $f_n$, such that $A_j \equiv \alpha \omega_0 j F(j), \omega \equiv \omega_j$. Note that $\{ \psi_j \}$ are randomly uniformly distributed over one cycle, and $f^j_n(\psi_j)$ is a function of random variable $\psi_j$ defining a random process over the time index, $n$. 

\begin{align}
f^j_n(\psi_j) & \equiv A_j \cos(\omega_j \Delta t n + \psi_j ) 
\end{align}

Let two arbitrarily chosen time indices, $n_1, n_2$ be spaced $\nu = |n_1 - n_2|$ apart. We consider an ensemble of realisations of $\state$, such that the random phase $\psi$ is different in each realisation, the distribution of these phases is uniform over one cycle. Consider the joint probability density of incurring any two random phases (see, e.g. \cite{gelb1974applied}):
\begin{align}
g_2(\psi_1, n_1; \psi_2, n_2) & \equiv \frac{1}{2\pi}, \psi \in [0, 2\pi] \label{eqn:SS_ensble_prob_density} \\
\end{align}

An expectation over an ensemble, $\mathcal{D}$, of sequences of $ \{ f^j_n(\psi_j) \}$ yields the covariance function:
\begin{align}
R(n_1, n_2) &\equiv \ex{(f^j_{n_1}(\psi_1) - \mu_\state )(f^j_{n_2}(\psi_2) - \mu_\state )}_\mathcal{D} \\
& = \ex{f^j_{n_1}(\psi_1) f^j_{n_2}(\psi_2)},  \quad \mu_\state \equiv 0 \\
& = \int  d\psi_1  \int  d\psi_1  \quad f^j_{n_1}(\psi_1) f^j_{n_2}(\psi_2) g_2(\psi_1, n_1; \psi_2, n_2) \\
& = \int_0^{2\pi} d \psi \frac{1}{2\pi} \quad f^j_{n_1}(\psi) f^j_{n_2}(\psi) \\
& = \frac{A_j^2}{4\pi} \int_0^{2\pi} d \psi \quad 2\cos(\omega_j \Delta t n_1 + \psi) \cos(\omega_j \Delta t n_2 + \psi) \\
& = \frac{A_j^2}{4\pi} \int_0^{2\pi} d \psi \quad \cos(\omega_j \Delta t (n_1 -n_2))  + \cos(\omega_j \Delta t (n_1 + n_2) + 2\psi) \\
& = \frac{A_j^2}{2} \cos(\omega_j \Delta t (n_1 -n_2))  + \frac{A_j^2}{4 \pi}\int_0^{2\pi} d \psi  \cos(\omega_j \Delta t (n_1 + n_2) + 2\psi) \\
& = \frac{A_j^2}{2} \cos(\omega_j \Delta t \nu), \nu = |n_1 -n_2|
\end{align}

A long time average taken over one $f^j_n(\psi_j)$ yields:
\begin{align}
R(n, n + \nu)  & \equiv \lim_{N \to \infty} \frac{1}{\Delta t N} \int_{-\Delta t N/2}^{\Delta t N/2} dn \quad  f^j_n(\psi_j) f^j_{n + \nu}(\psi_j)   \\
& = \lim_{N \to \infty} \frac{A_j^2}{2 \Delta t N} \int_{-\Delta t N/2}^{\Delta t N/2} dn \quad 2 \cos(\omega_j \Delta t n + \psi) \cos(\omega_j \Delta t n + \omega_j \Delta t \nu + \psi) \\
& = \lim_{N \to \infty} \frac{A_j^2}{4 \Delta t N} [ 2\Delta t N\cos(-\omega_j \Delta t \nu)   + \int_{-\Delta t N/2}^{\Delta t N/2} dn \quad \cos(2\omega_j \Delta t n + \omega_j \Delta t v + 2\psi) ]\\
& =  \frac{A_j^2}{2} \cos(\omega_j \Delta t \nu) \\
\end{align}
Typically, $f^j_n(\psi_j)$ so defined is only ergodic for uniformly distributed phases. To compute mean square ergodicity for the full $\state$, we reintroduce the sum over $j = 1, 2, ... , J$:
\begin{align}
f_n & = \sum_j^J f^j_n(\psi_j) \\
\ex{f_n} &= \sum_j^J \ex{f^j_n(\psi_j)} = 0 \\
\ex{f_{n_1}f_{n_2}} &=  \ex{\sum_j^J f^j_{n_1}(\psi_j) \sum_{j'}^{J} f^{j'}_{n_2}(\psi_{j'})}  \\
 & = \ex{\sum_j^J f^j_{n_1}(\psi_j) f^j_{n_2} (\psi_j)} + \ex{\sum_{j'}\sum_{j\neq j'}^J f^j_{n_1}(\psi_j) f^{j'}_{n_2}(\psi_{j'})} \\
  & = \sum_j^J \ex{f^j_{n_1}(\psi_j) f^j_{n_2}(\psi_j)} + \sum_{j'}\sum_{j\neq j'}^J \ex{f^j_{n_1}(\psi_j) f^{j'}_{n_2}(\psi_{j'})} \\
 & = \sum_j^J \frac{A_j^2}{2} \cos(\omega_j \Delta t \nu) + \sum_{j'}\sum_{j\neq j'}^J \ex{f^j_{n_1}(\psi_j) f^{j'}_{n_2}(\psi_{j'})}, \nu = |n_1 - n_2 |  \label{eqn:SS_fn_crossterm}\\
 & = \sum_j^J \frac{A_j^2}{2} \cos(\omega_j \Delta t \nu), \nu  = |n_1 - n_2 |, j = j'
\end{align}
 
 I argue that the second term in \ref{eqn:SS_fn_crossterm} is zero because $\psi_j, \psi_{j'}$ are uniformly distributed phases for cycles with different angular frequencies.
 I suggest that the joint probability density function of $ g_2(\psi_j, n_1; \psi_{j'}, n_2) = g(\psi_j, n_1) g(\psi_{j'}, n_2), j \neq j'$:

\begin{align}
\sum_{j'}\sum_{j\neq j'}^J \ex{f^j_{n_1}(\psi_j) f^{j'}_{n_2}(\psi_{j'})} & = A_j A_{j'} \int d \psi_j \quad g(\psi_j, n_1) \cos(\omega_j \Delta t n_1 + \psi_j) \int d \psi_{j'} \quad g(\psi_{j'}, n_2) \cos(\omega_{j'} \Delta t n_2 + \psi_{j'}) \\
& = A_j A_{j'} \int_0^{2\pi} d \psi_j \quad \frac{1}{2\pi} \cos(\omega_j \Delta t n_1 + \psi_j) \int_0^{2\pi} d \psi_{j'} \quad \frac{1}{2\pi}  \cos(\omega_{j'} \Delta t n_2 + \psi_{j'}) \\
& = 0 
\end{align}
 \\
 \\
  A long term time average yields:
\begin{align}
\ex{f_n f_{n+\nu}} & \equiv \lim_{N \to \infty} \frac{1}{\Delta t N} \int_{-\Delta t N/2}^{\Delta t N/2} dn \quad  \sum_j^J f^j_n(\psi_j) \sum_{j'}^{J} f^{j'}_{n + \nu}(\psi_{j'}) \\
&= \sum_j^J  \sum_{j'}^{J} \lim_{N \to \infty} \frac{1}{\Delta t N} \int_{-\Delta t N/2}^{\Delta t N/2} dn \quad   A_j A_{j'} \cos (\Delta t n(\omega_j - \omega_{j'}) - \Delta t \omega_{j'} \nu)  \cos (\Delta t n(\omega_j + \omega_{j'}) + \Delta t \omega_{j'} \nu + 2\psi)\\
&= \begin{cases}
& \sum_j^J \frac{A_j^2}{2} \cos(\omega_j \Delta t \nu),  j = j' \\
& 0, j \neq j'
\end{cases}
\end{align}

Hence, $f(n)$ is a covariance stationary erogdic process. For the evaluation of the long time average, we use product-to-sum formulae and observe that the case $j\neq j'$ has a zero contribution as any finite contribution from cosine terms over a symmetric integral are reduced to zero as $N \rightarrow \infty $.  For $j = j'$, only a single cosine term survives. The surviving term depends on $\nu$ and not $n$ - yielding a cancellation of $N$ and a finite contribution that matches the ensemble average.

We briefly comment that $\state$ is Gaussian by the central limit theorum in the regimes considered in this manuscript. The probability density function of a sum of random variables is a convolution of the individual probability density functions. The central limit theorum grants that each element of $\state_n$ at $n$ appears Gaussian distributed for large $J$, irrespective of the underlying properties of $x_{j,n}$, or the distribution of the phases $\psi$. Numerical analysis shows that $J > 15$ results in each $\state_n$ appearing approximately Gaussian distributed. 




\newpage
% ##############################################################################
\section{ AKF \label{sec:app:AKF}}
% ##############################################################################
% We consider two theoretic frameworks to represent any covariance stationary processes. The first is motivated by Wold's decomposition and gives rise to autogressive high order $p$ approaches in this paper. The second is given by the spectral decomposition theorum, and motivates the use of oscillator approaches in this paper. 

% \subsection{Autoregressive (AR($q$)) and Moving Averages (MA($p$)) Processes}
In this section, we justify the representation of $\state$ assumed by the AKF. In particular, we justify any $\state$ of arbitrary power spectral density satisfying the properties in \cref{sec:app:truenoise} can be approximated by a high order autoregressive process.   In particular, we will consider autoregressive (AR) processes of order $q$, (AR($q$)), and  moving average processes of order, $p$ (MA($p$)). A model incorporating both types of processes is known as an ARMA($q,p$) model in our notation. 

First, we define the lag operator, $\mathcal{L}$. This operator defines a map between time series sequences and enables a compact description of ARMA processes. For an infinite time series $\{ f_n \}_{n = -\infty}^{\infty}$ and a constant scalar, $c$, the lag operator is defined by the following properties:
\begin{align}
\mathcal{L} f_n & = f_{n-1} \\
\mathcal{L}^q f_n & = f_{n-q} \\
\mathcal{L}(cf_n) & = c\mathcal{L}f_n = cf_{n-1}  \\
\mathcal{L}f_n & = c, \quad \forall n, \implies \mathcal{L}^q f_n  = c
\end{align}
Next, we define a Gaussian white noise sequence, $\xi$, under the strong condition than what is stated simply in \cref{eqn:app:ARMA:xi_indep}, that $\xi_n, \xi_m$ are independent $\forall n, m $:
\begin{align}
\ex{\xi} &\equiv 0  \\
\ex{\xi_n \xi_m} &\equiv \sigma^2 \delta(n-m)\label{eqn:app:ARMA:xi_indep}   
\end{align}

With these definitions, we can define an autoregressive process and a moving average process of unity order.  \cref{eqn:app:ARMA:AR_1} defines an AR($q=1$) process and dynamics of $\state_n$ are given as lagged values of the variable $\state$. The second definition in \cref{eqn:app:ARMA:MA_1} depicts a MA($p = 1$) process where dynamics are given by lagged values of Gaussian white noise $\xi$. 
\begin{align}
(1 - \phi_1 \mathcal{L}) f_n  & = c + \xi_n  \label{eqn:app:ARMA:AR_1} \\
f_n & = c' + (\Psi_1 \mathcal{L} + 1)\xi_n  \label{eqn:app:ARMA:MA_1} 
\end{align}
Here, $\Psi_1, \phi_1$ are known scalars defining dynamics of $\state_n$; $w_n$ is a white noise Gaussian process, and $c, c'$ are fixed scalars. It is well known that an MA($\infty$) representation is equivalently an AR($1$) process, and the reverse relationship also applies. For example, we can re-write \cref{eqn:app:ARMA:AR_1} as:
\begin{align}
f_n & = c + \xi_n + \phi_1 f_{n-1} \\
& = w_n + \phi_1 f_{n-1} \\
& = w_n + \phi_1 (w_{n-1}+ \phi_1 f_{n-2} ) \\
& \vdots \\
& = \phi_1^{n+1} F_0 + \phi_1^{n} w_{0} + \phi_1^{n-1} w_{1} + \hdots w_n \\
& = \phi_1^{n+1} F_0 + \phi_1^{n} (c + \xi_{0}) + \hdots + (c + \xi_{n}) \\
& = \phi_1^{n+1} F_0 +  c (\phi_1^{n} + \phi_1^{n-1} + \hdots + 1) + \sum_{m=0}^{n} \phi_1^m \xi_{n-m} \\
w_n & \equiv c + \xi_n \\
F_0 & \equiv f_{n=-1} 
\end{align} We restrict $|\phi_1| < 1$ such that $\state$ is covariance stationary \cite{hamilton1994time}. % An MA process of any order is covariance stationary for any choice of coefficients - instead, the restrictions on the MA representation arise in the form of invertibility of an MA process \cite{hamilton1994time}.
Under these conditions, we take the limit of $f$ capturing an infinite past, or namely, as $n$ indexes an infinite number of terms. The initial state $F_0$ is eventually forgotten, $\phi_1^{n+1} F_0 \approx 0$ if $n$ is large and $|\phi_1| < 1$. Similarly, the terms $c (\phi_1^{n} + \phi_1^{n-1} + \hdots + 1)$  can be summarised as a geometric series in $\phi_1$. The remaining terms satisfy the definition of an MA($\infty$) process:
\begin{align}
f_n &= c \frac{1}{1 - |\phi_1|}  +  \sum_{m=0}^{\infty} \phi_1^m \xi_{n-m}, \quad |\phi_1| < 1
% & = c' + \sum_{m=0}^{\infty} \Psi_m \xi_{n-m}\\
% \Psi_m  & \equiv \Phi^m, \quad \sum_{m=0}^{\infty}  |\Psi_m| = \sum_{m=0}^{\infty}  |\phi|^m < \infty 
\end{align}
It is straightforward to show that the reverse is true, namely, an MR($1$) is equivalent to an AR($\infty$) representation \cite{hamilton1994time}.

The consideration of an MA($\infty$) process leads us directly to Wold's decomposition for arbitrary covariance stationary processes, namely, that any covariance stationary $\state$ can be represented as:
\begin{align}
\state_n & \equiv  c' + \sum_{k=0}^{\infty} \Psi_k \mathcal{L}^k \xi_{n}  \label{eqn:app:ARMA:MAinf} \\
% & =  \tilde{f}_n + \sum_{k=0}^{\infty} \Psi_k \mathcal{L}^k \xi_{n}  \\
c' & \equiv \ex{\state_n | \state_{n-1}, \state_{n-2}, \hdots} \\
% \xi_n & \equiv   \state_n - \ex{\state_n | \state_{n-1}, \state_{n-2}, \hdots} \\
\Psi_0 & \equiv 1 \\
\sum_{k=0}^{\infty} \Psi_k^2 & < \infty
% & = \sum_{k=0}^{\infty} \Psi_k \xi_{n-k} 
\end{align}
\cref{eqn:app:ARMA:MAinf} defines an MA($\infty$) process derived previously as an AR($1$) process. This process is ergodic for Gaussian $\xi$. However, such a representation of $\state$ requires fitting data to an infinite number of parameters $\{\Psi_1, \Psi_2, \hdots \}$  and approximations must be made. 

% A standard approximation is to leverage the dual behaviour of MA($p$) and AR($q$) process and consider finite order polynomials in an ARMA($q,p$) model with MA coefficients given by $\theta_{p \leq p'}$ and AR coefficients given by $\phi_{p \leq p'}$:
% \begin{align}
% \sum_{k=0}^{\infty} \Psi_k \xi_{n-k} & \to \frac{ 1 + \theta_{1}\mathcal{L}^{1} + \hdots + \theta_{p}\mathcal{L}^{p}}{1 - \phi_{1}\mathcal{L}^{1} - \hdots - \phi_{1}\mathcal{L}^{q}}\\
% & \quad |\phi_i|  < 1, i = 1, \hdots, q
% \end{align}
% Instead of using an infinite past or an ARMA approximation,

We approximate an arbitrary covariance stationary $\state$ using finite but high order AR($q$) processes. Below we show that any finite order AR($q$) process has an MA($\infty$) representation satisfying Wold's theorum.

We define an arbitrary AR($q$) process as:
\begin{align}
\xi_n & \equiv (1 - \phi_1 \mathcal{L}  - \phi_2 \mathcal{L} ^2 - \hdots -\phi_q \mathcal{L} ^q) (\state_n - c)
% & = (1 - \lambda_1 \mathcal{L}) \hdots (1 - \lambda_q \mathcal{L}) (\state_n - c) \label{eqn:app:ARMA:ar_p_1}
% \state_n - c & \equiv \frac{1}{(1 - \lambda_1 \mathcal{L}) \hdots (1 - \lambda_q \mathcal{L})}
\end{align}
In particular, we define $\lambda_i, i = 1, \hdots, q$ as eiqenvalues of the dynamical model, $\Phi$:
\begin{align}
\Phi &\equiv \begin{bmatrix} \phi_1 & \phi_2 & \phi_3 & \hdots & \phi_{q-1}  &\phi_q \\
1 & 0 & 0 & \hdots & 0 & 0 \\
0 & 1 & 0 & \hdots & 0 & 0 \\
0 & 0 & 1 & \hdots & 0 & 0 \\
\vdots & \vdots & \vdots & \hdots & \vdots & \vdots \\
0 & 0 & 0 & \hdots & 1 & 0 \\
 \end{bmatrix} \\
\bf{\lambda} & \equiv \begin{bmatrix} \lambda_1 \dots \lambda_q \end{bmatrix} \quad \text{s.t.} |\Phi - \bf{\lambda}\mathcal{I}_q|  = 0 \\
\end{align}
We use the following result from \cite{hamilton1994time} without proof that the above implies:
\begin{align}
1 & - \phi_1 \mathcal{L}  - \phi_2 \mathcal{L}^2 - \hdots -\phi_q \mathcal{L}^q \\
&\equiv (1 - \lambda_1\mathcal{L}) \hdots (1 - \lambda_q \mathcal{L}) 
\end{align}
This yields:
\begin{align}
\xi_n & = (1 - \lambda_1 \mathcal{L}) \hdots (1 - \lambda_q \mathcal{L}) (\state_n - c) \label{eqn:app:ARMA:ar_p_1}
\end{align}
For us to invert this problem and recover an MA process, we need to show that the inverse for each $(1 - \lambda_{q'} \mathcal{L})$ term exists for $q' = 1, \hdots, q$. To do this, we start by defining the operator $\Lambda_q(\mathcal{L}) $ :
\begin{align}
\Lambda_q(\mathcal{L}) & \equiv \lim_{k\to \infty} (1 + \lambda_q \mathcal{L} + \hdots + \lambda_q^k\mathcal{L})
\end{align}
% \begin{align}
% \xi_n & = (1 - \lambda_q \mathcal{L}) (\state_n - c) \\
% \Lambda_q(\mathcal{L}) \xi_n & = \Lambda_q(\mathcal{L}) (1 - \lambda_q \mathcal{L}) (\state_n - c) \\
%  & = \lim_{k\to \infty}(1 + \lambda_q^{k+1}\mathcal{L}^{k+1})(\state_n - c)
% \end{align}
We consider an arbitrary $q'$-th eigenvalue term in process and we multiply $\Lambda_q(\mathcal{L})$ :
\begin{align}
\Lambda_0(\mathcal{L}) \hdots \Lambda_q(\mathcal{L}) \hdots \xi_n &= 
\Lambda_0(\mathcal{L}) \hdots \Lambda_{q'}(\mathcal{L}) \hdots  (1 - \lambda_{0} \mathcal{L}) \hdots (1 - \lambda_{q'} \mathcal{L}) \hdots(\state_n - c) \\
 & = \lim_{k\to \infty}(1 + \lambda_{0}^{k+1}\mathcal{L}^{k+1}) \hdots \lim_{k'\to \infty}(1 + \lambda_{q'}^{k'+1}\mathcal{L}^{k'+1}) \hdots (\state_n - c)
\end{align}
Each of the residual terms,  $\lambda_{q'}^{k'+1}\mathcal{L}^{k'+1} \to 0 $ if $|\lambda_{q'}| < 1$  for large $k'$, and this case $\Lambda_{q'}(\mathcal{L})$ defines the inverse $(1 - \lambda_{q'} \mathcal{L})^{-1}$. This procedure is repeated for all $q'$ eigenvalues to invert \cref{eqn:app:ARMA:ar_p_1} and subsequently perform a partial fraction expansion as follows:
\begin{align}
\state_n - c & = \frac{1}{(1 - \lambda_1 \mathcal{L}) \hdots (1 - \lambda_q \mathcal{L})} \xi_n\\
& = \sum_{q'=1}^{q}\frac{a_{q'}}{1- \lambda_{q'} \mathcal{L}} \xi_n \\
a_{q'} & \equiv \frac{\lambda_{q'}^{q-1}}{\prod_{q''=1, q''\neq q'}^{q} (\lambda_{q'} - \lambda_{q''})}
\end{align} The coefficients are $a_{q'}$ as obtained via the partial fraction expansion method during which $\mathcal{L}$ is treated as an ordinary polynomial. At present, we have a represent $\state$ via a finite $q$ weighted average of values of $\xi$. However, in substituting the definition of $ \Lambda_{q'} \equiv (1- \lambda_{q'} \mathcal{L})^{-1}$, we recover the form of an MA representation (setting $c \equiv \tilde{\state}_n  = 0, \quad  \forall n$ for simplicity): 
\begin{align}
\state_n & = \left[ \sum_{q'=1}^{q} a_{q'} \mathcal{L}^0 +  \lim_{k \to \infty}  \sum_{k'=1}^{k} \left( \sum_{q'=1}^{q} a_{q'}  \lambda_{q'}^{k'} \right) \mathcal{L}^{k'}\right] \xi_n \\
& = \Psi_0 + \sum_{k=1}^{\infty} \Psi_k \mathcal{L}^k \xi_{n}  \\
\Psi_0 & \equiv \sum_{q'=1}^{q} a_{q'} \mathcal{L}^0  \\
\Psi_k & \equiv \sum_{q'=1}^{q} a_{q'}  \lambda_{q'}^{k'}
\end{align}
By examining the properties of $\Phi$ raised to arbitrary powers, it can be shown that $\sum_{q'=1}^{q} a_{q'} \equiv 1$ and $\Psi_k$ is the first element of $\Phi$ raised to the $k$-the power \cite{hamilton1994time}, yielding absolute summability of $\Psi_k$ if $|\phi_{q'<q}| < 1$. This ensures that Wold's theorum is fully satisfied and an AR($p$) process has an MA($\infty$) representation. In moving to an arbitrarily high $q$, we enable the approximation of any covariance stationary $\state$.

The proofs that high $q$ AR approximations for covariance stationary $f$ improve with $q$ for example, in \cite{wahlberg1989estimation}. The key correspondence is that the number of finite lag terms $q$ in an AR($q)$) model contribute to the first $q$ values of the covariance function. This approximation improves with $q$ even if $\state$ is not a true AR process \cite{wahlberg1989estimation,west1996bayesian}. Asymptotically efficient coefficient estimates for any $MA(\infty)$ representation of $\state$ are obtained by letting the order of a purely AR($q$) process tend to infinity and increasing total data size, $N$ \cite{wahlberg1989estimation}. 

When data is fixed at $N$, we expect a high $q$ model to gradually saturate in predictive estimation performance. One can arbitrarily increase performance by increasing both $q, N$ \cite{wahlberg1989estimation}.  In our application with finite data $N$, we increase $q$ to settle on a high order AR model while training LSF to track arbitrary covariance stationary power spectral densities \cite{brockwell1996introduction}. A high $q$ AR model is often the first step for developing models with smaller number of parameters, for example, considering a mixture of finite order AR($q$) and MA($p$) models and estimating $p+q$ number of coefficients using a range of standard protocols \cite{brockwell1996introduction,west1996bayesian}. The design of potential ARMA models for our application requires further investigation beyond the scope of this manuscript.




\newpage
% ##############################################################################
\section{Spectral Methods for LKFFB and GPR (Periodic Kernel)} \label{sec:app:spec_methods}
% ##############################################################################

 The well-known spectral representation theorum  guarantees that any covariance stationary random process (real or complex) can be represented in a generalised harmonic basis.  We defer a detailed treatment of spectral analysis of covariance stationary processes in standard textbooks, for example, \cite{hamilton1994time,karlin1975first} and present background and key results to provide insights into the choice of LKFFB and GPR (periodic kernel).

In this Appendix, the term $\state_n $ is granted to us by the spectral representation theorum and it defines the `true model' for an algorithm. However, $\state_n $ (from the spectral representation theorum) only approximates the true covariance stochastic phases of \cref{sec:app:truenoise} in the limit where total size of available sample data increases to infinity. The subtle difference between $\state_n$ defined in \cref{sec:app:truenoise} and $\state_n $ via the spectral representation theorum (below) arises from the fact that we have no apriori true model of describing stochastic qubit phases, and must rely on mean square approximations for tracking qubit phases. Henceforth, we retain $\state_n $ to be the true model for an algorithm with an understanding that this refers to an approximate representation of an arbitrary, covariance stationary sequence of stochastic qubit phases. We reserve the use of the $\hat{f_n}$ for the state estimates and predictions that an algorithm makes having considered a single measurement record. 

 The spectral representation theorum states that any covariance stationary random process has a representation given by $\state_n$, and correspondingly,  a probability distribution, $F(\omega)$ over $[-\pi, \pi]$ in the dual domain such that:
 
\begin{align}
\state_n & = \mu_\state + \int_{0}^{\pi} [ a(\omega) \cos(\omega n) +  b(\omega) \sin(\omega n) ] d\omega \\
R(\nu) & = \int_{-\pi}^{\pi} e^{-i\omega \nu } dF(\omega)
\end{align}
Here, $\mu_\state $ is the true mean of the process $\state$.  The processes $a(\omega) $ and $b(\omega)$ are zero mean and serially and mutually uncorrelated, namely, $\int_{\omega_1}^{\omega_{2}} a(\omega) d\omega$ is uncorrelated with $\int_{\omega_3}^{\omega_{4}} a(\omega) d\omega$ and $\int_{\omega_j}^{\omega_{j'}} b(\omega) d\omega$ for any $\omega_1 < \omega_2 < \omega_3 < \omega_4$ and any choice of $j, j'$ within the half cycle  $[0, \pi]$.

The distribution $F(\omega)$ exists as a limiting case of considering cumulative probability density functions for $\state_n$ at each $n$ and letting $n \to \infty$ such that a sequence of these density functions approach $F(\omega)$ \cite{karlin1975first}.  If $F(\omega)$ is differentiable with respect to $\omega$, then we see the power spectral density $S(\omega)$ and $R(\nu)$ are Fourier duals \cite{karlin1975first}:
\begin{align}
R(\nu) & = \int_{-\pi}^{\pi} e^{-i\omega \nu } S(\omega)d\omega \\
S(\omega) & \equiv \frac{dF(\omega)}{d\omega} 
\end{align}
The duality of the covariance function and the spectral density is formally expressed  in literature by the  Wiener Khinchin theorum.

We consider the finite sample analogue of the spectral representaiton theorum considered above by following \cite{hamilton1994time}. To proceed, we define mean square convergence as a distance metric for determining when a sequence of random variables $\{ \hat{f}_n\}$ converges to a random variable, $f_n$ in the mean square limit if:
\begin{align}
\ex{\hat{f}_n^2} & < \infty \quad \forall n \\
\lim_{n \to \infty}\ex{\hat{f}_n - f_n} & = \lim_{n \to \infty} ||\hat{f}_n- f_n || = 0
\end{align} 
The statement $||\hat{f}_n- f_n || = 0$ measures the closeness between random variables $\hat{f}_n$ and $f_n$ even though the mean square limit is defined for terms of a sequence of random variables, $\{ \hat{f}_n\}$, where convergence improves with $n \to \infty$. In context of this study, we define $\hat{f}_n$ as a linear predictor of $f_n$ belonging to a covariance stationary $\state$. Hence, each $\hat{f}_n$ for large $n$ is a linear combination of the set of random variables belonging to $\state$ (priori to $n$) and, in Kalman filtering, all past predictors. Mean square convergence of $||\hat{f}_n- f_n || = 0$ in our context is a statement of the quality of the predictor, $\hat{f}_n$ , in predicting $f_n$ as the total measurement data grows.

Next, we account for finite data and define the finite sample analogue for the spectral representation theorum. We suppose there exists a set of arbitrary, fixed frequencies  $\{\omega_j\}$  for $j = 1, \hdots , J$. We let $n$ denote finite time steps for observing $\state_n$ at $n= 1, \hdots, N$. Further, we define a set of zero mean, mutually and serially uncorrelated random process  $\{a_j \}$ and $\{b_j\}$ as finite sample analogues of the true  $a(\omega)$ and $b(\omega)$ for the $j$-th spectral component. In particular, these processes are constant over $n$ by covariance stationarity of $\state$. Then, the finite sample analogue for the spectral representation theorum becomes \cite{hamilton1994time}:
\begin{align} 
\state_n &= \mu_f  + \sum_{j=1}^{J}  [ a_j \cos(\omega_j n) +  b_j \sin(\omega_j n) ] \\
\ex{a_j} & = \ex{b_j} = 0\\
\ex{a_ja_{j'}} &= \ex{b_jb_{j'}} = \sigma^2 \delta(j - j') \\
\ex{a_jb_{j'}} &= 0 \quad \forall j, j' 
\end{align}

The first two moments are of the form:
\begin{align}
\ex{f_n} &=  \mu_\state +  \sum_{j=0}^{J} E[a_j] \cos(\omega_j n) + E[b_j] \sin(\omega_j n)  = 0\\
R(\nu) &= \sum_j^{J} \sum_{j'}^{J} \sigma_j^2\delta{j,j'} [\cos(\omega_j n)\cos(\omega_j' (n+\nu)) + \sin(\omega_j n)\sin(\omega_j' (n+\nu)) ]\\
% &= \sum_j^{J} \sigma_j^2 \cos(\omega_j\nu), \quad \text{$(\cos(a)\cos(b) + \sin(a)\sin(b) = \cos(a-b))$} \\
&= \sigma^2 \sum_j^{J}  p_j \cos(\omega_j\nu) \\
p_j & \equiv \frac{\sigma_j^2}{\sigma^2} \equiv \frac{\sigma_j^2}{\sum_j \sigma_j^2} 
\end{align}

We introduce process noise, $w_n$, into the formula for true $f_n$, and this establishes the link with Kalman filtering:
\begin{align} 
\state_n &= \mu_f  + \sum_{j=1}^{J}  [ a_j \cos(\omega_j (n-1)) +  b_j \sin(\omega_j (n-1)) ] + w_n 
\end{align}

In the absence of measurement noise and operating in the oversampling regime, an ordinary least squares (OLS) regression can be constructed by providing a collection of $J^{(B)}$ basis frequencies $\{\omega_j^{(B)}\}$, as in \cite{hamilton1994time}. The OLS problem is constructed by separating the set of coefficients $\{\hat{\mu}_\state, \hat{a}_1, \hat{b}_1, \hdots \hat{a}_J, \hat{b}_J\}$ and regressors $\{1,\cos(\omega_1 (n-1)), \sin(\omega_1 (n-1)), \hdots, \cos(\omega_J^{(B)} (n-1)), \sin(\omega_J^{(B)} (n-1)) \}$. For the specific particular choice of basis,  $J^{(B)} = (N-1)/2$, (odd $N$) and $\omega_j^{(B)} \equiv 2\pi j / N$, we state the key result from \cite{hamilton1994time} that the coefficient estimates are obtained as:

\begin{align}
\hat{\state}_n &= \hat{\mu}_f  + \sum_{j=1}^{J^{(B)}}  [\hat{a}_j \cos(\omega_j^{(B)} (n-1)) +  \hat{b}_j \sin(\omega_j^{(B)} (n-1)) ] \\
\hat{a}_j &\equiv \frac{2}{N} \sum_{n'=1}^{N} \hat{\state}_{n'} \cos(\omega_j^{(B)}(n'-1)) \\
\hat{b}_j &\equiv \frac{2}{N} \sum_{n'=1}^{N} \hat{\state}_{n'} \sin(\omega_j^{(B)}(n'-1))
\end{align}
This choice of basis results in the number of regressors being the same as the length of the measurement record. Further, the term $(\hat{a}_j^2 + \hat{b}_j^2)$ is proprotional to the total contribution of the $j$-th spectral component to the total sample variance of $\state$, or in other words, the amplitude estimate for the power spectral density of true $\state$.

Next, we depart from the OLS problem above by in several ways, firstly, by introducing measurement noise and secondly, by changing basis oscillators considered in the problem above. As in the main text, the linear measurement record is defined as:
\begin{align}
y_n &\equiv  f_n + v_n 
\end{align}
The link in GPR (periodic kernel) is direct and the link with LKFFB is made by setting $f_n \equiv H_nx_n$. In both frameworks, we incorporate the effect of measurement noise through the measurement noise variance, $R$, which has the effect of regularising the least squares estimation process discussed above {\color{red} [XX REF, e.g. \cite{west1996bayesian}]}

\subsection{GPR (Periodic Kernel)}\label{sec:ap_approxSP:GPRPKernel}

The departure from simple OLS to GPR (periodic kernel) arises from the fact that data is projected on an infinite basis of oscillators, namely, $J^{(B)} \to \infty$, and we follow high level remarks in \cite{solin2014explicit} to illustrate this below.

We use high level remarks in \cite{solin2014explicit} to explicitly work out that a sine squared exponential (Periodic Kernel) used in Gaussian Process Regression satisfies covariance function of trigometric polynomials. Here, the indice $j$ labels an infinite comb of oscillators and $m$ represents the higher order terms in the power reduction forumulae in the last line of the definition below:
\begin{align}
\omega_0^{(B)}  &\equiv \frac{\omega_j^{(B)} }{j}, j \in \{0, 1,..., J^{(B)}\} \\
R(\nu) &\equiv \sigma^2 \exp (- \frac{2\sin^2(\frac{\omega_0^{(B)}  \nu}{2})}{l^2}) \\
&=  \sigma^2 \exp (- \frac{1}{l^2}) \exp (\frac{\cos(\omega_0^{(B)}  \nu)}{l^2}) \label{eqn:periodic_0}\\
&=  \sigma^2 \exp (- \frac{1}{l^2}) \sum_{m = 0}^{M  \to\infty} \frac{1}{m!} \frac{\cos^m(\omega_0^{(B)}  \nu)}{l^{2m}} \label{eqn:periodic_1}
\end{align}
Next, we expand each cosine using power reduction formulae for odd and even powers respectively, and we re-group terms. For example, we expande the terms for  $m = 0,1,2,3,4,5...$ as:
\begin{align}
R(\nu) &= \sigma^2 \exp (- \frac{1}{l^2}) \cos(\omega_0^{(B)}  \nu) \left[ \frac{2}{(2l^2)}\binom{1}{0} + \frac{2}{(2l^2)^3} \frac{1}{3!} \binom{3}{1} +  \frac{2}{(2l^2)^5} \frac{1}{5!}\binom{5}{2} \dots \right] \label{eqn:cosine1}\\
& + \sigma^2 \exp (- \frac{1}{l^2}) \cos(2\omega_0^{(B)}  \nu) \left[ \frac{2}{(2l^2)^2} \frac{1}{2!} \binom{2}{0} + \frac{2}{(2l^2)^4} \frac{1}{4!} \binom{4}{1} + \dots \right] \\
& + \sigma^2 \exp (- \frac{1}{l^2}) \cos(3\omega_0^{(B)}  \nu) \left[ \frac{2}{(2l^2)^3} \frac{1}{3!} \binom{3}{0} + \frac{2}{(2l^2)^5} \frac{1}{5!}\binom{5}{1} \dots \right] \\
& + \sigma^2 \exp (- \frac{1}{l^2}) \cos(4\omega_0^{(B)}  \nu) \left[ \frac{2}{(2l^2)^4} \frac{1}{4!} \binom{4}{0} + \dots \right] \\
& + \sigma^2 \exp (- \frac{1}{l^2}) \cos(5\omega_0^{(B)}  \nu) \left[ \frac{2}{(2l^2)^5} \frac{1}{5!}\binom{5}{0} + \dots \right] \label{eqn:cosine5}\\
& \vdots \nonumber \\
& + \sigma^2 \exp (- \frac{1}{l^2}) \left[ \frac{1}{(2l^2)^2} \frac{1}{2!} \binom{2}{1} + \frac{1}{(2l)^4} \frac{1}{4!} \binom{4}{2} + \dots \right] + \sigma^2 \exp (- \frac{1}{l^2}) \label{eqn:eventerms}
\end{align}
In the expansion above, the vertical and horizontal dots represent contributions from $m>5$ terms. The key message is that truncating $m$ to a finite number of terms $M$ will forecably truncate $j$ to represent a finite number of oscillators. For the example above, if the power reduction expansion indexed by $m$ above was trucated to $M=5$ terms, then the  number of basis oscillators (number of rows) would also be truncated.  We now summarise the amplitudes \cref{eqn:cosine1} to  \cref{eqn:cosine5} in second term of $R(\nu)$ and  \cref{eqn:eventerms} corresponds to $p_{0,M}$ term below:
\begin{align}
R(\nu) &= \sigma^2 (p_{0,M} + \sum_{j=0}^{\infty} p_{j,M} \cos(j\omega_0^{(B)}  \nu))\\
p_{j,M} & \equiv \sigma^2 \exp (- \frac{1}{l^2}) \sum_{\beta = 0}^{\beta = \beta_{j,m}^{MAX}} \frac{2}{(2l^2)^{(j + 2\beta)}} \frac{1}{(j + 2\beta)!} \binom{j + 2\beta}{\beta} \label{eqn:beta_series2} \\
\beta &\equiv  0,1,..., \beta_{j,m}^{MAX}  \\
p_{0,M} &= \exp (- \frac{1}{l^2}) \sum_{\alpha = 0}^{\alpha = \alpha_{m}^{MAX}} \frac{1}{(2l^2)^{(2\alpha)}} \frac{1}{(2\alpha)!} \binom{2\alpha}{\alpha} \label{eqn:alpha_series}\\
\alpha &\equiv  0,1,..., \alpha_{m}^{MAX} 
\end{align}
By examining the cosine expansion, one sees that a truncation at $(M, J^{(B)} )$ means our summarised formulae will require $\beta_{j,M}^{MAX} = \lfloor\frac{M-j}{2}\rfloor$ and $\alpha_{M}^{MAX} = \lfloor\frac{M}{2}\rfloor$  where $\lfloor \rfloor$ denotes the ceiling floor. If we truncate with $M \equiv J^{(B)} $ such that $\alpha_{M}^{MAX} = \lfloor\frac{J^{(B)} }{2}\rfloor, \beta_{j,M}^{MAX} =  \lfloor\frac{J-j}{2}\rfloor $ and re-adjust the kernel for the zero-th frequency term, then we agree with results in \cite{solin2014explicit}.

In a truncated form, it is easier to see the correspondence with a covariance function for $f_n$ appoximating a covariance stationary process under the spectral representation theorum. We note, however, that the covariances $p_{j,M}$ are specified exactly by \cref{eqn:beta_series2} and this is not identical to those under the spectral representation theorum. Further, the periodicity of the kernel set by $\omega_0^{(B)} $ means that it may define non-stationary processes given the choices of hyperpameters, $\omega_0^{(B)}, l$ for a particular time series application.  

\subsection{LKFFB}
 In LKFFB, we depart from the OLS problem considered earlier by specifying a fixed basis of oscillators at the physical Fourier resolution established by the measurement record, and incorporating apriori assumptions about the extent to which a fast measurement action oversamples slowly drifting non-Markovian noise.  

Under the following correlation relations (below), and the Gaussian noise assumption, we see that LKFFB defines a stack of stochastic processes on a circle in \cref{eqn:cov_circle}, with the posterior Kalman state acting as the initial state for the next time step, such that $\nu = 0$, for each basis frequency:
 \begin{align}
 \ex{w_n} &= 0 \\
\ex{w_n,w_m} &= \sigma^2 \delta_{n,m}  \\
\ex{A^j_0} &=\ex{B^j_0} = 0 \\
\ex{A^j_n B^j_m} &= 0 \\
\ex{A^j_n A^j_m} &= \ex{B^j_n B^j_m} = \sigma_j^2 \delta_{n,m} \\
\ex{w_n A^j_m} &= \ex{w_n B^j_m} \equiv 0 \quad  \forall n, m \\
\end{align}
Consider a $j$-th substate, $x^j_n$, in the LKKFB, we obtain:
\begin{align}
\Phi(j \omega_0 \Delta t) &= \begin{bmatrix} \cos(j \omega_0 \Delta t) & -\sin(j \omega_0 \Delta t) \\ \sin(j \omega_0 \Delta t) & \cos(j \omega_0 \Delta t) \\ \end{bmatrix} \\
x^j_n & \equiv \begin{bmatrix} A^j_{n} \\ B^j_{n} \\ \end{bmatrix} = \Phi(j \omega_0 \Delta t) \left[\idn + \frac{w_{n-1}}{\sqrt{A^j_{n-1}{}^2 + B^j_{n-1}{}^2}} \right] \begin{bmatrix} A^j_{n-1} \\ B^j_{n-1} \\ \end{bmatrix} \\
\end{align}
\begin{align}
\implies \ex{x^j_n} &= 0 \\
\implies \ex{x^j_n x^j_m{}^T}_j & =   \Phi(j \omega_0 \Delta t) \ex{\begin{bmatrix} A^j_{n-1}A^j_{m-1} & A^j_{n-1}B^j_{m-1}\\ B^j_{n-1}A^j_{m-1} & B^j_{n-1}B^j_{m-1}\\ \end{bmatrix}} \Phi(j \omega_0 \Delta t)^T \label{eqn:cov_kf_term1}\\
& +   \Phi(j \omega_0 \Delta t) \left[\frac{w_{n-1}}{\sqrt{A^j_{n-1}{}^2 + B^j_{n-1}{}^2}} + \frac{w_{m-1}}{\sqrt{A^j_{m-1}{}^2 + B^j_{m-1}{}^2}} \right]\begin{bmatrix} A^j_{n-1}A^j_{m-1} & A^j_{n-1}B^j_{m-1}\\ B^j_{n-1}A^j_{m-1} & B^j_{n-1}B^j_{m-1}\\ \end{bmatrix} \Phi(j \omega_0 \Delta t)^T  \label{eqn:cov_kf_term2}\\
& +   \Phi(j \omega_0 \Delta t) \left[\frac{w_{n-1}w_{m-1}}{\sqrt{A^j_{n-1}{}^2 + B^j_{n-1}{}^2}\sqrt{A^j_{m-1}{}^2 + B^j_{m-1}{}^2}} \right]\begin{bmatrix} A^j_{n-1}A^j_{m-1} & A^j_{n-1}B^j_{m-1}\\ B^j_{n-1}A^j_{m-1} & B^j_{n-1}B^j_{m-1}\\ \end{bmatrix} \Phi(j \omega_0 \Delta t)^T \label{eqn:cov_kf_term3} \\
& = \sigma^2_j \delta_{n,m} \begin{bmatrix} 
1 & 0 \\ 
0 & 1  \\
\end{bmatrix} \label{eqn:cov_kf_term4}
 \end{align}

The cross correlation terms disappear under the temporal correlation functions so defined, namely, if assume $n \geq m$, then states $A^j_{m-1}, B^j_{m-1}$ at $m-1$ at most have a $w_{n-2}$ term (for the case $n=m$) and cannot be correlated with a future noise term $w_{n-1}$. This is a special case where $\nu=0$ for the simple stochastic process on a circle for $\nu \equiv 0$ \cite{karlin}.  The zero lag arises as the dynamical model propagates a random state $\Delta t$ ahead in time.

The dynamical trajectory in LKFFB is linearised for small $\Delta t$.  The linearisation is an approximation to a true, continuous time determinstic trajectory defining a stochastic process on a circle. We briefly visit this continous time trajectory to specify the link between LKFFB and GPR (periodic kernel). Let $t$ denote the continuous time deterministic dynamics for random initial state given by $a_j, b_j$, with zero mean, and mutually and serially uncorrelated properties as before:
\begin{align}
x^j(t) & \equiv \begin{bmatrix} A(t)^j \\ B(t)^j \\ \end{bmatrix} \equiv \begin{bmatrix} \cos(\omega_j t) & -\sin(\omega_j t) \\ \sin(\omega_j t) & \cos(\omega_j t) \\ \end{bmatrix} \begin{bmatrix} a_j \\ b_j \\ \end{bmatrix} \\
E[x^j(t)]&= 0 \\%\ex{\begin{bmatrix} a_j \\ b_j \\ \end{bmatrix}} = \begin{bmatrix} 0 \\ 0 \\ \end{bmatrix} \\
E[x^j(t) x^j(t'){}^T]&= \begin{bmatrix} \cos(\omega_j t') & -\sin(\omega_j t') \\ \sin(\omega_jt') & \cos(\omega_jt') \\ \end{bmatrix} \begin{bmatrix} a_j \\ b_j \\ \end{bmatrix} \begin{bmatrix} a_j & b_j \\ \end{bmatrix} \begin{bmatrix} \cos(\omega_j t) & -\sin(\omega_j t) \\ \sin(\omega_j t) & \cos(\omega_j t) \\ \end{bmatrix} \\ 
&=\sigma^2 \begin{bmatrix} 
\cos(\omega_j\nu) & 0 \\ 
0 & \cos(\omega_j\nu)  \\
\end{bmatrix}, \quad \nu \equiv |t'-t| \label{eqn:cov_circle}
\end{align}
We see that the initial state variables, $a_j, b_j$, must be zero mean and i.i.d. variables for $x^j(t)$ to be covariance stationary. If $a_j, b_j$ are Gaussian, then the joint distribution, $x^j(t)$, remains Gaussian. Hence, the continuous time limit of the dynamics in LKKFB for $J^{(B)}$  independent substates, $x^j(t)$, describe a process with the same first and second moments for a periodic kernel truncated at $J^{(B)}$. For Gaussian processes, this results in an approximate equivalent representation of classical Kalman filtering for $J^{(B)}$ stacked resonators with the periodic kernel. The link between that GPR (periodic kernel) and classical Kalman filtering, and the approximation error which arises from an arbitrary truncation of the periodic kernel, is fullly articulated in \cite{solin2014explicit}. 

While the formalism of LKFFB shares a common structure with GPR (periodic kernel) in a particular limit, the  physical interpretation of $A^j_{m-1}, B^j_{m-1}$ is that these are components of the Hilbert transform of the original signal \cite{livska2007}. In particular, we can calculate the instantaneous amplitude and phase associated with each basis oscillator. The efficacy of the Liska Kalman Filter in our application assumes an appropriate choice of the `Kalman basis' oscilaltors. 

We note that the choice of basis effects the interpretation of the state estimates. To illustrate, consider the choice of Basis A - C defined below. Basis A depicts a constant spacing above the Fourier resolution (e.g. $\omega_0^{(B)} \geq \frac{2\pi}{N_T \Delta t}$). Basis B  instroduces a minimum Fourier resolution and effectively creates an irregular spacing if one wishes to consider a basis frequency comb coarser than the experimentally established Fourier spacing over the course of the experiment. Basis C is identical to Basis B but allows a projection to arbitrarily low (zero)frequency components. 
\begin{align}
\text{Basis A: } & \equiv \{0, \omega_0^{(B)}, 2\omega_0^{(B)} \dots  J^{(B)} \omega_0^{(B)} \} \\
\text{Basis B: } & \equiv \{ \frac{2\pi}{N \Delta t}, \frac{2\pi}{N \Delta t} + \omega_0^{(B)} , \dots,   \frac{2\pi}{N \Delta t} + J^{(B)} \omega_0^{(B)} \} \\
\text{Basis C: } & \equiv \{ 0, \frac{2\pi}{N \Delta t}, \frac{2\pi}{N \Delta t} + \omega_0^{(B)},  \dots,   \frac{2\pi}{N \Delta t} + J^{(B)} \omega_0^{(B)} \} 
\end{align}
 While one can propagte LKFFB with zero gain, it may be advantageous for predictive control applications to generate predictions in one calculation rather than recursively. This means we sum constributions over all $j\in J^B$ oscillators and we reconstruct the signal for all future time values in one calculation, without having to propagate the filter recursively with zero gain. The interpretation of the predicted signal, $\hat{s}_n$, requires an additional (but time-constant) phase correction term $\psi_C$ that arises as a byproduct of the computational basis (i.e. Basis A, B or C).  The phase correction term corrects for a gradual mis-alignment between Fourier and computational grids which occurs if one specifies a non-regular spacing inherent in Basis B or C. Let $n_C$ denote the time-step at which instantaneous amplitudes $\norm{\hat{x}^j_{n_C}}$ and instantaneous phase $\theta_{\hat{x}^j_{n_C}}$ is extracted for the $j$-th oscillator:
\begin{align}
\hat{f} &= \sum_j\norm{\hat{x}^j_{n_C}} \cos(m\Delta t \omega_j + \theta_{\hat{x}^j_{n_C}} + \psi_C), \\
& \quad  n, n_C \in N_T, \quad m \in N_P \nonumber \\
\psi_C & \equiv \begin{cases}
0,  \quad \text{(Basis A)} \\
\equiv  \frac{2\pi}{\omega_0^{(B)} } (\omega_0^{(B)} - \frac{2\pi}{N \Delta t}), \quad \text{(Basis B or C)} \\
\end{cases}
\end{align}

Next, we define an analytical ratio to define the optimal training time, $n_C$, at which Kalman predictions should commence. 
\begin{align}
n_C &\equiv \frac{1}{\Delta t \omega_0^{(B)}} = \frac{f_s}{\omega_0^{(B)}} \label{eqn:sec:ap_liska_fixedbasis_nC}
\end{align}
Consider an arbitrarily chosen training period, $N_T \neq n_C $.  For $f_s$ fixed, our choice of $N_T > n_C $ means we are achieving a Fourier resolution which exceeds the resolution of the LKFFB basis. Now consider $N_T< n_C$. This means that we've extracted information prematurely, and we have not waited long enough to project on the smallest basis frequency, namely, $\omega_0^{(B)}$.  In the case where data is perfectly projected on our basis, this has no impact. For imperfect learning, we see that instantaneous amplitude and phase information slowly degrades for $N_T > n_C$; and trajectories for the smallest basis frequency have not stablised for $N_T < n_C$. 

Of these choices, Basis A for $\omega_0^{(B)} \equiv \frac{2\pi}{N_T \Delta t}$ is expected to yield best performance, at the expense of computational load, and this is confirmed in numerical experiments. All results in this manuscript are reported for Basis A with $N_T \equiv \frac{1}{\Delta t \omega_0^{(B)}} = \frac{f_s}{\omega_0^{(B)}} $.




\newpage
% ##############################################################################
\section{Quantised Kalman Filter \label{sec:app:qkf}}
% ##############################################################################

In this Appendix, we attempt to revist classical statistical theory of amplitude quantisation, and we propose a manner in which a coin flip measurement action can be incorporated into the existing classical frameowrk.  In the classical framework, we rely on the filtering and estimation theory outlined by \cite{karlsson2005} and we summarise their results here for convenience. We suggest a way of using this classical framework for deriving a CRLB applicable to the QKF. 

\subsection{QKF (Summary Definitions)}
The definition of the QKF and the accompanying the numerical results stand of their own accord in the main text and are not revisited here. 

As stated in the main text, we summarise the QKF definitons below for easy reference:
\begin{align}
d_n &\equiv \mathcal{Q}(\tilde{y})\\
\tilde{y}_n &= z_n + v_n \\
z_n & \equiv  h(x_n[0])  \equiv h(f_n) \equiv \frac{1}{2}\cos(\state_{n}) \\
x_n & = \Phi_n x_{n-1} + \Gamma_n w_n  \equiv  \begin{bmatrix} f_{n} \hdots f_{n-q+1} \end{bmatrix}^T \\
H_n &\equiv \frac{d h(\state_n)}{d\state_n} =  -\frac{1}{2}\sin(\state_{n}) \\
w_n & \sim \mathcal{N}(0, \sigma^2) \quad \forall n \\
v_n & \sim \mathcal{N}(0, R) \quad \forall n
\end{align}

The quantisation action, $\mathcal{Q}$, is performed by a binomial coin toss in QKF, where the bias on the coin in the QKF is given by $\tilde{y}_n$:
\begin{align}
\mathcal{Q}: Pr(d_n| \tilde{y}_n, \state_{n}, \tau) & \equiv \mathcal{B}(n_{\mathcal{B}}=1;p_{\mathcal{B}}= \tilde{y}_n + 0.5 ) \label{eqn:app:coinflipquantiser}
\end{align}

The numerical studies for establishing the performance of the QKF are detailed in the main text. 

\subsection{Classical Amplitude Quantisation and CRLB for Kalman Filters}

Many classical situations are give rise to scenarios where a continuous time, continuous amplitude signal is discretised in both time and amplitude. The discretisation in time is governed by well known Nyquist and Fourier domain sampling theory. Amplitude quantisation can occur for $2m = 2^b$ levels, where $b$ is the number of bits (namely, $b=1$ for our case). Examples include a classical sensor of $b$ bits generates quantised data measurements, where there are  $2^m$ values a measurement can take; or analogue information is digitised using an $b$ bit ADC. [XXX REFERENCES]

In classical probability theory, the underlying probability distribution of a true continous random process is discretised by the process of amplitude-quantisation. In particular, amplitude quantisation is seen as a \emph{linear} map by which the probability distribution of the underlying true, continous signal plus noise is divided into $2m$ sections, and the area under each section is calculated and condensed into a discrete (area-sampled) probability density function \cite{widrow1996,karlsson2005}.  This is given by Eq. (3)-(6) in \cite{karlsson2005}. We re-write these equations suggestively in our notation, with $\star$ denoting a convolution and $l(d)$ denotes a pulse train defined by $m$ and the Dirac-delta function, $\delta(\cdot)$:

\begin{align}
Pr_d(d) & \equiv l(y) (Pr_z \star Pr_U)(d) \\
l(d) & \equiv \sum_{i = -m}^{m-1} \delta (d - i2^{-b} +  \frac{2^{-b}}{2} ) \\
Pr_U & \equiv \begin{cases} 2^b, \quad -\frac{2^{-b}}{2} \leq d \leq  \frac{2^{-b}}{2} & 0, \quad \text{otherwise} \end{cases}
\end{align}
If amplitudes are quantised into $2m= 2^b$ levels, then the term $2^{-b}$ represents the size of the quantisation box in the dual domain, namely, it specifies the width of the uniform probability density function (area) by which to sample $Pr_z$.  The resulting error between the the true and quantised process is found to be zero mean and of variance $2^{-b} / 12$ \cite{widrow1996}. In the derivation of quantisation as area-sampling, the procedure is described purely in terms of the convolution of an arbitrary $Pr_z$ with a uniform distribution. Hence, amplitude quantisation is analalogised to Nyquist sampling and interpolation procedures in the time domain \cite{widrow1996}. 
Since a convolution operator and the multiplication with $l(d)$ are all linear transformations, the above relation describes a linear map between a continous $z$ and the quantised signal $y$. 

The statistical properties of $\mathcal{Q}$ are captured entirely by the defintions above. Hence, literature suggests that one can interpret amplitude quantisation as a linear map between probability distributions between a prior (for a continuous signal, $z$) and a posterior (for a quantised signal, $d$); just as Bayes rule defines a different type of map between probabability distribution of true and measured quantities of interest. 

For a typical Kalman filter,  the state, $x_n$ and its variance, $P_n$, is propagated by a set of equations. The equations for propagating the variance alone are known as the Ricatti equations \cite{grewal2001theory}. In information filtering, the quantity of interest is not $P_n$, but Fisher information, namely the inverse of $P_n$. When uncertainty about the state $x_n$ is high, working with information filtering is considered easier as one can model infinite variance in stable way by setting $P_n^{-1}$ to zero. The true theoretical CRLB binds the value of the Kalman covariance from below:

\begin{align}
%Cov(\state_n- \hat{\state}_n) & \equiv \ex{(\state_n- \hat{\state}_n)(\state_n- \hat{\state}_n)^T} \\
Cov(x_n- \hat{x}_n) & \equiv \ex{(x_n - \hat{x}_n)(x_n- \hat{x}_n)^T} \\
& \succcurlyeq P_n 
\end{align}

Where the variance propagation for $P_n$ can be decoupled from state propagation for $x_n$, the Ricatti equations will yield $P_n$ in the absence of measurement data based on the design of the filter dynamics ($\Phi_n$), measurement model, and noise strength covariances.  If these equations are recursively updated with the true noise parameters, then one obtains a theoretical CRLB [XXX References]. In the case where state and variance propagation are coupled, one must obtain the theoretical CRLB by performing the recursion below with, additionally, the true state $x_n$ information. 

The key aspect of deriving the CRLB for Kalman filters on quantised sensor information is the introduction of an extra `Fisher information term' purely due to quantisation in \cite{karlsson2005}. These information equations are analagous to the Ricatti equations except for the addition of an extra Fisher information term, $I_{m=1, n}$, for a $m=1$ bit quantisation procedure. We show this by re-writing Eqs. (48-53) in \cite{karlsson2005} in our notation:
\begin{align}
P_{n+1}^{-1} &= Q_n^{-1} + I_{m=1, n+1} - S_n^T(P^{-1}_n + V_n)^{-1} S^T_n \label{eqn:app:CRLB_recursion} \\
S_n &\equiv -\Phi_n^T Q_n^{-1} \\
&\equiv -\Phi^T Q^{-1}, \quad \forall n \\
V_n & \equiv \Phi_n Q_n^{-1} \Phi_n^T\\
& \equiv \Phi Q^{-1} \Phi^T , \quad \forall n 
\end{align} 
For time invariant linear dynamical models and time invariance of noise covariance matrices, the recursion simplies further to yield time invariant intermediary matrices, $S, V$. This means that the only time varying term in the specified recursion is $I_{m=1, n+1}$. The time dependence is  $I_{m=1, n+1}$ arises due to a non-linear measurement action, namely, via $h(x_n)$. In particular, a non-linear measurement action often couples state propogation to variance propagation, and the same effect applies to the calculation of the Fisher Information term, $I_{m=1, n+1}$. The form of $I_{m=1, n+1}$ as given by Theorum 5 of \cite{karlsson2005}, where $I(x)$ defines the total Fisher information for $N$ observations in a measurement record:
\begin{align}
I(x) & \equiv \sum_{n=1}^{N} I_{m=1, n} \\
I_{m=1, n} & \equiv  H_n^T I_{m=1}(z_n) H_n  \\
I_{m=1}(z_n) & \equiv - \ex{\frac{\partial^2}{\partial z_n^2} \log Pr(d_n|z_n)} \label{eqn:app:quantisedFisherinfo}
\end{align}

The classical quantiser, $\mathcal{Q}$, defined as area-sampling of probability distributions, has a corresponding effect such that there is an analytical from for $Pr(d_n|z_n)$. This form is derived in Theorum 3 in \cite{karlsson2005} for $m=1$ quantisation procedure. We re-state the results from Theorum 3 in our notation as:

\begin{align}
Pr(d_n| z_n) & \equiv \delta(k) \rho(-z_n/ \sqrt{R}) + \delta(k-1) (1- \rho(-z_n/ \sqrt{R})), \quad k = 0, 1 \label{eqn:app:classicalquantiser}\\
\rho(-z_n/ \sqrt{R}) & \equiv \int_{-\infty}^{-z_n/ \sqrt{R}} \frac{1}{\sqrt{2\pi}} e^{-\frac{v^2}{2 R}} dv \\
\delta(k) &\equiv \begin{cases} 1, &k=0 \\ 0, &k \neq 0 \end{cases}
\end{align}
Here, $\rho(-z_n/ \sqrt{R})$ is the \emph{erf} function, namely the normalised cumulative probability distribution function for Gaussian distributed variables with finite limits. The derivation assumes that the error  between the true signal and its quantised valu, $v_n$, is zero mean, white Gaussian distributed with variance $R$. The derivation proceeds by stating that if $z_n + v_n < 0$, then we are much more likely to quantise $z_n$ in the bottom level corresponding to $z_n = -1/2, k = 0$. For the conditional probability where $z_n$ is given, this is the same as the probability of seeing a value of the error such that $v_n < -z_n$.  Hence, the probability is given by summing the the Gaussian probability distribution to a finite upper limit given by value of $-z_n$, resulting in the definition of $\rho(-z_n/ \sqrt{R})$ as the normalised \emph{erf} function.

The definitions of the classical framework and the QKF state space system completely define the CRLB for the classical quantisation procedure, if a box-quantisation scheme (also known as a mid-riser / mid-tread quantisation in engineering) is used instead of the coin-flip measurement action defined by \cref{eqn:app:coinflipquantiser}. 

If we had a classical quantisation procedure, we would have used the state space and CRLB equations above to plot CRLB for a completely equivalent system. The procedure is as follows. For each realisation of true $\state$, one computes the theoretical CRLB (using true $x$ information).  Next, one computes root mean square error of residuals from Kalman filtering with a noisy measurement record (using Kalman $\hat{x}$ estimates). For both quantities, one takes the expectation over multiple runs over an ensemble of different realisations of the true $\state$ and Kalman residuals are used to compute the root mean square error (RMSE) for each time step, $n$. The ensemble averaged square root of the trace of $P_n^{-1}$ obtained the CRLB recursion defines the CRLB when one wishes to compare this to the RSME for a particular choice of the Kalman Filter. The ratio of these quantities, RMSE/ CRLB, is always larger than unity, and a value of unity is optimal for unbiased state estimation.

\subsection{QKF: Departure from Classical Quantisation}

Our key point of departure from classical quantisation is therefore given by \cref{eqn:app:coinflipquantiser}. This corressponds to a departure in the calculation of $Pr(d_n | z_n)$ in $I_{m=1}(z_n)$ for the CRLB recursion. In particular, the arguments for $I_{m=1, n}$ follows from general arguments about the additivity of information under appropriately linearised maps, and the addition of $I_{m=1, n}$ into information filtering equations appears to be a general argument about the influence of the measurement action on variance propagation (in this case, quantised) \cite{karlsson2005}. Further, the fundamental interpretation of quantisation as area-sampling appears to be unchanged based on an explicit consideration of the key derivation in \cite{widrow1996} and commentary in \cite{karlsson2005}. Namely, that the convolution with a uniform distribution followed by multiplication with a pulse train does not \emph{explicitly} appear to be related to the calculation of  $Pr(d_n | z_n)$ using a Gaussian error model for $v_n$ in \cite{karlsson2005}. The properties of the pulse train and the uniform disribution are set by the level spacing between quantised amplitude levels, namely, the spacing given by $2^b$ in the amplitude domain, and $2^{-b}$ spacing in the probability (dual) domain. These are fixed by the choice of application (i.e. the continous-amplitude axis being quantised) and the number of bits $b$ (i.e. the $2m=2^b$ equally spaced levels for the new quantised-amplitude axis).  Implicitly, the procedure above may be subject to re-interpretation and we leave this as a question for subsequent work. 

At present, we consider the calculation of $Pr(d_n | z_n)$, with the understanding that  a coin flip measurement given by \cref{eqn:app:coinflipquantiser} is the only point of departure from the classical amplitude quantisation framework. This is the subject of the section below.

\subsection{Discussion: Calculating $Pr(d_n | z_n)$ for CRLB using Coin-Flip Quantiser}

In the language of the classical framework outlined in \cite{karlsson2005, widrow1996}, we propose that $d_n$ is the quantised signal (with area-sampled posterior distribution) with the true continous amplitude process being given by $\{z_n \}$. The true signal, $\{z_n\}$, is a sequence of stochastically drifting `biases' derived from repeated applications of the Born rule. Born's rule governs the naturally quantised outcomes of a qubit and we are able to incorporate additional apriori information about the measurement and quantisation process in our filtering algorithm. These are appropriately rescaled to be zero mean and symmetric around zero for numeric purposes, via the quantity $z_n$. In the ideal case, the quantity $z_n$ is used to set the bias of a coin before performing a single coin toss, namely the quantised action $\mathcal{Q}$.  One may question whether the value of $z$ is continuous between $n$ and $n+1$ - we assert that this follows from the slow, non-Markovian drift assumption about the underlying covariance stationary process, $\delta \omega (t)$.  

During the quantisation, there is uncertainty in our knowledge of $z_n$ as this is a theoretically unobservable quantity and exists only in the limit of infinite coin toss experiments (in the frequentist sense). Hence, the bias  of the binomial distribution in QKF is set by $\tilde{y}_n$. Just quantisation errors  from a classical procedure were considered in \cite{karlsson2005} and yielded a model for $Pr(d_n | z_n)$ based on $\rho(-z_n/ \sqrt{R})$,  we \emph{define} the quantities $Pr(d_n | z_n, v_n)$ and $ Pr( \tilde{y}_n | z_n)$ and we use an appropriate marginalisation over our error model for $v_n$ to yield $Pr(d_n | z_n)$.


The first term, $Pr(d_n | z_n, v_n)$ is obtained by considering repeated applications of the Born rule. For each independent time step, $n$, the Born rule gives us the likelihood function for obtaining an outcome:
\begin{align}
Pr(d_n=1 | f_n, t, \tau) & \equiv \cos^2(\frac{f_n}{2}) 
\end{align}
Note that is a likelihood function. A typical inference procedure is to consider a map between probability distributions using Bayes Rule. This is implemented under the Kalman framework naturally through the Kalman update equations. At present, we wish to define a map \emph{within} the Kalman measurement action, namely,a map between the probability distributions of continous ideal measurements, $z_n$, and quantised measurements, $d_n$. In this formalism, we interpret $Pr(d_n=k | f_n, t, \tau)$ as a likelihood providing probabilities of different observed outcomes, labelled by $k$, for $k \in \{0, 1\}$. We shift the likelihood function for $k=1$ to obtain a zero mean process $z_n$ as follows: 
\begin{align} 
Pr(d_n=k=1 | f_n, t, \tau) &\equiv Pr(1 | f_n, t, \tau)\\
 & \equiv \cos^2(\frac{f_n}{2}) \\
& = \frac{1}{2} +  \frac{1}{2}\cos(f_n) \\
& \equiv \frac{1}{2} +  z_n  \\
Pr(0 | f_n, t, \tau) & = 1 - Pr(1 | f_n, t, \tau)\\
& = \frac{1}{2} - z_n \label{eqn:app:bornrule:up}
\end{align}

In actuality, we use $\tilde{y}_n$ to set the bias of the binomial distribution for a single coin toss experiment,that is, our quantisation procedure reflects our uncertainty in the knowledge of the true, unobservable bias. This means that the distribution arising for coin toss experiments is obtained by $z_n \to \tilde{y}_n$ in \cref{eqn:app:bornrule:up}, and the calculation for all outcomes labelled by $k \in \{0, 1\}$ is:
\begin{align}
Pr(d_n=k | z_n, v_n) &\equiv  \delta(k-1) (\tilde{y}_n + \frac{1}{2})  +  \delta(k) ( \frac{1}{2} - \tilde{y}_n) \\
& =  \tilde{y}_n \left( \delta(k-1) - \delta(k) \right)  + \frac{1}{2}\left(\delta(k-1) + \delta(k) \right) \\
& = \tilde{y}_n \left( \delta(k-1) - \delta(k) \right)  + \frac{1}{2}
\end{align}
The distribution above is conditioned both on $z_n$ (corresponding to stochastic qubit phase $f_n$) and the uncertainty $v_n$, such that $\tilde{y} = z_n + v_n$ is given.  In the first line above, the first term corresponds to the probability of observing the $\ket{k}, k=1$, and the second term denotes the probability of observing the qubit in state $k=0$. In the next line, the term $\frac{1}{2}\left(\delta(k-1) + \delta(k) \right)$ always contributes a $1/2$ factor and this simplification results in the final expression. Physically, this means that a qubit under no dephasing noise remains on the equator of the Bloch sphere with no stochastic phase accumulation, and an equal probability of being in either state, $\ket{0}, \ket{1}$. This is exactly what one obtains, $Pr(d_n=k | z_n=0, v_n=0) = 0.5 \quad \forall k$ under zero dephasing. 

Next, we consider the error model for incorporating the uncertainity in the knowledge of the bias. The simplest construction is to re-apply the Gaussian assumption for $v_n$ under which typical Kalman filtering and estimations problems are conducted, but without the traditional interpretation that this $v_n$ corresponds to additive white Gaussian measurement noise:
\begin{align}
Pr( \tilde{y}_n | z_n) & \equiv Pr( z_n + v_n | z_n) \\
& \equiv Pr(v_n) \\
Pr(v_n)  & \sim \mathcal{N}(0, R)
\end{align}

This model for $v_n$ is an insufficient description for  the uncertainty in the our knowledge of true $z_n$. In particular, the value of $z$ (ideal) is bounded by the Born rule to be between $[-0.5, 0.5]$. We assume that $v_n$ is Gaussian distributed additive white noise with mean zero. However, Gaussian noise is not bounded, whereas we wish to saturate the values of $\tilde{y}_n$ between $[-0.5, 0.5]$. These saturation effects are commonly encountered in studies of classical quantisation and a discussion of appropriately saturating the values of random variable without effecting positivity of the underlying distribution is borrowed from \cite{widrow1996}. Namely, the techique we use is to convolve with a uniform distribution defined between the limits $(a,b)$, where $a=-0.5, b=0.5$. In this manner, $Pr( \tilde{y}_n | z_n) \to Pr( \tilde{y}_n | z_n) \star \mathcal{U}(a,b)$, where the $\star$ denotes a convolution. 

Implicitly, this means the addition of an abstract uniformly distributed random variable to $\tilde{y}_n$. This uniformly distributed random variable is referred to as `dithering noise', and the addition of dithering noise to a system involves designing noise in a way such that the inference problem is left uncorrupted. While the addition of dithering noise is valid in many signal processing applications, where one can access the system before quantisation, qubit outcomes are naturally quantised. As yet, we do not assign a physical interpretation of our abstract dithering noise and we use a convolution with uniform distribution merely as an analytical  technique below to appropriately saturate a probability distribution for $v_n$ without losing positivity. We note that some information is lost in the saturation procedure, and the extent to which this approach modifies the inference problem for the ideal Kalman measurement, $z_n$, based on quantised observed values $d_n$, remains an open question for future work.  This convolution is computed below (with $\tau$ a dummy variable not to be confused with the Ramsey time earlier):
\begin{align}
Pr( \tilde{y}_n | z_n) \star \mathcal{U}(a,b) & =  Pr( v_n) \star \mathcal{U}(a,b) \\
&= \frac{1}{\sqrt{2\pi R}} \left( \frac{1}{b-a} \right)\int_{a}^{b} e^{\frac{-(v_n - \tau)^2}{2R}} d\tau \\
&= \frac{1}{\sqrt{2\pi R}} \left( \frac{1}{b-a} \right) \left( \int_{0}^{b} e^{\frac{-(v_n - \tau)^2}{2R}} d\tau - \int_{0}^{a} e^{\frac{-(v_n - \tau)^2}{2R}} d\tau \right) \\
&= \frac{1}{\sqrt{2\pi R}} \left( \frac{1}{b-a} \right) \left( \frac{\pi}{\sqrt{2}} \right) \left( erf(b - v_n) - erf(a - v_n)\right) 
\end{align}


We now use these calculations and obtain an expression for $Pr(d_n | z_n)$ under a coin flip measurement action. To do this, we marginalising over all values of $v_n$ as follows:
\begin{align}
Pr(d_n=k | z_n) & \equiv  \int_{-\infty}^{\infty} dv_n \quad  Pr(d_n=k | z_n, v_n) \left( Pr( \tilde{y}_n = z_n + v_n| z_n) \star \mathcal{U}(a,b) \right)  \\
 & \equiv  \int_{-\infty}^{\infty} dv_n \quad  Pr(d_n=k | z_n, v_n) \left( Pr( v_n) \star \mathcal{U}(a,b) \right)  \\
&=  \frac{1}{\sqrt{2\pi R}} \left( \frac{1}{b-a} \right) \left( \frac{\pi}{\sqrt{2}} \right) \int_{a}^{b} dv_n \quad  Pr(d_n=k | z_n, v_n) \left( \left( erf(b - v_n) - erf(a - v_n)\right)  \right) \\
&=  \frac{1}{2\sqrt{2\pi R}} \left( \frac{1}{b-a} \right) \left( \frac{\pi}{\sqrt{2}} \right) \int_{a}^{b} dv_n \quad  \left( \left( erf(b - v_n) - erf(a - v_n)\right)  \right) \nonumber \\
&+  \frac{\left( \delta(k-1) - \delta(k) \right)}{\sqrt{2\pi R}(b-a)}  \left( \frac{\pi}{\sqrt{2}} \right) \int_{a}^{b} dv_n \quad  \tilde{y}_n \left( \left( erf(b - v_n) - erf(a - v_n)\right)  \right) \label{eqn:app:proposedquantiser:1}\\
&=  \frac{1}{2\sqrt{2\pi R}} \left( \frac{1}{b-a} \right) \left( \frac{\pi}{\sqrt{2}} \right) \int_{a}^{b} dv_n \quad  \left( \left( erf(b - v_n) - erf(a - v_n)\right)  \right) \nonumber \\
&+  \frac{\left( \delta(k-1) - \delta(k) \right)}{\sqrt{2\pi R}(b-a)}  \left( \frac{\pi}{\sqrt{2}} \right) \int_{a}^{b} dv_n \quad  (z_n + v_n) \left( \left( erf(b - v_n) - erf(a - v_n)\right)  \right) \label{eqn:app:proposedquantiser:2}\\
&=  \frac{\frac{1}{2}+ z_n \left( \delta(k-1) - \delta(k) \right)}{\sqrt{2\pi R}(b-a)}  \left( \frac{\pi}{\sqrt{2}} \right) \int_{a}^{b} dv_n \quad  \left( \left( erf(b - v_n) - erf(a - v_n)\right)  \right) \nonumber \\
&+  \frac{\left( \delta(k-1) - \delta(k) \right)}{\sqrt{2\pi R}(b-a)}  \left( \frac{\pi}{\sqrt{2}} \right) \int_{a}^{b} dv_n \quad  v_n \left( \left( erf(b - v_n) - erf(a - v_n)\right)  \right) \label{eqn:app:proposedquantiser:3}
\end{align}
The first line of \cref{eqn:app:proposedquantiser:1} describes a contribution that exists irrespective of the value of $z_n$ or the outcome $k$. The second line of \cref{eqn:app:proposedquantiser:1} qubit behaviour in the presence of dephasing noise and it is distribution over the two possible outcomes that can be observed, namely, $k \in \{ 0, 1\}$. In \cref{eqn:app:proposedquantiser:2}, we expand $\tilde{y}_n$ in terms of $z_n, v_n$. Since $v_n$ is serially uncorrelated and independent of $z_n$, it is safe to treat $z_n$ outside the integral [CHECK]. This yields \cref{{eqn:app:proposedquantiser:3}} where terms have been regrouped in terms of factors preceeding the integrals. The output of each integral is a number and the value of this number does not depend on the outcome under consideration, $k$, or the conditioning random variable, $z_n$. Hence, we can hide the various constants and obtain a simpler form:
\begin{align}
Pr(d_n=k | z_n) & = \rho_0 \frac{1}{2} + (\rho_0 z_n + \rho_1)\left( \delta(k-1) - \delta(k) \right)  \label{eqn:app:proposedquantiser:4:main}\\ 
\rho_0 & \equiv  \frac{1}{\sqrt{2\pi R}} \left( \frac{1}{b-a} \right) \left( \frac{\pi}{\sqrt{2}} \right) \int_{a}^{b} dv_n \quad  \left( \left( erf(b - v_n) - erf(a - v_n)\right)  \right) \label{eqn:app:proposedquantiser:5}\\
\rho_1 &  \equiv \frac{1}{\sqrt{2\pi R}} \left( \frac{1}{b-a} \right) \left( \frac{\pi}{\sqrt{2}} \right)  \int_{a}^{b} dv_n \quad  v_n \left( \left( erf(b - v_n) - erf(a - v_n)\right)  \right) \label{eqn:app:proposedquantiser:6} 
\end{align}

The expressions given by \cref{eqn:app:proposedquantiser:4:main,eqn:app:proposedquantiser:5,eqn:app:proposedquantiser:6} (with $a= -0.5, b=0.5$) is the final result for this derivation and the proposed quantity to substitute into \cref{eqn:app:quantisedFisherinfo} for the calculation of a Fisher Information corresponding to a quantiser given a coin flip action. To reiterate, our present understanding is that the rest of the classical formalism can be implemented without modification. 

We compare \cref{eqn:app:proposedquantiser:4:main} to the classical counterpart given by  \cref{eqn:app:classicalquantiser}. While there are some structural similarties, the marginalisation procedure over $v_n$  yields essentially two numbers and the time dependence of this term arises from the dependence of $z_n$. The next step - the calculation of the Fisher information via \cref{eqn:app:quantisedFisherinfo} - is UNKNOWN OMG AND COULD ALL BE A LIE .

\subsection{Discussion: Quantised Fisher Information Term for QKF}

The calculation of the Fisher information term for quantised measurements, $I_{m=1, n}$, enables one to calculate the CRLB for the QKF with a coin-flip measurement action. We now calculate and plot this term. Our objective is to compare the classical quantisation CRLB recursion results with a CRLB from a coin-flip quantisation. We wish to sense-check the derivations and assumptions above; and attempt to gain a theoretical insight into the numerical results reported for the QKF. 

To proceed, we expand the argument of \cref{eqn:app:quantisedFisherinfo} using the quotient rule, and we expand the expectation value by summing over all possible values of $k \in \{0,1 \}$, as in \cite{karlsson2005}. 

\begin{align}
\frac{\partial}{\partial z_n} \log Pr(d_n|z_n) & \equiv \frac{1}{Pr(d_n|z_n)}  \frac{\partial}{\partial z_n} Pr(d_n|z_n) \\ 
\frac{\partial^2}{\partial z_n^2} \log Pr(d_n|z_n) & = \frac{Pr(d_n|z_n)  \frac{\partial^2}{\partial z_n^2}Pr(d_n|z_n) - \left(  \frac{\partial}{\partial z_n} Pr(d_n|z_n) \right)^2 }{Pr(d_n|z_n)^2} \\
I_{m=1}(z_n) & \equiv - \ex{\frac{\partial^2}{\partial z_n^2} \log Pr(d_n|z_n)} \\
& = - \sum_{k\in \{0, 1\}}  Pr(d_n=k|z_n)  \frac{\partial^2}{\partial z_n^2} \log Pr(d_n=k|z_n) \\
& = - \sum_{k\in \{0, 1\}}  Pr(d_n=k|z_n)  \frac{Pr(d_n=k|z_n)  \frac{\partial^2}{\partial z_n^2}Pr(d_n=k|z_n) - \left(  \frac{\partial}{\partial z_n} Pr(d_n=k|z_n) \right)^2 }{Pr(d_n=k|z_n)^2}\\
& = - \sum_{k\in \{0, 1\}}  \frac{\partial^2}{\partial z_n^2}Pr(d_n=k|z_n) - \frac{\left(  \frac{\partial}{\partial z_n} Pr(d_n=k|z_n) \right)^2 }{Pr(d_n=k|z_n)}
\end{align}

We now find expressions for the calculation of the partial derivatives with respect to $z_n$: 
\begin{align}
Pr(d_n=k|z_n) 
& = \rho_0 \frac{1}{2} + (\rho_0 z_n + \rho_1)\left( \delta(k-1) - \delta(k) \right) \\
\frac{\partial}{\partial z_n} Pr(d_n=k|z_n) & = \rho_0 \left( \delta(k-1) - \delta(k) \right)\\
\frac{\partial^2}{\partial z_n^2}Pr(d_n=k|z_n) & = 0
\end{align}

Substituting this value into the Fisher information term yields:

\begin{align}
I_{m=1}(z_n)  & = - \sum_{k\in \{0, 1\}}  0 - \frac{\left( \rho_0 \left( \delta(k-1) - \delta(k) \right) \right)^2 }{ \frac{\rho_0}{2} + (\rho_0 z_n + \rho_1)\left( \delta(k-1) - \delta(k) \right)} \\
& = \sum_{k\in \{0, 1\}} \frac{\rho_0^2}{ \frac{\rho_0}{2} + (\rho_0 z_n + \rho_1)\left( \delta(k-1) - \delta(k) \right)} \label{eqn:app:applied_fisherinfo:0}
\end{align}

The result follows after observing that $\left( \delta(k-1) - \delta(k) \right)$ yields $\pm 1 $ for $k \in \{ 0,1 \}$, and hence, $\left( \delta(k-1) - \delta(k) \right)^2 \equiv 1, \quad \forall k$. 

We sense this derivation by computing the coefficients $\rho_0, \rho_1$ for $a=-0.5, b = 0.5$ and substituting into \cref{eqn:app:proposedquantiser:5,eqn:app:proposedquantiser:6}:

\begin{align}
 \int_{a}^{b} dv_n \quad  \left( \left( erf(b - v_n) - erf(a - v_n)\right)  \right) & \approx 0.97213 \\
 \int_{a}^{b} dv_n \quad  v_n \left( \left( erf(b - v_n) - erf(a - v_n)\right)  \right) & = 0 \\
\implies \rho_0 & \approx \frac{0.97213 \sqrt{\pi}}{2\sqrt{R}} = \frac{0.861528}{\sqrt{R}} \label{eqn:app:applied_fisherinfo:1}\\
\implies \rho_1 &  = 0  \label{eqn:app:applied_fisherinfo:2}
\end{align}

The first integral suggests that approximately $ 0.97213 $ of the information is retained following the saturation of $v_n$ between allowed values of $a, b$. The second integral will always be zero for any symmetric choices of $|a|=b$, and this is necessarily $0.5$ in our case for the interpretation of $z_n + v_n$ as a probability of success in a coin toss experiment. Hence, for our application, the second term will always be zero. 

Substituting \cref{eqn:app:applied_fisherinfo:1,eqn:app:applied_fisherinfo:2} into \cref{eqn:app:applied_fisherinfo:0} yields the Fisher information in simplified approximate form as:
\begin{align}
I_{m=1}(z_n) & = \sum_{k\in \{0, 1\}} \frac{\rho_0}{ \frac{1}{2} + z_n \left( \delta(k-1) - \delta(k) \right)}\\
& = \frac{0.861528}{\sqrt{R}} \sum_{k\in \{0, 1\}} \frac{1}{ \frac{1}{2} + z_n \left( \delta(k-1) - \delta(k) \right)} \\
& = \frac{0.861528}{\sqrt{R}} \left( \frac{1}{ \frac{1}{2} + z_n} + \frac{1}{ \frac{1}{2} - z_n} \right) \\
& = \frac{0.861528}{\sqrt{R}} \frac{4}{ 1 - 4z_n^2} \label{eqn:app:applied_fisherinfo:final}
\end{align}
We see that the Fisher information should have units of $[z^-2]$ (inverse variance) and this is true since $\sqrt{R}$ is inherited as a unitless scaling factor from the Gaussian distributed error model for $v_n$. In the absence of dephasing noise, $z_n=0$, and the Fisher information contribution for $k=0$ or $k=1$ to the total sum does not depend on the measurement outcome $k$. The physical interpretation is that the qubit has an equal probability of being in both $k$ states in the absence of dephasing noise, and hence, the associated information contributions are indistinguishable. Lasltly, we sense check this term with the classical case in \cite{karlsson2005} given by \cref{eqn:app:classicalquantiser}. We observe that both $I_{m=1}(z_n)$ terms take as input parameters the value of $z_n$ at each time step, and the scaling of this term is proportional to the noise covariance strength $\propto 1/\sqrt{R}$. No other inputs are required for computation of this term during the recursion for the CRLB, and hence, there is structural agreement between the classical and coin-flip versions of the quantisation procedure. However, this formula diverges at the boundaries, namely at $|z_n| \equiv 0.5$

Lastly, we consider the zero dephasing limit by revisting $Pr(d_n=k | z_n)$  and  we check whether \cref{eqn:app:proposedquantiser:1} provides a sensible value for $|a|=b=0.5$. In the event that a qubit experiences zero dephasing, $z_n=0$, at $n$, we physically expect \cref{eqn:app:proposedquantiser:1} to be precisely $1/2$ for any choice of $k$. However, our description of the error model (namely, $v_n \neq 0$ even if $z_n=0$) will create an artefact given by:
\begin{align}
Pr(d_n=k | z_n=0) & = \left( \frac{0.97213}{2} \right) \left( \frac{\sqrt{\pi}}{2\sqrt{R}} \right) \\
\end{align}
The surviving constants on the first line of \cref{eqn:app:proposedquantiser:1} are depicted above. Tthe second line of  \cref{eqn:app:proposedquantiser:1} disappears entirely for $z_n=0$ and $\rho_1 \equiv 0$. Of the surving terms, we interpret the first bracket as $\frac{0.97213}{2} \to 1/2$ if our uncertainty in the bias diappears, namely, the distribution for $v_n$ was not saturated between $[-0.5, 0.5]$. The term inside the second bracket are scaling factors that arise from our error model, $v_n$. Since $\sqrt{\pi} / 2 \approx 1.77 /2 $ which is a number slightly below unity, we see that the net artefact created by our description depends on the modelled strength of the uncertainty in our knowledge of the bias, $R$. When non-Markovian drift is present, the tuning of this abstract term, $R$, during the training of the QKF will ensure physically interpretable results. 

If, however, no non-Markovian drift exists, and the Kalman tuning procedure for QKF fails to converge on $R\approx \frac{\pi \times 0.97213^2}{4} \approx 0.74$ such that $Pr(d_n=k | z_n=0) \approx 0.5$, then one risks introducing a systematic bias given our description. In this context, one may wish to develop or test a tuning procedure for a physical QKF  implementation for an engineered case where no dephasing drift exists, and a strong rationale is needed for an optimiser to move away from $R \neq 0.74$.

\subsection{Classical CRLB vs. Coin Flip CRLB in Numerical Results Fig(8a)}

We plot and compare the CRLB given by the recursion equation in \cref{eqn:app:CRLB_recursion} for the results presented in Fig. 8(a) for the QKF in the main text. This case corresponds to a perfectly learned dynamical model and noise parameters, such that the effect of the non-linear quantised measurement action is being tested for a non-Markovian, covariance stationary $\state'$. The oversampling regime is slowly reduced and we see a report reduction in prediction horizon in the main text. 

For the true $\state'$, we calculate the CRLB using a Fisher information term for coin-flip measurements, \cref{eqn:app:applied_fisherinfo:final}, and the classical counterpart using the Fisher information term for quantised measurements given by \cref{eqn:app:classicalquantiser}. In all cases, we discard data for the first $100$ points, as the AKF model needs to accumulate enough measurements before dynamic updates can begin sensibly. We the expectation values of the Kalman residuals [not shown] lie above the CRLB by 2 orders of magnitude.  

In \cref{fig:app:CRLB}, we plot the ratio of the classical CRLB / coin-flip CRLB, where a ratio of unity implies that both calculations are approximately identical in the mean square limit. There are three key observations: firstly, the coin flip CRLB always lies below the classical counterpart for the cases considered, namely, the ratio is greater than unity for all cases. Secondly, the ratio is constant, which means that both CRLB approximately behave the same with $n$ for the cases considered. Thirdly, the ratio tends to unity as oversampling reduces and QKF predictive performance decreases. The applicability of these findings to general implementations remains an open question. 

\begin{figure}[h!]
    \includegraphics[scale=1]{CRLB} 
    \caption{ \label{fig:app:CRLB} TBC}
\end{figure}
\FloatBarrier
\newpage
% ##############################################################################
\section{Experimental Verification Procedure \label{sec:app:exptverfication}}
% ##############################################################################

We show that a stochastic detuning is indistingushable from the derivative of phase noise in a carrier enabling experimental verification of algorithms in this manuscript.
This is considered in \cite{soare2014} and we provide an alternative description that enables direct access to equation of motion for qubit probability amplitudes during the application of engineered carrier phase noise. 

We define system and interaction Hamiltonians for a two level system with energy splitting corresponding to $\omega_A$  interacting with a magnetic field. 
\\
\begin{align}
\op{\mathcal{H}}_0 &= \frac{1}{2} \hbar \omega_A \p{z} \\
\tilde{g} & \equiv \vec{g} \cdot \op{z} = \bra{1} \op{d} \cdot \op{z} \ket{2} \\
\op{\mathcal{H}}_{AF} & = -\tilde{g} \Omega(t)  \cos(\omega_\mu t + \phi(t)) \p{-} \\
& - \tilde{g}^* \Omega(t)  \cos(\omega_\mu t + \phi(t)) \p{+}
\end{align} Here, $\op{\mathcal{H}}_0$ is the system Hamiltonian, $\op{d}$ is the dipole operator for a spin half particle in a magnetic field, $\vec{g}$ is a complex system-field coupling term that inherits the direction of the dipole operator, $\tilde{g}$ is a complex scalar that captures the coupling strength, and $\op{\mathcal{H}}_{AF}$ is the Schoedinger picture atom-field interaction Hamiltonian in the rotating wave approximation.

The carrier with noise is identically defined as in \cite{soare2014}:
\begin{align}
\vec{B} &\equiv \Omega(t) \cos(\omega_\mu t + \phi(t)) \op{z} \\
\phi(t) & \equiv \phi_C(t) + \phi_N(t)
\end{align}
with a real control amplitude $\Omega(t)$, carrier frequency $\omega_\mu$, controlled phase $\phi_C(t)$ and stochastic phase noise $\phi_N(t)$.  It is possible to add amplitude noise to the control, i.e. $\Omega(t) \equiv \Omega_C(t) + \Omega_N(t)$, however we set $\Omega_N(t) =0$ at present. 

Substituting $\op{\mathcal{H}}_{N}$ for a stochastic detuning, $\delta \omega (t)$ from \cref{sec:app:setup}, the equations of motion for probability amplitudes of our two state system under dephasing, for arbitrary $\ket{\psi} = c_1 \ket{1} + c_2\ket{2} $ are:
\begin{align}
\dr{\pjs{1}{\psi}} &\equiv \dot{c_1} \\
& = \frac{-i}{\hbar}\bra{1} \op{\mathcal{H}}_0 + \op{\mathcal{H}}_{N} + \op{\mathcal{H}}_{AF}^{RWA}  \ket{\psi} \\
& = \frac{i(\omega_A - \delta\omega(t)) }{2}c_1 - \frac{i}{\hbar}Z_0\dd e^{i\omega_\mu t}c_2\\
\dr{\pjs{2}{\psi}} &\equiv \dot{c_2} \\
& = \frac{-i}{\hbar}\bra{2} \op{\mathcal{H}}_0 + \op{\mathcal{H}}_{N} + \op{\mathcal{H}}_{AF}^{RWA}  \ket{\psi} \\
& = -\frac{i(\omega_A - \delta\omega(t)) }{2}c_2 - \frac{i}{\hbar}Z_0^*\dd e^{-i\omega_\mu t}c_1 \\
\quad Z_0\dd &\equiv \frac{\Omega\dd\tilde{g}}{2}e^{i\phi\dd}.
\end{align}

The first interaction picture transformation is with respect to the carrier, namely, we define an arbitrary transformation as:
\begin{align}
\tilde{c}_1 &= c_1e^{i\lambda t} \\
\tilde{c}_2 &= c_2e^{-i\lambda t}
\end{align}
The equations of motion transform as follows:
\begin{align}
\dot{\tilde{c}}_1 & = \dot{c}_1 e^{i \lambda t} + i \lambda \dot{\tilde{c}}_1 \\
& =  i(\frac{\omega_A - \delta\omega(t)}{2} + \lambda)\tilde{c}_1 - \frac{i}{\hbar}Z_0\dd e^{i(\omega_\mu +2\lambda)t}\tilde{c}_2\\
& \nonumber \\
\dot{\tilde{c}}_2 & = \dot{c}_2 e^{-i \lambda t} - i \lambda \dot{\tilde{c}}_2 \\
& =  -i(\frac{\omega_A - \delta\omega(t)}{2} + \lambda)\tilde{c}_2- \frac{i}{\hbar}Z_0^*\dd e^{-i(\omega_\mu +2\lambda)t}\tilde{c}_1 \\
\end{align}
We set $2\lambda \equiv -\omega_\mu$ and $\Delta \omega(t) \equiv \omega_A - \delta\omega(t) -\omega_\mu$ to remove the time dependence due to the carrier, resulting in:
\begin{align}
\dot{\tilde{c}}_1 & =  \frac{i\Delta \omega(t)}{2}\tilde{c}_1 - \frac{i}{\hbar}Z_0\dd \tilde{c}_2\\
& \nonumber \\
\dot{\tilde{c}}_2 & =  -\frac{i\Delta \omega(t) }{2}\tilde{c}_2- \frac{i}{\hbar}Z_0^*\dd \tilde{c}_1\\
 \Rightarrow \op{\mathcal{H}}_{\omega_\mu}^{I} &\equiv \frac{\hbar \Delta \omega(t)}{2}\p{z} + Z_0\dd\p{-} + Z_0^*\dd\p{+}.
\end{align}
In the last line, $\op{\mathcal{H}}_{\omega_\mu}^{I}$ is the effective interaction picture Hamiltonian acting on the transformed state $\ket{\psi} = \tilde{c}_1\ket{1} + \tilde{c}_2\ket{2}$ with transformed field amplitudes $Z_0\dd$.

The second interaction picture transformation is with respect to a classical phase noise field. While only attainable in engineered phase noise demonstrations during experiment, the resulting interaction picture Hamiltonian reveals the indistinguisbaility of engineered phase noise and environmental dephasing, as asserted in \cite{soare2014}. We define an arbitrary transformation with respect to a time-varying classical quantity $\lambda (t)$:
\begin{align}
\alpha_1 &= \tilde{c}_1e^{i\lambda\dd} \\
\alpha_2 &= \tilde{c}_2e^{-i\lambda\dd}
\end{align}
The equations of motion transform as follows:
\begin{align}
\dot{\alpha}_1 & = \dot{\tilde{c}}_1 e^{i \lambda\dd} + i \dot{\lambda}\dd \alpha_1 \nonumber \\
& =  i(\frac{\Delta \omega(t)}{2} + \dot{\lambda}\dd)\alpha_1 - \frac{i}{\hbar}Z_0\dd e^{i2\lambda\dd}\alpha_2 \\
& \nonumber \\
\dot{\alpha}_2 & = \dot{\tilde{c}}_2 e^{-i \lambda\dd} - i \dot{\lambda}\dd\dot{\alpha}_2 \nonumber \\
& =  -i(\frac{\Delta \omega(t)}{2} + \dot{\lambda}\dd)\alpha_2- \frac{i}{\hbar}Z_0^*\dd e^{-i2\lambda\dd}\alpha_1
\end{align}
We substitute $Z_0\dd$ and set $2\lambda\dd \equiv -\phi_N\dd$ resulting in:

%%%%% [SECTION BELOW IS CHANGED FROM ORIGINAL NOTES] 
\begin{align}
\dot{\alpha}_1 & =  i\frac{\Delta \omega(t) - \dot{\phi}_N \dd}{2}\alpha_1 \nonumber \\ %%%% ADDED PHI_N
& - \frac{i}{2\hbar}(\Omega\dd \tilde{g} e^{i\phi_C\dd}) \alpha_2  \\
\dot{\alpha}_2 & =  -i\frac{\Delta \omega(t) - \dot{\phi}_N\dd}{2}\alpha_2 \nonumber  \\ %%%% ADDED PHI_N
& - \frac{i}{2\hbar}(\Omega\dd \tilde{g}^* e^{-i\phi_C\dd})\alpha_1  \\
\Rightarrow \op{\mathcal{H}}^{I}_{\omega_\mu,\phi_N\dd} &\equiv \frac{\hbar}{2}(\Delta \omega(t)- \dot{\phi}_N\dd)\p{z} \nonumber \\
& + \frac{\Omega\dd}{2} (\tilde{g} e^{i\phi_C\dd}\p{-} + \tilde{g}^* e^{-i\phi_C\dd}\p{+}) \label{eqn:Hi}
\end{align}
In the last line, $\op{\mathcal{H}}^{I}_{\omega_\mu,\phi_N\dd}$ is the effective interaction picture Hamiltonian acting on the transformed state $\ket{\psi} = \alpha_1\ket{1} + \alpha_2\ket{2}$. For the case where we set $\tilde{g}^* = \tilde{g} \equiv 1$ (real coupling constant), we recover the effective interaction picture Hamiltonian in \cite{soare2014}.


\newpage
% ##############################################################################
\section{Derivation of LKFFB (alternative to \cite{livska2007})} \label{sec:ap_liska_deriv}
% ##############################################################################
The Liska Kalman Filter has adaptive noise matrix, $Q$. We ensure that the standard classical  Kalman Filter derivations are unchanged by the presence of the proposed $Q$ in \cite{livska2007}. To derive the Kalman algorithm, we adapt the approach outlined in \cite{grewal2001theory} for standard applications. 

To begin, we search for a linear predictor $\hat{x}_n(+)$ defined by the unknown weights, $\lambda_n, \gamma_n$, at each time-step $n$:
\begin{align}
\hat{U} & \equiv \{y_1, y_2,\dots, y_n\} \\
\hat{x}_n(+) & \equiv \lambda_n \hat{x}_n(-) + \gamma_n y_n \label{eqn:KF_predictor}\\
\hat{x}_n(-) & \equiv \Phi_{n-1} \hat{x}_{n-1}(+) \\
(+) &\equiv \text{Aposteriori state estimate at $n$}\\
(-) &\equiv \text{Apriori state estimate at $n$}
\end{align}
To find unknowns $\lambda_n, \gamma_n$, we impose orthogonality of estimator to the set of all known data by invoking the general linear prediction theorum for covariance stationary processes, as per standard textbooks, (e.g. \cite{grewal2001theory,karlin2012first}). This means that the following must be satisfied:
\begin{align}
\ex{(x_n - \hat{x}_n(+))U^T} &= 0 \label{eqn:KF_reln_1}\\
\implies \ex{(x_n - \hat{x}_n(+))y_i^T} &= 0, \quad i = 1,\dots, n-1 \label{eqn:KF_reln_2}\\
\implies \ex{(x_n - \hat{x}_n(+))y_n^T} &= 0 \label{eqn:KF_reln_3}
\end{align}
First, we state the following properties of process and measurement noise are true:
With $n,m$ denoting time indices, the properties of noise processes are:
\begin{align}
\ex{w_n} & = \ex{v_n}= 0 \quad \forall n \label{eqn:KF_stat_noisemean}\\
\ex{w_nw_m^T} &= \sigma^2 \delta(m-n),  \quad \forall n,m \label{eqn:KF_stat_noisepros}\\
\ex{v_n v_m^T}&= R\delta(m-n), \quad  \forall n,m  \label{eqn:KF_stat_noisemsmt} \\
\ex{w_n v_m} &= 0,  \quad \forall n,m \label{eqn:KF_stat_crosscorr}
\end{align} 
The process noise variance, $\sigma^2$ is a known scalar. The measurement noise variance $R$ is a known covariance matrix in general, but will turn out to be a scalar for our choice of measurement action $H$. This means we can interpret \ref{eqn:KF_stat_crosscorr} as product between scalars. Based on noise properties and the state space models defined in the main text, we conclude that the following correlation relations are true:
\begin{align}
\ex{x_{n-1}w_{n-1}} & = 0 \label{eqn:KF_reln_4}\\
\ex{w_{n-1} y_i} &= 0, \quad i = 1,2,\dots n-1 \label{eqn:KF_reln_5}\\
\ex{\frac{x_{n-1}}{\norm{x_{n-1}}}w_{n-1}} & = 0 \label{eqn:KF_reln_6}\\
\ex{x_{n-1} w_{n-1} x_{n-1}^T } & = 0 \label{eqn:KF_reln_7}\\
\ex{\frac{x_{n-1}}{\norm{x_{n-1}}} w_{n-1} x_{n-1}^T } & = 0 \label{eqn:KF_reln_8} \\
\ex{x_{n-1} w_{n-1} y_i } & = 0, \quad i = 1,\dots, n-1 \label{eqn:KF_reln_9}\\
\ex{\frac{x_{n-1}}{\norm{x_{n-1}}} w_{n-1} y_i } & = 0, \quad i = 1,\dots, n-1 \label{eqn:KF_reln_10}
\end{align} We physically interpret these relations by observing that $x_{n-1}$ or $y_i, i = 1,2,\dots n-1$ do not contain a process noise term of the form $w_{n-1}$. At most, the expansion of $x_{n-1}, y_{n-1}$ terms contain $w_{n-2}$. Where terms of the form $w_{n-k}\dots w_{n-2}w_{n-1}$ appear, we invoke \ref{eqn:KF_stat_noisepros} and set these terms to zero. Where terms of the form $\propto w_{n-2}w_{n-2}w_{n-1}$ appear, we invoke uncorrelated process noise and zero mean process noise and set these terms to zero. The $\norm{x_{n-1}}$ term depends on $x_{n-1}$, and we assert that a similar logic holds where physically, a normed state cannot be correlated with a future process noise term. The set of correlation relations thus far imply that:
\begin{align}
\ex{x_n} &= \Phi_{n-1}\ex{x_{n-1}} \label{eqn:KF_reln_11}\\
\ex{(x_n - \apx{n})x_0^T} &= 0 \label{eqn:KF_reln_12}\\
\ex{(x_n - \apx{n})\hat{y}_n(-)^T} &=0 \label{eqn:KF_reln_13} \\
\ex{(x_n - \amx{n})y_i^T} &=0, \quad i = 1,\dots, n-1\label{eqn:KF_reln_14}\\
\ex{\Gamma_n w_n \apx{n}^T} &=0 \label{eqn:KF_reln_16}\\
\ex{\Gamma_n w_n x_n^T} &=0 \label{eqn:KF_reln_17} 
\end{align}
These relations are used to find unknown weights, $\lambda_n$ and $\gamma_n$. In particular, satisfying an orthogonality condition (\cref{eqn:KF_reln_2}) at different time steps yields the former, and satisfying an orthogonality condition at same time steps (\cref{eqn:KF_reln_3}) yields the latter (Kalman gain). 

First, we find $\lambda_n$ using \cref{eqn:KF_stat_noisemsmt,eqn:KF_reln_10,eqn:KF_reln_11,eqn:KF_reln_2}:
\begin{align}
&\ex{(x_n - \apx{n})y_i^T} =0, \quad i = 1,\dots, n-1\label{eqn:KF_reln_15}\\ 
\implies 0 & = \ex{(x_n - \apx{n})y_i^T} \\
& = \ex{(\Phi_{n-1}x_{n-1}(\idn + \frac{w_{n-1}}{\norm{x_{n-1}}}) - \lambda_n \hat{x}_n(-) - \gamma_n y_n)y_i^T} \\
& = \Phi_{n-1}\ex{x_{n-1}y_i^T} + \Phi_{n-1}\ex{\frac{x_{n-1}w_{n-1}}{\norm{x_{n-1}}}y_i^T}  - \lambda_n \ex{ \hat{x}_n(-)y_i^T} - \gamma_n H_n \ex{ x_n y_i^T}  - \gamma_n \ex{ v_n y_i^T}  \\
&= \Phi_{n-1}\ex{x_{n-1}y_i^T} + \Phi_{n-1}\ex{\frac{x_{n-1}w_{n-1}}{\norm{x_{n-1}}}y_i^T} - \lambda_n \ex{ \hat{x}_n(-)y_i^T} - \gamma_n H_n\ex{ x_n y_i^T}, \quad \text{by \ref{eqn:KF_stat_noisemsmt} } \\
&= \ex{\Phi_{n-1}x_{n-1}y_i^T} - \lambda_n \ex{ \hat{x}_n(-)y_i^T} - \gamma_n H_n\ex{ x_n y_i^T}, \quad \text{by \ref{eqn:KF_reln_10} } \\
&=\ex{x_{n}y_i^T} - \lambda_n \ex{ \hat{x}_n(-)y_i^T} - \gamma_n H_n\ex{ x_n y_i^T}, \quad \text{by \ref{eqn:KF_reln_11}} \\
&= \ex{x_{n}y_i^T} + \lambda_n \ex{( x_n - \hat{x}_n(-))y_i^T} - \lambda_n \ex{ x_n y_i^T}  - \gamma_n H_n\ex{ x_n y_i^T} \label{eqn:app:LKFFB_addproxy}  \\
&= \ex{x_{n}y_i^T} - \lambda_n \ex{ x_n y_i^T} - \gamma_n H_n\ex{ x_n y_i^T}\\
& = \ex{(\idn - \lambda_n  - \gamma_n H_n)x_n y_i^T} \\
&\ex{x_n y_i^T} \neq 0 \implies \lambda_n = \idn  - \gamma_n H_n
\end{align} We add $\pm \lambda_n \ex{ x_n y_i^T}$ in \cref{eqn:app:LKFFB_addproxy} to obtain the final result. 
The discussion so far has not made explicit reference to properties of measurement noise. The calculation of the Kalman gain incorporates information about incoming noisy measurements, and we define:
\begin{align}
e_n & \equiv \amx{n} - x_n,   \label{eqn:app:LKFFB_resid}\\
\amp{n} & \equiv E[e_ne_n^T] \label{eqn:app:LKFFB_P}\\
R_n & \equiv E[v_nv_n^T] \label{eqn:app:LKFFB_R}\\
E[v_n e_n^T]&=0 \label{eqn:app:LKFFB_uncorr_resid}
\end{align}
The Kalman Gain $\gamma_n$ satisfies orthogonality conditions at the same time step:
\begin{align}
0 &= \ex{(x_n - \apx{n})(\hat{y}_n(-) - y_n)^T} \\ 
& = E \left[ (x -\lambda_n \amx{n} - \gamma_n H_n x_n - \gamma_n v_n) \right. \\
& \left. (H_n (\amx{n} - x_{k}) -v_n)^T\right]  \label{eqn:app:LKFFB_apriori_y}\\
&= E\left[(x_n - \amx{n} + \gamma_n H_n\amx{n} - \gamma_n H_n x_n - \gamma_n v_n)(H_n (\amx{n} - x_{k}) -v_n)^T\right]  \\
&= E\left[(x_n - \amx{n} + \gamma_n H_n\amx{n} - \gamma_n H_n x_n - \gamma_n v_n)(H_n (\amx{n} - x_{k}) -v_n)^T\right].  \\
&= E\left[(-e_n + \gamma_n H_ne_n - \gamma_n v_n)(H_n e_n -v_n)^T\right]  \\
&= E\left[(-e_ne_n^T H_n^T+ \gamma_n H_ne_ne_n^T H_n^T - \gamma_n v_ne_n^T H_n^T + e_nv_n^T - \gamma_n H_ne_nv_n^T + \gamma_n v_nv_n^T \right]  \\
&= -\amp{n} H_n^T+ \gamma_n H_n\amp{n}H_n^T + \gamma_n R_n \\
& \implies \gamma_n = \amp{n} H_n^T(H_n\amp{n}H_n^T + R_n)^{-1} \label{eqn:KF_update_gamma}
\end{align} 

We use \cref{eqn:KF_reln_3,eqn:KF_reln_13} to intiate the derivation. We expand terms according to the design of our apriori and aposterori predictors, as well as the measurement action in \cref{eqn:app:LKFFB_apriori_y}. Terms are regrouped such that we substitute the expression for the residuals defined in \cref{eqn:app:LKFFB_resid}, enabling a straightforward simplification using (\cref{eqn:app:LKFFB_resid,eqn:app:LKFFB_P,eqn:app:LKFFB_R,eqn:app:LKFFB_uncorr_resid}) to get to the final result. 

The first Kalman equation - the measurement update to the true Kalman state - is:
\begin{align}
\apx{n} &= (1 - \gamma_nH_n) \amx{n} + \gamma_ny_n \\
& =  \amx{n} + \gamma_n(y_n - H_n\amx{n}) \\
&= \amx{n} + \gamma_n (y_n - \hat{y}_n(-)) \label{eqn:KF_update_x+} 
\end{align}
 The second Kalman update equation - the  update to the uncertainty of the true Kalman state - is:
\begin{align}
\apx{n} &=  \amx{n} - \gamma_nH_n \amx{n} + \gamma_nH_n x_n + \gamma_nv_n  \\
e^{+}_n &\equiv \apx{n} - x_n \\
&=  \amx{n} - \gamma_nH_n \amx{n} + \gamma_nH_n x_n + \gamma_nv_n - x_n \nonumber \\
 &=  e_n - \gamma_nH_n e_n + \gamma_nv_n \\
& = \left[1 - \gamma_nH_n \right] e_n + \gamma_nv_n  \\
\app{n} &\equiv \ex{e_n^+ e_n^{+T}} \\
& = E \left[ \left[1 - \gamma_nH_n \right] e_n e_n^T \left[1 - \gamma_nH_n \right]^T + \gamma_nv_n e_n^T \left[1 - \gamma_nH_n \right]^T   +  \left[1 - \gamma_nH_n \right] e_n v_n^T \gamma^T + \gamma_nv_n v_n^T \gamma^T \right] \\
& = \left[1 - \gamma_n H_n \right] \amp{n}\left[1 - \gamma_n H_n \right]^T + \gamma_n R_n \gamma_n^T  \\
& =  \amp{n} - \amp{n}H_n^T\gamma_n^T - \gamma_n H_n \amp{n} + \gamma_n \left[ H_n \amp{n}H_n^T +  R_n \right] \gamma_n^T, \quad \text{using $\gamma_n$}\\
&= \amp{n} - \amp{n}H_n^T\gamma_n^T - \gamma_n H_n \amp{n} +  \amp{n}H_n^T \gamma_n^T  \\ 
&= \left[1  - \gamma_n H_n \right] \amp{n} \label{eqn:KF_update_p+}
\end{align}
While the true Kalman state is propagated in time via the dynamical model, we must also propagate the Kalman estimate of the uncertainity in the true state. Unlike a typical Kalman filter, true Kalman state estimation cannot be decoupled from state variance estimation due to $\Gamma_n$ in LKFFB. This means Kalman gains cannot be calculated in advance of data collection. We confirm below that Kalman uncertainty estimates can be propagated as:
\begin{align}
e_n &= \amx{n} - x_n \\
&= \Phi_{n-1}\left[\apx{n-1} - x_{n-1} \right] - \Gamma_{n-1}w_{n-1} \\
&= \Phi_{n-1}e^{+}_{n-1} - \Gamma_{n-1}w_{n-1} \\
\amp{n} &= E[e_ne_n^T] \\
&= \Phi_{n-1}E\left[e^{+}_{n-1}e^{+T}_{n-1}\right]\Phi_{n-1}^T  - \Phi_{n-1}E\left[e^{+}_{n-1}w_{n-1}^T\Gamma_{n-1}^T\right] - E\left[\Gamma_{n-1}w_{n-1}e^{+T}_{n-1}\right]\Phi_{n-1}^T  \\
& + E\left[\Gamma_{n-1}w_{n-1}w_{n-1}^T\Gamma_{n-1}^T\right] \\
&= \Phi_{n-1}E\left[\app{n-1}\right]\Phi_{n-1}^T  + E\left[\Gamma_{n-1}w_{n-1}w_{n-1}^T\Gamma_{n-1}^T\right] - \Phi_{n-1}E\left[(\apx{n-1} - x_{n-1})w_{n-1}^T\Gamma_{n-1}^T\right] \nonumber \\
& - E\left[\Gamma_{n-1}w_{n-1}(\apx{n-1} - x_{n-1})^T\right]\Phi_{n-1}^T  \nonumber  \\
&= \Phi_{n-1} \app{n-1} \Phi_{n-1}^T + E\left[\Gamma_{n-1}w_{n-1}w_{n-1}^T\Gamma_{n-1}^T\right], \label{eqn:KF_update_p-_0} \\
&= \Phi_{n-1} \app{n-1} \Phi_{n-1}^T + Q_{n-1}, \label{eqn:KF_update_p-} \\
Q_{n} & \equiv E\left[\Gamma_{n}w_{n}w_{n}^T\Gamma_{n}^T\right] 
\end{align}
We use \cref{eqn:KF_reln_16,eqn:KF_reln_17} to obtain \cref{eqn:KF_update_p-} in the last step. Hence, the standard Kalman predictor equations - \ref{eqn:KF_update_gamma}, \ref{eqn:KF_update_x+}, \ref{eqn:KF_update_p+}, \ref{eqn:KF_update_p-} - are valid for adaptive noise features given in \cite{livska2007}, as stated in the main text. 
\newpage
\subsection{Proofs \ref{eqn:KF_reln_4} - \ref{eqn:KF_reln_17}:}

\begin{align}
\ex{x_{n-1} w_{n-1}} & = \ex{\Phi_{n-2}x_{n-2}(\idn + \frac{w_{n-2}}{\norm{x_{n-2}}}) w_{n-1}} \\
& = \ex{\Phi_{n-2}x_{n-2}w_{n-1}} +\ex{ \frac{\Phi_{n-2}x_{n-2} w_{n-2}}{\norm{x_{n-2}}} w_{n-1}} \\
& = \ex{\Phi_{n-2}x_{n-2}w_{n-1}} \\
& = \ex{\Phi_{n-2}\Phi_{n-3}x_{n-3}(\idn + \frac{w_{n-3}}{\norm{x_{n-3}}})w_{n-1}} \\
& \vdots \\
& = \Phi_{n-2}\dots\Phi_{n-i+1}x_0\ex{w_{n-1}} \\
&= 0 \\
\nonumber \\ 
\ex{w_{n-1} y_i} &= \ex{w_{n-1}  H_{n-1} x_{n-1} + w_{n-1}v_{n-1}}  \\
 &= \ex{w_{n-1}  H_{n-1} x_{n-1}}, \quad\text{by \ref{eqn:KF_stat_crosscorr} } \\ 
 &= H_{n-1} \ex{w_{n-1} x_{n-1}} \\
 &= 0, \quad\text{by \ref{eqn:KF_reln_4}} \\
\nonumber \\ 
\ex{\frac{x_{n-1}}{\norm{x_{n-1}}}w_{n-1}} & = \ex{\frac{\Phi_{n-2}x_{n-2}(\idn + \frac{w_{n-2}}{\norm{x_{n-2}}})}{\norm{x_{n-1}}}w_{n-1}} \\
& = \ex{\frac{\Phi_{n-2}x_{n-2}w_{n-1} + \frac{w_{n-2}w_{n-1}}{\norm{x_{n-2}}}}{\norm{x_{n-1}}}} \\
& = \ex{\frac{\Phi_{n-2}x_{n-2}w_{n-1}}{\norm{x_{n-1}}} + \frac{w_{n-2}w_{n-1}}{\norm{x_{n-2}}\norm{x_{n-1}}}} \\
&\vdots \\
&= 0 \quad \text{since $x_{n-1, n-2, \dots}$, $\norm{x_{n-1,n-2, \dots}}$ cannot be correlated with future  noise, $w_{n-1}$.}\\
\nonumber \\ 
\ex{x_{n-1} w_{n-1} x_{n-1}^T} &= \ex{\Phi_{n-2}x_{n-2}(\idn + \frac{w_{n-2}}{\norm{x_{n-2}}}) w_{n-1} \Phi_{n-2}x_{n-2}(\idn + \frac{w_{n-2}}{\norm{x_{n-2}}})} \\
&= \ex{\Phi_{n-2}x_{n-2} w_{n-1} x_{n-2}^T\Phi_{n-2}^T(\idn + \frac{w_{n-2}}{\norm{x_{n-2}}})^2} \\
&= 0 \quad \text{since $x_{n-1, n-2, \dots}$, $\norm{x_{n-1,n-2, \dots}}$ cannot be correlated with future  noise, $w_{n-1}$.}
\end{align}
\ref{eqn:KF_reln_8} is justified identically to \ref{eqn:KF_reln_7}. Recognising that $y_i = H_i x_i + v_i$ and that $\ex{w_jv_i} = 0 \forall i,j$, we can justify \ref{eqn:KF_reln_9}, \ref{eqn:KF_reln_10} with the same reasoning as \ref{eqn:KF_reln_7}.
\begin{align}
\ex{x_n} &= \ex{\Phi_{n-1}x_{n-1}(\idn + \frac{w_{n-1}}{\norm{x_{n-1}}})} \\
&= \Phi_{n-1}\ex{x_{n-1}} + \Phi_{n-1}\ex{x_{n-1}\frac{w_{n-1}}{\norm{x_{n-1}}})} \\
&= \Phi_{n-1}\ex{x_{n-1}}, \quad \text{by \ref{eqn:KF_reln_6}} \\
\nonumber \\ 
\ex{(x_n - \apx{n})x_0} &= \ex{(x_n - \apx{n})y_1} \\
&= 0 \quad \text{by \ref{eqn:KF_reln_2}, for $i=1$.}
\end{align}
\begin{align}
\text{To prove } \ex{(x_n - \apx{n})\hat{y}_n(-)^T} &=0, \quad \text{note:} \\
\amx{n} & \equiv \Phi_{n-1} \apx{n-1} \label{eqn:amx}\\
\hat{y}_n(-) & \equiv H_n \amx{n} \\
&= H_n \Phi_{n-1} \apx{n-1} \\
&= H_n \Phi_{n-1} \lambda_{n-1} \amx{n-1} + H_n \Phi_{n-1} \gamma_{n-1} y_{n-1}, \quad \text{by \ref{eqn:KF_predictor}} \\
&= H_n \Phi_{n-1} \lambda_{n-1} \Phi_{n-2} \apx{n-2} + H_n \Phi_{n-1} \gamma_{n-1} y_{n-1} \\
&\vdots \nonumber \\
& = \beta_0 x_0 + \sum_{k=1}^{n-1} \beta_k y_k, \quad \text{$\beta_k$ is a deterministic coefficent $H, \Phi, \gamma, \lambda$ over $\{n\}$.}\\
\implies \ex{(x_n - \apx{n})\hat{y}_n(-)} &= \ex{(x_n - \apx{n})(x_0^T\beta_0^T  + \sum_{k=1}^{n-1} y_k^T \beta_k^T } \\
&= \ex{(x_n - \apx{n})x_0^T}\beta_0^T  + \sum_{k=1}^{n-1} \ex{(x_n - \apx{n})y_k^T} \beta_k^T  \\
&= \sum_{k=1}^{n-1} \ex{(x_n - \apx{n})y_k^T} \beta_k^T, \quad \text{by \ref{eqn:KF_reln_12}}\\
&=0, \quad \text{by \ref{eqn:KF_reln_2}} \\
\nonumber \\ 
\ex{(x_n - \amx{n})y_i^T} &= \ex{(x_n - \Phi_{n-1} \apx{n-1})y_i^T}, \quad i = 1,\dots,n-1 \\
&=0, \quad \text{by \ref{eqn:KF_reln_2}} \\
\nonumber \\
\ex{\Gamma_n w_n \apx{n}^T} &=\ex{\Gamma_n w_n  \hat{x}_n(-)^T \lambda_n^T} +  \ex{\Gamma_n w_n  y_n^T \gamma_n^T} \quad \text{by \ref{eqn:KF_predictor}}\\
&=\ex{\frac{\Phi_{n}x_{n}}{\norm{x_{n}}} w_n  \hat{x}_n(-)^T \lambda_n^T} +  \ex{\frac{\Phi_{n}x_{n}}{\norm{x_{n}}} w_n  y_n^T \gamma_n^T} \\
&=\ex{\frac{\Phi_{n}x_{n}}{\norm{x_{n}}} w_n  \hat{x}_n(-)^T \lambda_n^T} \quad \text{by \ref{eqn:KF_reln_10}} \\
&=\Phi_{n}\ex{\frac{x_{n}}{\norm{x_{n}}} w_n \apx{n-1}^T} \Phi_{n-1}^T \lambda_n^T \quad \text{by \ref{eqn:amx}} \\
&\vdots \quad \text{repeatedly apply \ref{eqn:KF_predictor}, \ref{eqn:KF_reln_10}, \ref{eqn:amx}} \\
&=0, \quad \text{past terms uncorrelated with future $w_n$} \\
\nonumber \\ 
\ex{\Gamma_n w_n x_n^T} &= \Phi_{n}\ex{\frac{x_{n}}{\norm{x_{n}}} w_n x_n^T} =0 \quad \text{by \ref{eqn:KF_reln_8} } 
\end{align}








\iffalse %%%%%%%%%% THIS BELONGS IN THE APPENDIX, NOT THE MAIN TEXT AS ITS A NUMERICAL DETAIL
\begin{align}
\sigma_k, R_k &\equiv \iota_0 10^{\iota_1} \\
\iota_0 & \sim \mathcal{U}[0, 1]\\
\iota_1 & \sim \mathcal{U}[\{ -\iota_{max}, -\iota_{max} + 1,  \hdots,  \iota_{min}\}]
\end{align}
Scale magnitudes are set by $\iota_1$, a random integer chosen with uniform probability over $\{ -\iota_{max}, \hdots, \iota_{min} \}$ where we set $\iota_{min} = 3, \iota_{max} = 8$  such that $10^{-\iota_{max}}$ is sufficiently high to avoid machine floating point errors from recursive calculations over $> 10^3$ measurements. Uniformly distributed floating points for $\sigma_k, R_k $ in each order of magnitude is set by $\iota_0$. 
\fi