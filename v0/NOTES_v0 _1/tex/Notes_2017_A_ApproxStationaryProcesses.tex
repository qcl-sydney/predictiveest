\subsection{Approximate Representations of Covariance Stationary Processes for State Dynamics} \label{sec:ap_approxSP}

In the absence of a theoretical dynamical model for the evolution of $\beta_z\dd$,the approximate representations of covariance stationary processes are used to conduct state estimation of $f(n)$ within our learning algorithms.

\begin{table}[h]
	\centering
	\begin{tabular}{lc|c|c} 
		\hline
		\textbf{Learning Algorithm} & Autoreg. Moving Avg  & Trig. Polynomials & Stat. Pro. on a Circle \\
		\hline
		\textit{Least Squares Filtering } & X & & \\
		\textit{Autoregressive Kalman Filtering } & X & &\\
		\textit{Liska Kalman Filtering with Fixed Basis } & & & X\\
		\textit{Gaussian Process Regression - Periodic Kernel*} & & X & \\
        \hline
        \textbf{Theory Def.} & Def. \ref{sec:ap_ssp_ar} &  Def. \ref{sec:ap_ssp_tp} &  Def. \ref{sec:ap_ssp_circle} \\
		\end{tabular}
	\caption[Learning algorithms: Approach to State Estimation]{Summary of state estimation techniques used by learning algorithms}
\end{table}
Associated proofs that any covariance stationary process can have approximate representations can be found in standard textbooks (e.g. \cite{karlin2012}). Here, we restate key definitions and results for convenience. 

\begin{defn} \label{sec:ap_ssp_ar} Wold's Decomposition. Any zero mean covariance stationary process with absolutely summable autocovariances has a representation as an Moving Average process of infinite order, MA($\infty$). Let ${f_n}$ be a covariance stationary process, $w$ a white noise process, $V_n$ deterministic, such that:
\begin{align}
\ex{w_n} & = 0 \\
\ex{w_n^2} & = \sigma^2  < \infty\\
\ex{w_nw_{n-j}} & = 0 \quad \forall n, \forall j\neq 0 \\
\ex{f_n} & = 0 \\
\gamma(j) & \equiv \ex{f_n f_{n-j}}, \quad \sum_{j=0}^{\infty} |\gamma(j)| < \infty \\
\text{Then: } f_n & = \sum_{j=0}^{\infty} c_j w_{n-j} + V_n  \\
\text{If $V_n \equiv 0 \quad \forall n$, then: } f_n & = \sum_{j=0}^{\infty} c_j w_{n-j} \quad \text{i.e. MS($\infty $) Process} \\
c_0 & = 1 \\
c_j & = \ex{\frac{f_n w_{n-j}}{\sigma^2}}
\end{align}
Wold's representation is unique with probability one as we can define a space of covariance stationary random variables, where we define an inner product $\ex{XY} \equiv \langle X,Y \rangle$ i.e. a Hilbert space. The inner product holds since covariance stationarity demands finite second moments, and uniqueness of representation follows by the properties of Hilbert spaces.
\\
\\
Any MA($\infty$) (or their generalisation via autoregressive moving average models ARMA($q, p$)) can be arbitrarily approximated by a so-called autoregressive process of order $q_m$ AR($q_m$), where convergence to true process falls with $m$. If a true zero mean ${f_n}$ is an AR($q$) process, $w$ a white noise process as before, then:
\begin{align}
\Phi(L) f_n & = w_n \\
L^r: f_n &\mapsto f_{n-r} \quad \forall r \leq n \quad \text{(lag operator)} \\
\Phi(L) & \equiv  1 - \phi_1 L - \phi_2 L^2 - ... - \phi_q L^q 
\end{align}
If the roots of polynomial equation defined by  $\Phi(L)$ lie outside the unit circle, then $\Phi(L)^{-1}$ exists such that $f_n = \Phi(L)^{-1} w_n$, and the weights $\{\phi_r: r = 1, ... q\}$ can be used to extract the spectral density and long run variance as:
\begin{align}
S(\omega) & = \frac{\sigma^2}{2 \pi }\frac{1}{|\Phi(e^{-i\omega})|^2} \label{eqn:sec:ap_ssp_ar_spectden}\\
\Sigma & = \frac{\sigma^2}{\Phi(1)^2}
\end{align}
\end{defn}

\begin{defn} \label{sec:ap_ssp_tp} Trigonometric Polynomials.  Any zero mean covariance stationary process can be represented as the mean square limit of a sequence of processes, called trignometric polynomials, defined below. Let $A_j, B_j$ be i.i.d random variables with mean,$\mu_j$, and variance, $\sigma_j^2$. We have not specified the distribution that $A_j, B_j$ are drawn from. In particular, let the abstract index be $j \in J, j = 0,1,2... $, and:
\begin{align}
\ex{A_k A_j} &= \ex{B_k B_j} = \sigma_j^2 \delta_{j,k} \quad \forall k,j \in J\\
\ex{A_k B_j} &= 0 \quad  \forall k,j \in J \label{eqn:karlin_trig_cross} \\
f_n &= \sum_{j=0}^{J} A_j \cos(\omega_j n) + B_j \sin(\omega_j n) \label{eqn:E_f_trigpolynomial} \\
\text{Then: }\ex{f_n} &=  \sum_{j=0}^{J} E[A_j] \cos(\omega_j n) + E[B_j] \sin(\omega_j n) \\
& =  \sum_{j=0}^{J}  \mu_j(\cos(\omega_j n) + \sin(\omega_j n)) \\
\text{$\ex{f_n}$ is constant $\forall n$} \quad \Leftrightarrow \mu_j &=0  \quad \forall j\\
\implies \ex{f_n} &=  0 \\
\ex{f_n f_{n+v}} &= \sum_j^{J} \sum_{j'}^{J} \sigma_j^2\delta{j,j'} [\cos(\omega_j n)\cos(\omega_j' (n+v)) + \sin(\omega_j n)\sin(\omega_j' (n+v)) ]\\
&= \sum_j^{J} \sigma_j^2 \cos(\omega_jv), \quad \text{$(\cos(a)\cos(b) + \sin(a)\sin(b) = \cos(a-b))$} \\
&= \sigma^2 \sum_j^{J}  p_j \cos(\omega_jv). \quad \text{$(p_j = \frac{\sigma_j^2}{\sum_j \sigma_j^2} = \frac{\sigma_j^2}{\sigma^2})$}
\end{align}
Consider $\omega_j =0$. For this frequency, \{$f_n$\} behaves like a perfectly correlated random variable (DC term), violating Assumption \ref{azm:Rbandlimit}. Consider $J \to \infty$, with all non zero $\omega_j$ equally represented, hence $p_j \to 0$ . Then we expect $R(v)$ is nearly zero for all $v\neq 0$ and will $R(v)=\sigma^2$ for $v=0$, that is, \{$f_n$\} behaves like a delta-correlated process. 
\end{defn}

\begin{defn} \label{sec:ap_ssp_circle} Stationary Processes on a Circle. Define a stochastic initial state and a deterministic evolution given by a rotation matrix. In particular, let  $A, B$ be random variables with mean, $\mu_{A,B}$, and variance, $\sigma_{A,B}^2$. Consider abstract index $t$ to denote time:
\begin{align}
x(t) &= \begin{bmatrix} x_1(t) \\ x_2(t) \\ \end{bmatrix} = \begin{bmatrix} \cos(t) & -\sin(t) \\ \sin(t) & \cos(t) \\ \end{bmatrix} \begin{bmatrix} A \\ B \\ \end{bmatrix} \\
E[x(t)]&= \begin{bmatrix} \mu_A \cos(t) - \mu_B \sin(t)\\ \mu_A \sin(t) + \mu_B \cos(t)\end{bmatrix}  \\
& = 0 \quad \text{(constant), $\iff \mu_{A,B} =0$} \\ 
E[x(t)x(s)^T]&= \begin{bmatrix} \cos(t) & -\sin(t) \\ \sin(t) & \cos(t) \\ \end{bmatrix} \begin{bmatrix} A \\ B \\ \end{bmatrix} \begin{bmatrix} A & B \\ \end{bmatrix} \begin{bmatrix} \cos(s) & \sin(s) \\ -\sin(s) & \cos(s) \\ %
\end{bmatrix} \\ 
&=\begin{bmatrix} 
\sigma_A^2\cos(t)\cos(s) + \sigma_B^2\sin(t)\sin(s) & (\sigma_A^2 - \sigma_B^2)\cos(t)\sin(s) \\ 
(\sigma_A^2 - \sigma_B^2)\cos(s)\sin(t) & \sigma_A^2 \cos(t)\cos(s) + \sigma_B^2\sin(t)\sin(s) \\ 
\end{bmatrix}, \text{$\iff E[AB] =0$} \\ 
&=\sigma^2 \begin{bmatrix} 
\cos(v) & 0 \\ 
0 & \cos(v)  \\
\end{bmatrix}, \text{$ \iff \sigma_A^2 = \sigma_B^2 = \sigma^2$ and with $v= |t-s|$} \label{eqn:cov_circle}
\end{align}
We see that the initial state variables, $A,B$, must be zero mean and i.i.d. variables for $x(t)$ to be covariance stationary. So far, we have not made any assumptions about the distributions that the initial state is drawn from, however, if $A,B$ are Gaussian, then the joint distribution, $x(t)$, remains Gaussian.
\end{defn}

The above selection of representations for covariance stationary processes are not exhaustive, but are reported here as they align with algorithms which appear to work best for our specific application.
\\
\\
When the form of the covariance function is the subject of a learning procedure, Bochner's theorum ensures that a particular class of generating functions, $\mu$, will always yield covariance functions to describe covariance stationary processes in the mean square limit.

\begin{thm} Bochner's Theorum \cite{rasmussen2006gaussian}. A complex valued  function $R(v), \quad v \in \Re^D$  is the covariance function of a covariance stationary mean square continuous complex valued random process on $\Re^D$ if and only if it can be represented as:  
\begin{align}
R(v) & = \int_{\Re^D} \exp^{2 \pi s v} d\mu(s) \label{eqn:bochnerthm}
\end{align}
for $\mu$ some positive finite measure. If $\mu$ has a density with respect to the Lebesque measure, then $R(v)$ and the density $S(s)$ are Fourier duals.
\end{thm}
Bochner's Theorum is widely quoted in Gaussian Process Regression (GPR) approaches to machine learning. The covariance function, $R(v)$ is commonly referred to as a kernel. The objective of GPR methods is to design kernel (equivalently, a covariance function) to best enable learning correlations in a dataset. The effectiveness of GPR methods, and other so-called kernel learning machines, is influenced by the choice of kernel, the optimisation over unknown kernel parameters, and the design of cost function to conduct optimisation. 
\\
\\
We implement Gaussian Process Regression with a Periodic Kernel and we use analytical arguments to simplify the corresponding optimisation problem over unknown parameters in $R(v)$. We consider standard and sophisticated machine learning kernels representing covariance stationary processes with complex power spectral densities and we justify their exclusion from detailed analysis in this paper on analytical and computationally pragmatic grounds, as summarised in Appendix \ref{sec:ap_gpr_kernels}. 

\subsubsection{Least Squares and Autoregressive Kalman Filter} \label{sec:ap_approxcov_lsakf}

We describe how the Least Squares Filter and the Autoregressive Kalman Filter satisfy Definition \ref{sec:ap_ssp_ar}.

The Least Squares Filter applies gradient descent algorithm to directly learn the weights, $\{\phi_j\}, j = 1, ... , q $, of the previous $q$ past measurements. For $n=1$, the LS Filter implements effectively an AR($q$) process. 
\\
\\
Any AR($q$) process can be recast (non-uniquely) into state space form easily:
\begin{align}
\Phi(L) f_n & = w_n \\
f_n & = (\phi_1 L +  \phi_2 L^2  + ... + \phi_q L^q) f_n + w_n \\ 
& = \phi_1 f_{n-1}  +  \phi_2 f_{n-2}   + ... + \phi_q f_{n-q} + w_n \\ 
\implies \begin{bmatrix}
f_{n} \\  
f_{n-1} \\ 
\vdots \\ 
f_{n-q + 2} \\ 
f_{n-q+1} 
\end{bmatrix} & = 
\begin{bmatrix}
\phi_1 & \phi_2 & \hdots & \phi_{q-1} & \phi_q \\ 
1 & 0 & \hdots & 0 & 0 \\  
0 & 1 & \ddots & \vdots & \vdots \\ 
0 & 0 & \ddots & 0 & 0 \\ 
0 & 0 & \hdots & 1 & 0 
\end{bmatrix}
\begin{bmatrix}
f_{n-1} \\  
f_{n-2} \\ 
\vdots \\  
f_{n-q + 1} \\ 
f_{n-q} 
\end{bmatrix}
 + \begin{bmatrix}
w_{n} \\  
0 \\ 
\vdots \\  
0 \\ 
0 
\end{bmatrix} \\
\Phi_{AKF} & \equiv 
\begin{bmatrix}
\phi_1 & \phi_2 & \hdots & \phi_{q-1} & \phi_q \\ 
1 & 0 & \hdots & 0 & 0 \\  
0 & 1 & \ddots & \vdots & \vdots \\ 
0 & 0 & \ddots & 0 & 0 \\ 
0 & 0 & \hdots & 1 & 0 
\end{bmatrix} \label{eqn:akf_Phi}\\
\implies Q_{AKF} & \equiv 
\begin{bmatrix}
\sigma^2 & 0 & \hdots & 0 & 0 \\ 
0 & 0 & \hdots & 0 & 0 \\  
0 & 0 & \ddots & \vdots & \vdots \\ 
0 & 0 & \ddots & 0 & 0 \\ 
0 & 0 & \hdots & 0 & 0 
\end{bmatrix} \label{eqn:akf_Q}
\end{align}

The matrix $\Phi_{AKF}$ is the dynamical model used to recursively propagate the unknown state during state estimation in the Autoregressive Kalman Filter.  The matrix $Q_{AKF}$ is process noise covariance matrix. The ${\phi_i}$ in $\Phi_{AKF}$ are ideally the output of a maximum likelihood optimisation problem. We side-step this optimisation problem by using ${\phi_i}$ from LS Filtering to define the dynamic model. Subsequently, the role of the Autoregressive Kalman Filter is to filter measurement noise. 

\subsubsection{Gaussian Process Regression with Periodic Kernel} \label{sec:ap_approxSP:GPRPKernel}

We use high level remarks in \cite{solin2014} to explicitly work out that a sine squared exponential (Periodic Kernel) used in Gaussian Process Regression satisfies Def. \ref{sec:ap_ssp_tp}:

\begin{align}
R(v) &\equiv \sigma^2 \exp (- \frac{2\sin^2(\frac{\omega_0 v}{2})}{l^2}) \\
\omega_0 &\equiv \frac{\omega_j}{j}, j \in \{0, 1,..., J\} \\
\text{Using } 2\sin^2(\frac{\omega_0 v}{2}) &=  1 - \cos(\omega_0 v) \quad \text{we get:}\\
R(v) &=  \sigma^2 \exp (- \frac{1}{l^2}) \exp (\frac{\cos(\omega_0 v)}{l^2}) \\
 &=  \sigma^2 \exp (- \frac{1}{l^2}) \sum_{n = 0}^{\infty} \frac{1}{n!} \frac{\cos^n(\omega_0 v)}{l^{2n}} \label{eqn:periodic_1}\\
&= \sigma^2 \exp (- \frac{1}{l^2}) \left[\sum_{n_{odd}}^{\infty} \frac{1}{n!} \frac{\cos^n(\omega_0 v)}{l^{2n}} + \sum_{n_{even}}^{\infty} \frac{1}{n!} \frac{\cos^n(\omega_0 v)}{l^{2n}} + 1 \right] \label{eqn:cosine_powers} \\ \begin{split} 
& =\sigma^2 \exp (- \frac{1}{l^2})  \left[ \sum_{n_{odd}}^{\infty} \frac{1}{2^{n-1}} \frac{1}{l^{2n}} \frac{1}{n!}  \sum_{k=0}^{\frac{n}{2} - \frac{1}{2}} \binom{n}{k} \cos(\omega_0v (n -2k))\right] \\
& +\sigma^2 \exp (- \frac{1}{l^2})  \left[ \sum_{n_{even}}^{\infty}  \frac{1}{2^{n-1}} \frac{1}{l^{2n}} \frac{1}{n!}  \sum_{k=0}^{\frac{n}{2} - 1} \binom{n}{k} \cos(\omega_0v (n -2k))\right] \\
& +\sigma^2 \exp (- \frac{1}{l^2})  \left[ 1 +  \sum_{n_{even}}^{\infty}  \frac{1}{2^{n}} \binom{n}{\frac{n}{2}} \frac{1}{l^{2n}} \frac{1}{n!} \right] \label{eqn:cosine_powers2}
\end{split} 
\end{align}
At \ref{eqn:cosine_powers}, we expand each cosine using power reduction formulae for odd and even powers respectively. The $n=0$ term gives rise to the constant $1$ term.  We can expand each of the sums in \ref{eqn:cosine_powers2} and then find a pattern that allows us to regroup the terms across summations. For example, if we expand for $n = 0,1,2,3,4,5...$, we get:
\begin{align}
\text{1st Line of \ref{eqn:cosine_powers2}} &= \frac{2}{(2l^2)}\binom{1}{0} \cos(\omega_0 v) + \frac{2}{(2l^2)^3} \frac{1}{3!} \left[\binom{3}{0} \cos(3 \omega_0 v) + \binom{3}{1} \cos(\omega_0v) \right]  \\
&+ \frac{2}{(2l^2)^5} \frac{1}{5!} \left[\binom{5}{0} \cos(5\omega_0v) + \binom{5}{1} \cos(3\omega_0 v) + \binom{5}{2} \cos(\omega_0 v)\right] + ... \\
\text{2nd Line of \ref{eqn:cosine_powers2}} &= \frac{2}{(2l^2)^2} \frac{1}{2!} \binom{2}{0}\cos(2 \omega_0 v) + \frac{2}{(2l^2)^4} \frac{1}{4!} \left[\binom{4}{0} \cos(4 \omega_0 v) + \binom{4}{1} \cos(2 \omega_0 v) \right] + ... \\
\text{3rd Line of \ref{eqn:cosine_powers2}} &=  1 + \frac{1}{(2l^2)^2} \frac{1}{2!} \binom{2}{1} + \frac{1}{(2l)^4} \frac{1}{4!} \binom{4}{2} + ...
\end{align}
Substituting and rearranging, $R(v)$ becomes:
\begin{align}
R(v) &= \sigma^2 \exp (- \frac{1}{l^2}) \cos(\omega_0 v) \left[ \frac{2}{(2l^2)}\binom{1}{0} + \frac{2}{(2l^2)^3} \frac{1}{3!} \binom{3}{1} +  \frac{2}{(2l^2)^5} \frac{1}{5!}\binom{5}{2} \dots \right] \label{eqn:cosine1}\\
& + \sigma^2 \exp (- \frac{1}{l^2}) \cos(2\omega_0 v) \left[ \frac{2}{(2l^2)^2} \frac{1}{2!} \binom{2}{0} + \frac{2}{(2l^2)^4} \frac{1}{4!} \binom{4}{1} + \dots \right] \\
& + \sigma^2 \exp (- \frac{1}{l^2}) \cos(3\omega_0 v) \left[ \frac{2}{(2l^2)^3} \frac{1}{3!} \binom{3}{0} + \frac{2}{(2l^2)^5} \frac{1}{5!}\binom{5}{1} \dots \right] \\
& + \sigma^2 \exp (- \frac{1}{l^2}) \cos(4\omega_0 v) \left[ \frac{2}{(2l^2)^4} \frac{1}{4!} \binom{4}{0} + \dots \right] \\
& + \sigma^2 \exp (- \frac{1}{l^2}) \cos(5\omega_0 v) \left[ \frac{2}{(2l^2)^5} \frac{1}{5!}\binom{5}{0} + \dots \right] \label{eqn:cosine5}\\
& \vdots \nonumber \\
& + \sigma^2 \exp (- \frac{1}{l^2}) \left[ \frac{1}{(2l^2)^2} \frac{1}{2!} \binom{2}{1} + \frac{1}{(2l)^4} \frac{1}{4!} \binom{4}{2} + \dots \right] + \sigma^2 \exp (- \frac{1}{l^2}) \label{eqn:eventerms}
\end{align}
Here, the vertical and horizontal dots represent contributions from $n>5$ terms. We now summarise the amplitudes of each cosine term in \ref{eqn:cosine1} to  \ref{eqn:cosine5} as $p_j$:
\begin{align}
\text{\ref{eqn:cosine1} to  \ref{eqn:cosine5}} & =\sum_{j=1}^{\infty} p_j \cos(j\omega_0 v) \label{eqn:beta_series1} \\
p_j & \equiv \sigma^2 \exp (- \frac{1}{l^2}) \sum_{\beta = 0}^{\beta = \beta_{j,n}^{MAX}} \frac{2}{(2l^2)^{(j + 2\beta)}} \frac{1}{(j + 2\beta)!} \binom{j + 2\beta}{\beta} \label{eqn:beta_series2} \\
\beta &\equiv  0,1,..., \beta_{j,n}^{MAX} 
\end{align}
The truncation of the sum $\beta_{j,n}^{MAX}$ depends on $j$ and on the truncation, if any, of original index $n = 0,1,2....$. Hence, an analytic value for $\beta_{j,n}^{MAX}$ requires us to explicitly truncate $n$ at $N$, and we defer this for now. We summarise \ref{eqn:eventerms} as:
\begin{align}
\text{\ref{eqn:eventerms}} &= \sigma^2 \exp (- \frac{1}{l^2}) \sum_{\alpha = 0}^{\alpha = \alpha_{n}^{MAX}} \frac{1}{(2l^2)^{(2\alpha)}} \frac{1}{(2\alpha)!} \binom{2\alpha}{\alpha} \label{eqn:alpha_series}\\
\alpha &\equiv  0,1,..., \alpha_{n}^{MAX} 
\end{align}
Note that $\alpha_{n}^{MAX}$ depends only on $n$ (and the truncation of this index), but not $j$. If we set $j=0$ in \ref{eqn:beta_series2}, \ref{eqn:alpha_series} and \ref{eqn:beta_series2} differ only by a factor of $1/2$ and are otherwise identical in form. 
\\
\\
Next, we note that truncation of $n$ at $N$ effectively truncates the index $j$ at $J$. As in \cite{solin2014}, if the index $n$ is truncated at $N$, we observe that $\beta_{j,N}^{MAX} = \lfloor\frac{N-j}{2}\rfloor$ and $\alpha_{N}^{MAX} = \lfloor\frac{N}{2}\rfloor$  where $\lfloor \rfloor$ denotes the ceiling floor. As a concrete example, Table \ref{tab:truncation_n} demonstrates value of $\beta_{j,N}^{MAX}$ for two examples, namely truncation at $N=4$ (even powers) and truncation at $N=5$ (odd powers). 

\begin{table}[h]
	\centering
	\begin{tabular}{rc|c|cc} 
		\hline
		N & j & $\beta_{j,N}^{MAX}$ (from \ref{eqn:cosine1}-\ref{eqn:cosine5}) & $\frac{N-j}{2}$ & $\lfloor\frac{N-j}{2}\rfloor$ \\
		\hline
		N=5 & 1& 2& 2 & 2 \\
			& 2& 1& 1.5 & 1 \\
			& 3& 1& 1 & 1 \\
			& 4& 0& 0.5 & 0 \\
			& 5& 0& 0 & 0 \\
		\hline
		N=4 & 1& 1& 1.5 & 1 \\
			& 2& 1& 1 & 1 \\
			& 3& 0& 0.5 & 0 \\
			& 4& 0& 0 & 0 \\
		\hline
	\end{tabular}
	\caption[Covariance Functions: Values of $\beta_{j,N}^{MAX}$ given $j,N$ ]{Values of $\beta_{j,N}^{MAX}$ given $j,N$}
	\label{tab:truncation_n}
\end{table}
The values for $\alpha_{N=4}^{MAX} = \alpha_{N=5}^{MAX} = 2$ are straightforward to confirm by observing \ref{eqn:eventerms}. Hence it is possible to start the sum in \ref{eqn:beta_series1} at $j=0$ and add (or subtract) half the value of \ref{eqn:alpha_series} depending on whether we want to keep (eliminate) the true value of a $j=0$ DC term. The final result, with $j=0$ term retained, and truncation at $N \equiv J$, is:
\begin{align}
R(v) &= \sigma^2 (p_{0,J} + \sum_{j=0}^{J} p_{j,J} \cos(j\omega_0 v))\\
p_{0,J} & \equiv \frac{1}{2} \exp (- \frac{1}{l^2}) \sum_{\alpha = 0}^{\alpha = \lfloor\frac{J}{2}\rfloor} \frac{1}{(2l^2)^{(2\alpha)}} \frac{1}{(2\alpha)!} \binom{2\alpha}{\alpha} \label{eqn:p0J}\\
p_{j,J} & \equiv \exp (- \frac{1}{l^2}) \sum_{\beta = 0}^{\beta = \lfloor\frac{J-j}{2}\rfloor} \frac{2}{(2l^2)^{(j + 2\beta)}} \frac{1}{(j + 2\beta)!} \binom{j + 2\beta}{\beta} \label{eqn:pjJ}
\end{align}
This derivation agrees with final results reported in \cite{solin2014}. We adjust the covariance function for the constant $j=0$ to recover the same form as \ref{sec:ap_ssp_tp}:
\begin{align}
R'(v) & \equiv R(v) - \sigma^2 p_{0,J} = \sigma^2 \sum_{j=0}^{J} p_{j,J} \cos(j\omega_0 v) 
\end{align}
However, we note that $p_{j,J}$ is not the normalised $p_j$ defined in  \ref{sec:ap_ssp_tp}; instead the coefficients are given by \ref{eqn:p0J}.

\subsubsection{Liska Kalman Filter with Fixed Basis} \label{sec:ap_approxSP:LKFFB}
We recast the modified Liska Kalman Filter in \cite{liska} and confirm that it has a covariance function of the form asserted in Def. \ref{sec:ap_ssp_circle}. 
\\
\\
Unlike the classical Kalman equations, the Liska Kalman filter introduces a process noise features matrix, where matrix elements depend on state estimates from the previous time-step. 
\\
\\
At each time step, $n$, with discrete temporal spacing between measurements $\Delta t$, the state dynamic equation in \cite{liska} for the $j$-th basis osscillator with frequency $\omega_j$ is:
\begin{align}
\begin{bmatrix} x^{j,1}_{n} \\ x^{j,2}_{n} \\ \end{bmatrix} & \equiv \begin{bmatrix} A_{n} \\ B_{n} \\ \end{bmatrix} = \Phi(j \omega_0 \Delta t) \left[\idn + \frac{w_{n-1}}{\sqrt{A_{n-1}^2 + B_{n-1}^2}} \right] \begin{bmatrix} A_{n-1} \\ B_{n-1} \\ \end{bmatrix} \\
\Phi_{LKFFB}(n) &\equiv  \Phi(j \omega_0 \Delta t) \quad \forall n  = \begin{bmatrix} \cos(j \omega_0 \Delta t) & -\sin(j \omega_0 \Delta t) \\ \sin(j \omega_0 \Delta t) & \cos(j \omega_0 \Delta t) \\ \end{bmatrix} \label{eqn:ap_approxSP:LKFFB_Phi} \\
\ex{w_n} &= 0 \\
\ex{w_n,w_m} &= \sigma^2 \delta_{n,m} \quad \text{(scalar Gaussian R.V)} \\
\ex{A_0} &=\ex{B_0} = 0 \\
\ex{A_n B_m} &= 0 \\
\ex{A_n A_m} &= \ex{B_n B_m} = \sigma_j^2 \delta_{n,m} \quad \text{given $\omega_j$} \\
\ex{w_n A_m} &= \ex{w_n B_m} \equiv 0 \quad  \forall n, m \\
\implies \ex{x_n}_j &= 0 \quad \text{by means of $A_{n}, B_{n}, w$} \\
\implies \ex{x_n x_m^T}_j & =   \Phi(j \omega_0 \Delta t) \ex{\begin{bmatrix} A_{n-1}A_{m-1} & A_{n-1}B_{m-1}\\ B_{n-1}A_{m-1} & B_{n-1}B_{m-1}\\ \end{bmatrix}} \Phi(j \omega_0 \Delta t)^T \label{eqn:cov_kf_term1}\\
& +   \Phi(j \omega_0 \Delta t) \left[\frac{w_{n-1}}{\sqrt{A_{n-1}^2 + B_{n-1}^2}} + \frac{w_{m-1}}{\sqrt{A_{m-1}^2 + B_{m-1}^2}} \right]\begin{bmatrix} A_{n-1}A_{m-1} & A_{n-1}B_{m-1}\\ B_{n-1}A_{m-1} & B_{n-1}B_{m-1}\\ \end{bmatrix} \Phi(j \omega_0 \Delta t)^T  \label{eqn:cov_kf_term2}\\
& +   \Phi(j \omega_0 \Delta t) \left[\frac{w_{n-1}w_{m-1}}{\sqrt{A_{n-1}^2 + B_{n-1}^2}\sqrt{A_{m-1}^2 + B_{m-1}^2}} \right]\begin{bmatrix} A_{n-1}A_{m-1} & A_{n-1}B_{m-1}\\ B_{n-1}A_{m-1} & B_{n-1}B_{m-1}\\ \end{bmatrix} \Phi(j \omega_0 \Delta t)^T \label{eqn:cov_kf_term3} \\
& = \sigma^2_j \delta_{n,m} \begin{bmatrix} 
1 & 0 \\ 
0 & 1  \\
\end{bmatrix} \label{eqn:cov_kf_term4}
\end{align}
The terms \ref{eqn:cov_kf_term2} and \ref{eqn:cov_kf_term3} disappear under the temporal correlation functions so defined. (Proofs are deferred to Appendix \ref{sec:ap_prediction} - intuitively, if we assume $n \geq m$, then states at $m-1$ at most have a $w_{n-2}$ term (for the case $n=m$) and cannot be correlated with a future noise term $w_{n-1}$.) 
\\
\\
This is a special case where $v=0$ for the simple stochastic process on a circle considered in \ref{eqn:cov_circle}. By using block diagonal matrices for $\Phi$, we can stack resonators corresponding to a set of $\{\omega_j\}$. While the full covariance function of  Definition \ref{sec:ap_ssp_circle} has a cosine dependence on discrete time lags, $v$ (implicitly, with $\omega \equiv 1$), the Liska Kalman filter approximates this by providing a basis of osscilators corressponding to different time lags ($\omega_j \Delta t$).
\\
\\

