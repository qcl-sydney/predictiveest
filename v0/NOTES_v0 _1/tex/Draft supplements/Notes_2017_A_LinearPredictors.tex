\section{Linear Predictors for Covariance Stationary Processes} \label{sec:ap_prediction}.
We apply generalised linear prediction theory for covariance stationary processes. 
\begin{thm} \label{thm:prediction} Let $f$ be a random variable having a finite second moment and let \textbf{H} be the space of allowable 'predictors', $\hat{f}$ i.e. another set of random variables with finite second moments.  Assume \textbf{H} is a linear space. Then:
\begin{itemize}
\item[] An optimal predictor $\hat{f}^*$ is minimum mean squared error$ \quad \Leftrightarrow E[(f - \hat{f}^*)\hat{U}] = 0 \quad \forall \hat{U} \in \textbf{H} $.
\item[] $\hat{f}^*$ is unique in the mean square sense
\item[] If \textbf{H} is closed in the mean square sense, then $\hat{f}*$ exists
\end{itemize} \label{predictiontheorem}
\end{thm}

Let $f$ be a true, unobserved state, $y$ be a measurement on $f$ and suppose we pick a predictor $\hat{f}$ such that:
\begin{align}
\hat{U} &= a + b(y - \mu_y), a,b \in \Re \\
\hat{f}^* &= a^* + b^*(y - \mu_y) \\
\text{Solve for $a^*, b^*$ since } E[(f - \hat{f}^*)\hat{U}] &= 0 \quad \forall a,b\\
&= \begin{cases} E[f - a^* - b^*(y - \mu_y)]   \text{ for $a=1,b=0$} \\
E[(f - a^*)(y - \mu_y)] - b^*E[(y - \mu_y)(y - \mu_y)] \text{ for $a=0,b=1$} \end{cases}\\
\implies a^* &= \mu_f, \text{ and } b^* = \frac{\sigma_{f,y}}{\sigma^2_y} \\
\hat{f}^* &= \mu_f + \frac{E[(f - a^*)(y - \mu_y)]}{E[(y - \mu_y)(y - \mu_y)]} (y-\mu_y)
\end{align}

Theorum \ref{predictiontheorem} ensures $\hat{f}^*$ is both unique and optimal. The Gaussian Process predictive equations have a state update that can be recast into the form of $\hat{f}^*$ above. The Kalman gain is equivalent to $b*$ for zero mean processes. There is an additional Kalman term which arises from including a linear combination of all previous state estimates - details are deferred to standard textbooks, or the derivation of the Liska Kalman Filter in this Appendix.

\subsection{Least Squares and Autoregressive Kalman Filter}

LS Filtering learns a set of weights, ${\phi_i}$ to predict the $n$-th step head outcome. Prediction outcomes from $n$ sets of learned weights are collated to form a forecast for all steps ahead from $n= 1, 2,...$. Details are found in [CITE: Sandeep and Virginia's paper.]
\\
\\
In autoregressive Kalman Filtering, we defer to the standard classical Kalman filter equations, for which predictions are achieved by setting the Kalman gain equal to zero and recusively propagating the algorithm forwards. Details can be found in any standard textbook, or via the derivation of the Liska Kalman Filter in this Appendix with $x$ redefined as the state vector in Appendix  \ref{sec:ap_approxSP}, with replacements $\Phi \to \Phi_{AKF}, Q \to Q_{AKF}, \Gamma \equiv \idn$. 

\subsection{Gaussian Process Prediction with Arbitrary Choice of Kernel} \label{sec:ap_prediction:GPR}
Let $f,y$ be random variables with finite second moments and a known joint distribution. We want to predict $f$ based on measurements on $y$, and have no further information.
We assume the joint distribution is known:
\begin{align}
P(f,y) = P(f|y)P(y) = P(y|f)P(f) \label{eqn:GPR_productrule}
\end{align}
In machine learning, $P(f|y)$ can be estimated numerically by rearranging above as Bayes Rule. However, for the purposes of our textbook analysis, we consider only the means ($\mu_f, \mu_y$) and variances ($\Sigma^2_f, \Sigma^2_y$) of the marginal distributions $P_f,P_y$, and the covariance function given by $\Sigma^2_{f,y} = E[(f-\mu_f)(y-\mu_y)]$.  We impose a linear form to the connection between $f$ and $y$, namely we suggest there is a measurement noise process, $v \sim P_{v}(\mu_{v}, R)$, as per Kalman notation, such that:
\begin{align}
y_n &= f_n + v_n; \label{eqn:GPR_msmteqn} \\
f &\sim P_f(\mu_f, \Sigma^2_f); \\
v &\sim P_{v}(\mu_{v}, R) \\
y & \sim P_y(\mu_y, \Sigma^2_y) \\
\Sigma^2_{f,y}& = E[(f-\mu_f)(y-\mu_y)]
\end{align}
To set up a link with Kalman Filtering, $f \leftrightarrow Hx$. We assume above that the measurement errors are uncorrelated and independent i.e. set $R_n \equiv R \quad \forall n$. If we assume $P_{f}(n), P_{v}(n)$ are Gaussian:
\begin{align}
\mu_y &= \mu_f + \mu_{v}\\
\Sigma_y^2 &= \Sigma_f^2 + R , \quad \Sigma_f^2 \equiv K(N,N) 
\end{align}
$K(N,N)$ is matrix whose elements are defined by the covariance function $R(v), v=|n-m|$ in Appendix \ref{sec:ap_covariancefunctions}.
\\
\\
The indices $n,m \in N$ denote training points, and $n*,m* \in N*$ denote testing points in machine learning language. We now define the joint distribution $P_{y,f*}$, where $f*$ is our prediction for the true process at test points:
\begin{align}
\begin{bmatrix} f* \\y \end{bmatrix} & \sim \mathcal{N} (\begin{bmatrix} \mu_{f*} \\ \mu_y
\end{bmatrix} , \begin{bmatrix}   K(N*,N*)&K(N,N*) \\ K(N*,N) & K(N,N) + R \end{bmatrix} )
\end{align}
There are two ways to get to the final form of the Gaussian Process predictors. Reading out from standard Gaussian identities (see Appendix of \cite{rasmussen2006gaussian}), the conditional distribution for $f*|y*$ is:
\begin{align}
\ex{f*|y} &= \mu_f + K(N*,N)(K(N,N) + R )^{-1} (y - \mu_y) \\
\text{cov}[f*|y] &= K(N*,N*) - K(N*,N)(K(N,N) + R)^{-1}K(N,N*) 
\end{align}
Alternatively, we invoke Theorem \ref{thm:prediction} and identify:
\begin{align}
\ex{(f - \mu_f)(y - \mu_y)} &\equiv K(N*,N) \\
\ex{(y - \mu_y)(y - \mu_y)} &\equiv K(N,N) + R 
\end{align}
$K(N,N)$ can incorporate any choice of covariance function $R(v)$(e.g. Periodic, RQ, SM Kernels) to specify $K(N,N)$.

\subsection{Liska Kalman Predictor with Fixed Basis} \label{sec:ap_liska_deriv}

The Liska Kalman Filter has adaptive noise matrix, $Q$. We ensure that the standard classical  Kalman Filter derivations are unchanged by the presence of the proposed $Q$ in \cite{liska}.
\\
\\
We consider zero mean Gaussian process and measurement noise $w,v$ (respectively); a `known' deterministic dynamical model $\Phi$ for the unobserved true state $x$, where $x$ is measured through the action of known deterministic $H$ to yield noisy measurements $y$. The Kalman system equations at discrete time steps, indexed by $n$, are:
\begin{align}
x_n &\equiv \Phi_{n-1}x_{n-1} + \Gamma_{n-1}w_{n-1} \label{eqn:KF_statemodel}\\
\Gamma_{n-1} &\equiv \Phi_{n-1}\frac{x_{n-1}}{\norm{x_{n-1}}} \\
y_n &\equiv H_n x_n + v_n \label{eqn:KF_msmtmodel}
\end{align}
To be specific, here, we are actually considering a single state space model associated with a frequency $\omega_j$. The full Kalman model will 'stack' many resonators to form a basis of oscillators on which our data will be projected. Further, this Kalman model can also track the instantaneous phase information, namely, the real and imaginary parts of a Hilbert transformed signal with adaptive noise features, as derived in \cite{liska}. The summary definitions are:
\begin{align}
s_n & \equiv \text{true signal component strength for $\omega_j$} \\
\mathcal{H}[s_n] & \equiv \text{imaginary part of Hilbert transform of $s_n$ for $\omega_j$} \\
x_n &\equiv \begin{bmatrix} s_n \\ \mathcal{H}[s_n] \end{bmatrix} \\
\norm{x_n}&\equiv \sqrt{x_{n}[0]^2 + x_{n}[1]^2} \text{ , [i] denoting $i^{th}$ component of $x_n$}
\end{align}
Without loss of generality, we ignore the Hilbert transform layer of the model ( and is deferred to Appendix [placeholder]). Instead, we proceed onwards by taking a two component state $x$ to represent our true Kalman state. 
\\
\\
The initial condition, $x_0$, is assumed known. We standardise notation such that $Hx \equiv f$ satisfies requirements of Appendices \ref{sec:ap_setup} - \ref{sec:ap_covariancefunctions}. The equations above differ from an ordinary Kalman Filter due to elements of $\Gamma$ depending on the state $x$ itself. The form of $\Phi, H$ is arbitrary and do not require a definition yet.
\\
\\
With $n,m$ denoting time indices, the properties of noise processes are:
\begin{align}
\ex{w_n} & = \ex{v_n}= 0 \quad \forall n \label{eqn:KF_stat_noisemean}\\
\ex{w_nw_m^T} &= \sigma^2 \delta(m-n),  \quad \forall n,m \label{eqn:KF_stat_noisepros}\\
\ex{v_n v_m^T}&= R\delta(m-n), \quad  \forall n,m  \label{eqn:KF_stat_noisemsmt} \\
\ex{w_n v_m} &= 0,  \quad \forall n,m \label{eqn:KF_stat_crosscorr}
\end{align} 
The process noise variance, $\sigma^2$ is a known scalar. The measurement noise variance $R$ is a known covariance matrix in general, but will turn out to be a scalar for our choice of measurement action $H$. This means we can interpret \ref{eqn:KF_stat_crosscorr} as product between scalars. Both $\sigma^2, R$ are tunable design parameters (so called hyper-parameters in machine learning language). Tuning these parameters through an additional optimisation routine is detailed in Appendix [placeholder]. 
\\
\\
At time step $n$, we search for a linear predictor $\hat{x}_n(+)$, with properties given by Theorem \ref{thm:prediction}. In particular, we must find unknowns $\lambda_n, \gamma_n$ below:
\begin{align}
\hat{U} & \equiv \{y_1, y_2,\dots, y_n\} \\
\hat{x}_n(+) & \equiv \lambda_n \hat{x}_n(-) + \gamma_n y_n \label{eqn:KF_predictor}\\
\hat{x}_n(-) & \equiv \Phi_{n-1} \hat{x}_{n-1}(+) \\
(+) &\equiv \text{Aposteriori state estimate  (state post prediction update) for time step $n$}\\
(-) &\equiv \text{Apriori state estimate (state priori to prediction update) for time step $n$}
\end{align}
To find unknowns $\lambda_n, \gamma_n$, we use Theorem \ref{thm:prediction} to impose orthogonality of estimator to the set of all known data:
\begin{align}
\ex{(x_n - \hat{x}_n(+))U^T} &= 0 \label{eqn:KF_reln_1}\\
\implies \ex{(x_n - \hat{x}_n(+))y_i^T} &= 0, \quad i = 1,\dots, n-1 \label{eqn:KF_reln_2}\\
\implies \ex{(x_n - \hat{x}_n(+))y_n^T} &= 0 \label{eqn:KF_reln_3}
\end{align}
We assert that the following correlation relations are true:
\begin{align}
\ex{x_{n-1}w_{n-1}} & = 0 \label{eqn:KF_reln_4}\\
\ex{w_{n-1} y_i} &= 0, \quad i = 1,2,\dots n-1 \label{eqn:KF_reln_5}\\
\ex{\frac{x_{n-1}}{\norm{x_{n-1}}}w_{n-1}} & = 0 \label{eqn:KF_reln_6}\\
\ex{x_{n-1} w_{n-1} x_{n-1}^T } & = 0 \label{eqn:KF_reln_7}\\
\ex{\frac{x_{n-1}}{\norm{x_{n-1}}} w_{n-1} x_{n-1}^T } & = 0 \label{eqn:KF_reln_8} \\
\ex{x_{n-1} w_{n-1} y_i } & = 0, \quad i = 1,\dots, n-1 \label{eqn:KF_reln_9}\\
\ex{\frac{x_{n-1}}{\norm{x_{n-1}}} w_{n-1} y_i } & = 0, \quad i = 1,\dots, n-1 \label{eqn:KF_reln_10}
\end{align}
We interpret these relations by observing that $x_{n-1}$ or $y_i$ do not contain a process noise term of the form $w_{n-1}$. At most, the expansion of $x_{n-1}, y_{n-1}$ terms contain $w_{n-2}$. Where terms of the form $w_{n-k}\dots w_{n-2}w_{n-1}$ appear, we invoke \ref{eqn:KF_stat_noisepros} and set these terms to zero. Where terms of the form $\propto w_{n-2}w_{n-2}w_{n-1} = \sigma^2 w_{n-1}$ appear, we invoke zero mean noise and set these terms to zero. The $\norm{x_{n-1}}$ term depends on $x_{n-1}$, and we assert that a similar logic holds where physically, a normed state cannot be correlated with a future process noise term.
\\
\\
The above relations imply that:
\begin{align}
\ex{x_n} &= \Phi_{n-1}\ex{x_{n-1}} \label{eqn:KF_reln_11}\\
\ex{(x_n - \apx{n})x_0^T} &= 0 \label{eqn:KF_reln_12}\\
\ex{(x_n - \apx{n})\hat{y}_n(-)^T} &=0 \label{eqn:KF_reln_13} \\
\ex{(x_n - \amx{n})y_i^T} &=0, \quad i = 1,\dots, n-1\label{eqn:KF_reln_14}\\
\ex{\Gamma_n w_n \apx{n}^T} &=0 \label{eqn:KF_reln_16}\\
\ex{\Gamma_n w_n x_n^T} &=0 \label{eqn:KF_reln_17} 
\end{align}
We  find $\lambda_n$ using orthogonality conditions at different time steps:
\begin{align}
\ex{(x_n - \apx{n})y_i^T} &=0, \quad i = 1,\dots, n-1\label{eqn:KF_reln_15}\\ 
\implies 0 &= \ex{(x_n - \apx{n})y_i^T} \\
& = \ex{(\Phi_{n-1}x_{n-1}(\idn + \frac{w_{n-1}}{\norm{x_{n-1}}}) - \lambda_n \hat{x}_n(-) - \gamma_n y_n)y_i^T} \\
&= \Phi_{n-1}\ex{x_{n-1}y_i^T} + \Phi_{n-1}\ex{\frac{x_{n-1}w_{n-1}}{\norm{x_{n-1}}}y_i^T} - \lambda_n \ex{ \hat{x}_n(-)y_i^T} - \gamma_n H_n \ex{ x_n y_i^T} - \gamma_n \ex{ v_n y_i^T} \\
&= \Phi_{n-1}\ex{x_{n-1}y_i^T} + \Phi_{n-1}\ex{\frac{x_{n-1}w_{n-1}}{\norm{x_{n-1}}}y_i^T} - \lambda_n \ex{ \hat{x}_n(-)y_i^T} - \gamma_n H_n\ex{ x_n y_i^T}, \quad \text{by \ref{eqn:KF_stat_noisemsmt} } \\
&= \ex{\Phi_{n-1}x_{n-1}y_i^T} - \lambda_n \ex{ \hat{x}_n(-)y_i^T} - \gamma_n H_n\ex{ x_n y_i^T}, \quad \text{by \ref{eqn:KF_reln_10} } \\
&=\ex{x_{n}y_i^T} - \lambda_n \ex{ \hat{x}_n(-)y_i^T} - \gamma_n H_n\ex{ x_n y_i^T}, \quad \text{by \ref{eqn:KF_reln_11}} \\
&= \ex{x_{n}y_i^T} + \lambda_n \ex{( x_n - \hat{x}_n(-))y_i^T} - \lambda_n \ex{ x_n y_i^T} - \gamma_n H_n\ex{ x_n y_i^T}, \quad \text{add $\pm \lambda_n \ex{ x_n y_i^T}$} \\
&= \ex{x_{n}y_i^T} - \lambda_n \ex{ x_n y_i^T} - \gamma_n H_n\ex{ x_n y_i^T}, \quad \text{by \ref{eqn:KF_reln_2}} \\
0 &= \ex{(\idn - \lambda_n  - \gamma_n H_n)x_n y_i^T} \\
\ex{x_n y_i^T} \neq 0 \\
\implies \lambda_n &= \idn  - \gamma_n H_n
\end{align}
We find $\gamma_n$ using orthogonality conditions with all terms at the same time step:
\begin{align}
e_n & \equiv \amx{n} - x_n,  \\
\amp{n} & \equiv E[e_ne_n^T] \\
R_n & \equiv E[v_nv_n^T]\\
E[v_n e_n^T]&=0  \\
\ex{(x_n - \apx{n})(\hat{y}_n(-) - y_n)^T} &=0 \quad \text{by \ref{eqn:KF_reln_3}}, \ref{eqn:KF_reln_13} \\
\implies 0 &= E\left[(x -\lambda_n \amx{n} - \gamma_n H_n x_n - \gamma_n v_n)(H_n (\amx{n} - x_{k}) -v_n)^T\right] \\
&= E\left[(x_n - \amx{n} + \gamma_n H_n\amx{n} - \gamma_n H_n x_n - \gamma_n v_n)(H_n (\amx{n} - x_{k}) -v_n)^T\right]  \\
&= E\left[(x_n - \amx{n} + \gamma_n H_n\amx{n} - \gamma_n H_n x_n - \gamma_n v_n)(H_n (\amx{n} - x_{k}) -v_n)^T\right].  \\
&= E\left[(-e_n + \gamma_n H_ne_n - \gamma_n v_n)(H_n e_n -v_n)^T\right]  \\
&= E\left[(-e_ne_n^T H_n^T+ \gamma_n H_ne_ne_n^T H_n^T - \gamma_n v_ne_n^T H_n^T + e_nv_n^T - \gamma_n H_ne_nv_n^T + \gamma_n v_nv_n^T \right]  \\
&= -\amp{n} H_n^T+ \gamma_n H_n\amp{n}H_n^T + \gamma_n R_n \\
\implies \gamma_n &= \amp{n} H_n^T(H_n\amp{n}H_n^T + R_n)^{-1} \quad \text{(Kalman Gain)} \label{eqn:KF_update_gamma}
\end{align}
Hence, the form of optimal predictor $\apx{n}$ in the same time step $n$ is:
\begin{align}
\apx{n} &= (1 - \gamma_nH_n) \amx{n} + \gamma_ny_n \\
& =  \amx{n} + \gamma_n(y_n - H_n\amx{n}) \\
&= \amx{n} + \gamma_n (y_n - \hat{y}_n(-)) \label{eqn:KF_update_x+} 
\end{align}
We also define optimal predictor for the state variance:
\begin{align}
\text{Using $2^{nd}$ line, }\apx{n} &=  \amx{n} - \gamma_nH_n \amx{n} + \gamma_nH_n x_n + \gamma_nv_n  \nonumber \\
e^{+}_n &\equiv \apx{n} - x_n \\
&=  \amx{n} - \gamma_nH_n \amx{n} + \gamma_nH_n x_n + \gamma_nv_n - x_n \nonumber \\
&=  e_n - \gamma_nH_n e_n + \gamma_nv_n = \left[1 - \gamma_nH_n \right] e_n + \gamma_nv_n \nonumber \\
\app{n} &\equiv \ex{e_n^+ e_n^{+T}} \\
& = E \left[ \left[1 - \gamma_nH_n \right] e_n e_n^T \left[1 - \gamma_nH_n \right]^T + \gamma_nv_n e_n^T \left[1 - \gamma_nH_n \right]^T   +  \left[1 - \gamma_nH_n \right] e_n v_n^T \gamma^T + \gamma_nv_n v_n^T \gamma^T \right] \\
& = \left[1 - \gamma_n H_n \right] \amp{n}\left[1 - \gamma_n H_n \right]^T + \gamma_n R_n \gamma_n^T  \\
& =  \amp{n} - \amp{n}H_n^T\gamma_n^T - \gamma_n H_n \amp{n} + \gamma_n \left[ H_n \amp{n}H_n^T +  R_n \right] \gamma_n^T, \quad \text{using $\gamma_n$}\\
&= \amp{n} - \amp{n}H_n^T\gamma_n^T - \gamma_n H_n \amp{n} +  \amp{n}H_n^T \gamma_n^T  \\ 
&= \left[1  - \gamma_n H_n \right] \amp{n} \label{eqn:KF_update_p+}
\end{align}
This optimal variance can be propagated from one time step to the next as:
\begin{align}
e_n &= \amx{n} - x_n \\
&= \Phi_{n-1}\left[\apx{n-1} - x_{n-1} \right] - \Gamma_{n-1}w_{n-1} \\
&= \Phi_{n-1}e^{+}_{n-1} - \Gamma_{n-1}w_{n-1} \\
\amp{n} &= \Phi_{n-1}E\left[e^{+}_{n-1}e^{+T}_{n-1}\right]\Phi_{n-1}^T  \\
& - \Phi_{n-1}E\left[e^{+}_{n-1}w_{n-1}^T\Gamma_{n-1}^T\right] - E\left[\Gamma_{n-1}w_{n-1}e^{+T}_{n-1}\right]\Phi_{n-1}^T  \\
& + E\left[\Gamma_{n-1}w_{n-1}w_{n-1}^T\Gamma_{n-1}^T\right] \\
&= \Phi_{n-1}E\left[\app{n-1}\right]\Phi_{n-1}^T  + E\left[\Gamma_{n-1}w_{n-1}w_{n-1}^T\Gamma_{n-1}^T\right] \\
& - \Phi_{n-1}E\left[(\apx{n-1} - x_{n-1})w_{n-1}^T\Gamma_{n-1}^T\right] - E\left[\Gamma_{n-1}w_{n-1}(\apx{n-1} - x_{n-1})^T\right]\Phi_{n-1}^T  \\
&= \Phi_{n-1} \app{n-1} \Phi_{n-1}^T + E\left[\Gamma_{n-1}w_{n-1}w_{n-1}^T\Gamma_{n-1}^T\right], \quad \text{$2^{nd}$ line zero by \ref{eqn:KF_reln_16}, \ref{eqn:KF_reln_17}} \\
 &= \Phi_{n-1} \app{n-1} \Phi_{n-1}^T + Q_{n-1}, \quad Q_{n} \equiv E\left[\Gamma_{n}w_{n}w_{n}^T\Gamma_{n}^T\right] \label{eqn:KF_update_p-}
\end{align}
Hence, the standard Kalman predictor equations - \ref{eqn:KF_update_gamma}, \ref{eqn:KF_update_x+}, \ref{eqn:KF_update_p+}, \ref{eqn:KF_update_p-} - are valid for adaptive noise features given in \cite{liska}. They are summarised below in order of computation. 
\begin{align}
\amx{n} & = \Phi_{n-1} \apx{n-1} \\
\Gamma_{n-1} &=\Phi_{n-1}\frac{\apx{n-1}}{\norm{\apx{n-1}}} \\
Q_{n-1} & = \sigma^2 \Gamma_{n-1}\Gamma_{n-1}^T \\
\amp{n}&= \Phi_{n-1} \app{n-1} \Phi_{n-1}^T + Q_{n-1} \\
\gamma_n &= \amp{n} H_n^T(H_n\amp{n}H_n^T + R_n)^{-1} \\
\hat{y}_n(-) & = H_n \amx{n} \\
\apx{n} &= \amx{n} + \gamma_n (y_n - \hat{y}_n(-)) \\
\app{n} &= \left[1  - \gamma_n H_n \right] \amp{n}
\end{align}
Unlike a typical Kalman filter, state estimation cannot be decoupled from state variance estimation due to $\Gamma$. This means Kalman gains cannot be calculated in advance of data collection.

\newpage
\subsection{Proofs \ref{eqn:KF_reln_4} - \ref{eqn:KF_reln_17}:}
\\
\begin{align}
\ex{x_{n-1} w_{n-1}} & = \ex{\Phi_{n-2}x_{n-2}(\idn + \frac{w_{n-2}}{\norm{x_{n-2}}}) w_{n-1}} \\
& = \ex{\Phi_{n-2}x_{n-2}w_{n-1}} +\ex{ \frac{\Phi_{n-2}x_{n-2} w_{n-2}}{\norm{x_{n-2}}} w_{n-1}} \\
& = \ex{\Phi_{n-2}x_{n-2}w_{n-1}} \\
& = \ex{\Phi_{n-2}\Phi_{n-3}x_{n-3}(\idn + \frac{w_{n-3}}{\norm{x_{n-3}}})w_{n-1}} \\
& \vdots \\
& = \Phi_{n-2}\dots\Phi_{n-i+1}x_0\ex{w_{n-1}} \\
&= 0 \\
\nonumber \\ 
\ex{w_{n-1} y_i} &= \ex{w_{n-1}  H_{n-1} x_{n-1} + w_{n-1}v_{n-1}}  \\
 &= \ex{w_{n-1}  H_{n-1} x_{n-1}}, \quad\text{by \ref{eqn:KF_stat_crosscorr} } \\ 
 &= H_{n-1} \ex{w_{n-1} x_{n-1}} \\
 &= 0, \quad\text{by \ref{eqn:KF_reln_4}} \\
\nonumber \\ 
\ex{\frac{x_{n-1}}{\norm{x_{n-1}}}w_{n-1}} & = \ex{\frac{\Phi_{n-2}x_{n-2}(\idn + \frac{w_{n-2}}{\norm{x_{n-2}}})}{\norm{x_{n-1}}}w_{n-1}} \\
& = \ex{\frac{\Phi_{n-2}x_{n-2}w_{n-1} + \frac{w_{n-2}w_{n-1}}{\norm{x_{n-2}}}}{\norm{x_{n-1}}}} \\
& = \ex{\frac{\Phi_{n-2}x_{n-2}w_{n-1}}{\norm{x_{n-1}}} + \frac{w_{n-2}w_{n-1}}{\norm{x_{n-2}}\norm{x_{n-1}}}} \\
&\vdots \\
&= 0 \quad \text{since $x_{n-1, n-2, \dots}$, $\norm{x_{n-1,n-2, \dots}}$ cannot be correlated with future  noise, $w_{n-1}$.}\\
\nonumber \\ 
\ex{x_{n-1} w_{n-1} x_{n-1}^T} &= \ex{\Phi_{n-2}x_{n-2}(\idn + \frac{w_{n-2}}{\norm{x_{n-2}}}) w_{n-1} \Phi_{n-2}x_{n-2}(\idn + \frac{w_{n-2}}{\norm{x_{n-2}}})} \\
&= \ex{\Phi_{n-2}x_{n-2} w_{n-1} x_{n-2}^T\Phi_{n-2}^T(\idn + \frac{w_{n-2}}{\norm{x_{n-2}}})^2} \\
&= 0 \quad \text{since $x_{n-1, n-2, \dots}$, $\norm{x_{n-1,n-2, \dots}}$ cannot be correlated with future  noise, $w_{n-1}$.}
\end{align}
\ref{eqn:KF_reln_8} is justified identically to \ref{eqn:KF_reln_7}. Recognising that $y_i = H_i x_i + v_i$ and that $\ex{w_jv_i} = 0 \forall i,j$, we can justify \ref{eqn:KF_reln_9}, \ref{eqn:KF_reln_10} with the same reasoning as \ref{eqn:KF_reln_7}.
\begin{align}
\ex{x_n} &= \ex{\Phi_{n-1}x_{n-1}(\idn + \frac{w_{n-1}}{\norm{x_{n-1}}})} \\
&= \Phi_{n-1}\ex{x_{n-1}} + \Phi_{n-1}\ex{x_{n-1}\frac{w_{n-1}}{\norm{x_{n-1}}})} \\
&= \Phi_{n-1}\ex{x_{n-1}}, \quad \text{by \ref{eqn:KF_reln_6}} \\
\nonumber \\ 
\ex{(x_n - \apx{n})x_0} &= \ex{(x_n - \apx{n})y_1} \\
&= 0 \quad \text{by \ref{eqn:KF_reln_2}, for $i=1$.}
\end{align}
\begin{align}
\text{To prove } \ex{(x_n - \apx{n})\hat{y}_n(-)^T} &=0, \quad \text{note:} \\
\amx{n} & \equiv \Phi_{n-1} \apx{n-1} \label{eqn:amx}\\
\hat{y}_n(-) & \equiv H_n \amx{n} \\
&= H_n \Phi_{n-1} \apx{n-1} \\
&= H_n \Phi_{n-1} \lambda_{n-1} \amx{n-1} + H_n \Phi_{n-1} \gamma_{n-1} y_{n-1}, \quad \text{by \ref{eqn:KF_predictor}} \\
&= H_n \Phi_{n-1} \lambda_{n-1} \Phi_{n-2} \apx{n-2} + H_n \Phi_{n-1} \gamma_{n-1} y_{n-1} \\
&\vdots \nonumber \\
& = \beta_0 x_0 + \sum_{k=1}^{n-1} \beta_k y_k, \quad \text{$\beta_k$ is a deterministic coefficent $H, \Phi, \gamma, \lambda$ over $\{n\}$.}\\
\implies \ex{(x_n - \apx{n})\hat{y}_n(-)} &= \ex{(x_n - \apx{n})(x_0^T\beta_0^T  + \sum_{k=1}^{n-1} y_k^T \beta_k^T } \\
&= \ex{(x_n - \apx{n})x_0^T}\beta_0^T  + \sum_{k=1}^{n-1} \ex{(x_n - \apx{n})y_k^T} \beta_k^T  \\
&= \sum_{k=1}^{n-1} \ex{(x_n - \apx{n})y_k^T} \beta_k^T, \quad \text{by \ref{eqn:KF_reln_12}}\\
&=0, \quad \text{by \ref{eqn:KF_reln_2}} \\
\nonumber \\ 
\ex{(x_n - \amx{n})y_i^T} &= \ex{(x_n - \Phi_{n-1} \apx{n-1})y_i^T}, \quad i = 1,\dots,n-1 \\
&=0, \quad \text{by \ref{eqn:KF_reln_2}} \\
\nonumber \\
\ex{\Gamma_n w_n \apx{n}^T} &=\ex{\Gamma_n w_n  \hat{x}_n(-)^T \lambda_n^T} +  \ex{\Gamma_n w_n  y_n^T \gamma_n^T} \quad \text{by \ref{eqn:KF_predictor}}\\
&=\ex{\frac{\Phi_{n}x_{n}}{\norm{x_{n}}} w_n  \hat{x}_n(-)^T \lambda_n^T} +  \ex{\frac{\Phi_{n}x_{n}}{\norm{x_{n}}} w_n  y_n^T \gamma_n^T} \\
&=\ex{\frac{\Phi_{n}x_{n}}{\norm{x_{n}}} w_n  \hat{x}_n(-)^T \lambda_n^T} \quad \text{by \ref{eqn:KF_reln_10}} \\
&=\Phi_{n}\ex{\frac{x_{n}}{\norm{x_{n}}} w_n \apx{n-1}^T} \Phi_{n-1}^T \lambda_n^T \quad \text{by \ref{eqn:amx}} \\
&\vdots \quad \text{repeatedly apply \ref{eqn:KF_predictor}, \ref{eqn:KF_reln_10}, \ref{eqn:amx}} \\
&=0, \quad \text{past terms uncorrelated with future $w_n$} \\
\nonumber \\ 
\ex{\Gamma_n w_n x_n^T} &= \Phi_{n}\ex{\frac{x_{n}}{\norm{x_{n}}} w_n x_n^T} =0 \quad \text{by \ref{eqn:KF_reln_8} } 
\end{align}