% ##############################################################################\
\section{Stochastic Qubit Dynamics under Environmental Dephasing \label{sec:app:setup}}
% ##############################################################################

Environmental dephasing manifests as a time dependent stochastic detuning which couples to a $\p{z}$ term in the Hamiltonian, $\op{H}_{N} (t)$, governing qubit dynamics. Consider the state on the equator of the Bloch sphere evolving under $\op{H}_{N} (t)$ during a Ramsey experiment for duration $\tau$. 
\begin{align}
\op{H}_{N}(t) & \equiv -\frac{\hbar}{2}\delta\omega(t)\p{z}
\end{align}
Initially, the qubit state is prepared on the equator of the Bloch sphere. The qubit evolves freely for $\tau$ time and subsequently, the qubit state is rotated before a projective measurement is performed with respect to the $\p{z}$ axis. In the semiclassical approximation, this Hamiltonian commutes with itself at different $t$, and hence we can write the time evolution operator during the Ramsey wait time as:
\begin{align}
\op{U}(t, t + \tau) & = e^{-\frac{i}{\hbar} \state(t, t + \tau)} \\
\state(t, t + \tau) & \equiv \int_{t}^{t + \tau} \op{H}_{N} (t') dt' \label{eqn:app:phases}
\end{align}
We impose a condition that $\state(t, t + \tau) < \pi$ such that accumulated phase over $\tau$ can be inferred from a projective measurement on the $\p{z}$ axis. 
\\
\\
Consider that the initial $\pi/2$ control pulse perfectly delivers the state vector to the equatorial plane of the Bloch sphere, for example, the  $\ket{\p{x} +}$ state and is allowed to evolve under $\op{U}(t, t + \tau)$. Then, the probability of measuring in $\ket{\p{x} \pm}$ yielding a single shot measurement, $d \in \{0, 1\}$, is:
\begin{align} 
|\bra{\p{x} \pm} \op{U}(t, t + \tau) \ket{\p{x} +}|^2= \begin{cases} \cos(\frac{\state(t, \tau)}{2})^2 \quad \text{for $ \ket{\p{x} +}$} \\   \sin(\frac{\state(t, \tau)}{2})^2  \quad \text{for $ \ket{\p{x} -}$} \end{cases} \label{eqn:app:likelihood}
\end{align}
The second $\pi/2$ control pulse rotates the state vector such that a measurement in $\p{z}$ basis is possible, and the probabilities correspond to $\ket{\p{z} \pm}$
respectively. Hence, \cref{eqn:app:likelihood} defines the likelihood for single shot qubit measurement in \cref{eqn:main:likelihood} and defines the non linear measurement action on phase noise jitter, $\state(t, \tau)$.

% ##############################################################################
\section{Experimentally Controlled Discretisation of Dephasing Noise \label{sec:app:exptres}} 
% ##############################################################################
 In this section, we consider a sequence of Ramsey measurements. At time $t$, \cref{sec:app:setup} describes the qubit measurement likelihood at one instant under dephasing noise. We assume that the dephasing noise is slowly drifting with respect to a fast measurement action on timescales of order $\tau$. In this regime, \cref{eqn:app:phases} discretises the continuous time process $\delta\omega(t)$, at time $t$, for a number of $n= 0, 1, ..., N$ equally spaced measurements with $t = n \Delta t$. Performing the integral for $\tau \ll \Delta t$ and slowly drifting noise such that, $\delta\bar{\omega}_n \equiv \delta\omega(t')|_{t'=n \Delta t }$ in \cref{eqn:app:phases}:
\begin{align}
\state(n\Delta t, n\Delta t + \tau) & = -\frac{\hbar}{2}\p{z}\delta\bar{\omega}_n \tau \label{eqn:app:phases_constantdetuning}
\end{align}
In this notation, $\delta\bar{\omega}_n $ is a random variable realised at time, $t = n \Delta t$, and it remains constant over short duration of the measurement action, $\tau$.  We use the shorthand $\state_n \equiv \state(n\Delta t, n\Delta t + \tau)$ to label a sequence of stochastic, temporally correlated qubit phases $ \state \equiv \{\state_n \}$. 
 \\
 \\
 The discretisation of $\delta\omega(t)$ into  a discrete time stochastic phase sequence, $\state$, governs the physical Fourier resolution at which dephasing noise is sampled. If we take a sequence, $f$ to be $N$ samples long,
 then we define the resulting physical Fourier domain resolution over training and prediction is:
 \begin{align}
 N \equiv N_T + N_P \\
 \Delta f_{EXPT} \equiv \frac{1}{\Delta t N} \equiv \frac{f_s}{N}
 \end{align}
By specifying  $\Delta t, N_P, N_T$, an experiment fully defines the Fourier resolution for analysis. In particular, an implicit assumption about the bandwidth of dephasing noise, $f_B$ is captured in $\Delta t$, namely:
\begin{align}
\Delta t \equiv \frac{1}{f_s} \equiv \frac{1}{r_{Nqy}f_B}
 \end{align}
In numerical simulations, we choose $r_{Nqy} \gg 2$ and fix $f_B, \Delta t, N_T, N_P$. For true noise engineering, we compare true noise spacing $\omega_0/ 2\pi$ relative to the physical resolution of the entire experimental run, i.e. $\Delta f_{EXPT}$. Further, if the oversampling regime for any algorithm is varied, it can interpreted as relaxing $f_B$ assumption or equivalently, reducing $r_{Nqy}$, however, we ensure there is no physical aliasing and $r_{Nqy}>2$ in all cases.
\\
\\
The computational resolution in algorithms has a natural interpretation such that  $\omega_0^B / 2\pi \geq \frac{1}{\Delta t N_T} > \Delta f_{EXPT}$. This is necessarily the case since an algorithm ceases to receive measurement data but we continue to `sample' true noise to confirm accuracy of predictions in our study. Numerically, this is significant in optimising spacing between adjacent oscillators for  LKFFB and GPR algorithms and linking their performance to a physical interpretation of sampling rates.  

% ##############################################################################
\section{True Dephasing Noise Engineering \label{sec:app:truenoise}} 
% ##############################################################################

In the absence of an apriori model for describing qubit dynamics under dephasing noise, we impose the following properties on $\state$ such that we can design meaningful predictors of qubit state dynamics. In the first section, we state these global assumptions citing relevant technical terms from \cite{karlin2012first}. Next, we define a computational method of engineering a stochastic phase sequence $\state$ compatible with experimental noise engineering protocols in \cite{soare2014}. Lastly, we show that engineered noise $\state$ satisfies the assumed properties of a covariance stationary and mean square ergodic process (see, for example, \cite{gelb1974applied}).

\subsection{Global Assumptions}
In the absence of an apriori model for describing qubit dynamics under dephasing noise, we impose the following properties on $\state$: 
\begin{azm}\label{azm:truef}
	 A stochastic process $\{f(n_k): n\in N, k \in K \}$ indexed by a set of values $\{n_k\}$ is covariance stationary:
	\begin{align}
	\ex{f(n_k)} &= \mu \quad \text{(constant mean)} \\
	\ex{f(n_k)^2} & < \infty \text{(finite second moments)} \\
	\ex{(f(n_k) - \mu)(f(n_j) - \mu)} &= R(|n_k - n_j|), \forall k,j \in K  \label{azm:corrfunc}
	\end{align}
\end{azm}

\begin{defn} \label{dfn:ms} $\{f(n_k): n\in N, k \in K \} \rightarrow \{g\}$ in mean square (M.S) if:
	\begin{align}
	\ex{f(n_k)^2} & < \infty \text{(finite second moments)} \\
	\lim_{k \rightarrow \infty}\ex{(f(n_k) - g)^2} &= 0
	\end{align}
\end{defn}

\begin{thm} \label{thm:erogodic}
	[Stated without proof.] Let $\bar{f_k} = \frac{1}{K} \sum_k f(n_k)$ be an observed sample mean of the true process $\{f(n_k)\}$ given in Assumption \ref{azm:truef}. The true process $\{f(n_k)\}$ is ergodic in M.S. if:
	\begin{align}
	&\lim_{K \to \infty} \frac{1}{K} \sum_{v=0}^{K-1} R(v) = 0, \quad v = |n_k - n_j|, \forall k,j \in K \\
	\iff &\lim_{K \to \infty} \ex{(\bar{f}_k - \mu)^2} = 0.\quad \text{(sufficiently large $K$)}.
	\end{align}
\end{thm}

\begin{azm} \label{azm:Rbandlimit}
	$\{f(n_k)\}$ is M.S. ergodic. Equivalently, $R(|n_k - n_j|)$ is bandlimited for sufficiently large (but unknown) $K$, where $k,j \in K$. If this were not the case, then no convergence of sample moments to true moments is guaranteed as correlations never `die out'. Hence, our predictors for one realisation will fail for a different realisation of the same true process. 
\end{azm}

\begin{azm}\label{azm:PSDbandlimit}
	The dual of $R(|n_k - n_j|)$ is bandlimited. If this were not the case, then delta-correlations exist in the time domain and no prediction is possible. 
\end{azm}

\begin{azm}\label{azm:fastmsmtaction}
	If the dual of $R(|n_k - n_j|)$ is bandlimited at $B$ by Assumption \ref{azm:PSDbandlimit}, then the (Ramsey) measurement action is sufficiently fast such that the experimental sampling rate $f_s = rf_B, \quad r_{Nyq} >> 2$, where $r_{Nyq} $ is the Nyquist multipler.
\end{azm}

\subsection{Engineered Stochastic Qubit Phases, $\state$}
For the purposes of experimental noise engineering, we satisfy the assumptions above by defining the discretised process, $\state$, as:
\begin{align}
\state_n &= \alpha \omega_0 \sum_{j=1}^{J} j F(j)\cos(\omega_j n \Delta t + \psi_j) \label{eqn:app:noiseengineering} \\
F(j) & = j^{\frac{p}{2}-1} 
\end{align}

Using the notation of \cite{soare2014}, $\alpha$ is an arbitrary scaling factor, $\omega_0$ is the fundamental spacing between true adjacent discrete frequencies, such that $\omega_j = 2 \pi f_0 j =\omega_0 j, j = 1, 2, ...J$. For each frequency component, there exists a uniformly distributed random phase, $\psi_j \in [0, \pi]$. The free parameter $p$ allows one to specify an arbitrary shape of the true power spectral density of $\state$. In particular, the free parameters $\alpha, J, \omega_0, p$ are true dephasing noise parameters which any prediction algorithm cannot know beforehand.

\subsection{Supporting Proofs for \cref{sec:app:truenoise}}

\subsubsection{$\state$ is Covariance Stationary and M.S. Ergodic}
With uniformly distributed phase information, we now affirm that $\state$ is mean square ergodic and covariance stationary \cite{gelb1974applied}.
Consider one term in the harmonic sum \cref{eqn:app:noiseengineering} and drop the $j$ index to ease notation such that $A \equiv \alpha \omega_0 j F(j), \omega \equiv \omega_j, \psi \equiv \psi_j$. Note that $\{ \psi \}$ are randomly uniformly distributed over one cycle, and $x(\psi, n)$ is a function of random variable $\psi$ defining a random process over the time index, $n$. 
\begin{align}
x(\psi, n) & \equiv A \cos(\omega \Delta t n + \psi ) 
\end{align}
An expectation over an ensemble of $x(\psi, n)$ yields the covariance function (and the autocorrelation function for a zero mean process):
\begin{align}
R_{xx}(n_1, n_2) &\equiv \ex{(x(\psi_1, n_1) - \mu )(x(\psi_2, n_2) - \mu )} \\
& = \ex{x(\psi_1, n_1) x(\psi_2, n_2)}, \mu = 0 \\
& = \int  d\psi_1  \int  d\psi_1  \quad x(\psi_1, n_1) x(\psi_2, n_2) g_2(\psi_1, n_1; \psi_2, n_2) \\
g_2(\psi_1, n_1; \psi_2, n_2) & \equiv \frac{\delta^2 Pr(\psi(n_1) \leq \psi_1, \psi(n_2) \leq \psi_2 )}{\delta \psi_1 \delta \psi_2} \\
& = \frac{1}{2\pi}, \psi \in [0, 2\pi] \label{eqn:SS_ensble_prob_density} \\
\Rightarrow R_{xx}(n_1, n_2) & = \int_0^{2\pi} d \psi \frac{1}{2\pi} \quad x(\psi, n_1) x(\psi, n_2) \\
& = \frac{A^2}{4\pi} \int_0^{2\pi} d \psi \quad 2\cos(\omega \Delta t n_1 + \psi) \cos(\omega \Delta t n_2 + \psi) \\
& = \frac{A^2}{4\pi} \int_0^{2\pi} d \psi \quad \cos(\omega \Delta t (n_1 -n_2))  + \cos(\omega \Delta t (n_1 + n_2) + 2\psi) \\
& = \frac{A^2}{2} \cos(\omega \Delta t (n_1 -n_2))  + \frac{A^2}{4 \pi}\int_0^{2\pi} d \psi  \cos(\omega \Delta t (n_1 + n_2) + 2\psi) \\
& = \frac{A^2}{2} \cos(\omega \Delta t v), v = |n_1 -n_2|
\end{align}
A long time average taken over one $x(\psi, n)$ yields:
\begin{align}
R_{xx}(n, n + v) & \equiv \lim_{N \to \infty} \frac{1}{2 \Delta t N} \int_{-\Delta t N}^{\Delta t N} dn \quad  x(\psi, n) x(\psi, n + v)  \\
& = \lim_{N \to \infty} \frac{A^2}{4 \Delta t N} \int_{-\Delta t N}^{\Delta t N} dn \quad 2 \cos(\omega \Delta t n + \psi) \cos(\omega \Delta t n + \omega \Delta t v + \psi) \\
& = \lim_{N \to \infty} \frac{A^2}{4 \Delta t N} [ 2\Delta t N\cos(-\omega \Delta t v)  + \int_{-\Delta t N}^{\Delta t N} dn \quad \cos(2\omega \Delta t n + \omega \Delta t v + 2\psi) ]\\
& =  \frac{A^2}{2} \cos(\omega \Delta t v), v = |n_1 -n_2| \\
\end{align}
Typically, $x(\psi, n)$ so defined is only ergodic for uniformly distributed phases. 
\\
\\
We reintroduce the sum over $j = 1, 2, ... , J$:
\begin{align}
f(n) & = \sum_j^J x(\psi_j, n) \\
\ex{f(n)} &= \sum_j^J \ex{x(\psi_j, n)} = 0 \\
\ex{f(n)f(m)} &=  \ex{\sum_j^J x(\psi_j, n) \sum_{j'}^{J} x(\psi_{j'}, m)}  \\
 & = \ex{\sum_j^J x(\psi_j, n) x(\psi_{j}, m)} + \ex{\sum_{j'}\sum_{j\neq j'}^J x(\psi_j, n) x(\psi_{j'}, m)} \\
  & = \sum_j^J \ex{x(\psi_j, n) x(\psi_{j}, m)} + \sum_{j'}\sum_{j\neq j'}^J \ex{x(\psi_j, n) x(\psi_{j'}, m)}  \text{since $j$ is deterministic.}\\
 & = \sum_j^J \frac{A_j^2}{2} \cos(\omega_j \Delta t v) + \sum_{j'}\sum_{j\neq j'}^J \ex{x(\psi_j, n) x(\psi_{j'}, m)}, v = |n - m |  \label{eqn:SS_fn_crossterm}\\
 & = \sum_j^J \frac{A_j^2}{2} \cos(\omega_j \Delta t v), v = |n - m |, j = j'
\end{align}
 
 I argue that the second term in \ref{eqn:SS_fn_crossterm} is zero because $\psi_j, \psi_{j'}$ are uniformly distributed phases for cycles with different angular frequencies.
 I suggest that the joint probability density function of $ g_2(\psi_j, n; \psi_{j'}, m) = g(\psi_j, n) g(\psi_{j'}, m), j \neq j'$:

\begin{align}
\sum_{j'}\sum_{j\neq j'}^J \ex{x(\psi_j, n) x(\psi_{j'}, m)} & = A_j A_{j'} \int d \psi_j \quad g(\psi_j, n) \cos(\omega_j \Delta t n + \psi_j) \int d \psi_{j'} \quad g(\psi_{j'}, n) \cos(\omega_{j'} \Delta t m + \psi_{j'}) \\
& = A_j A_{j'} \int_0^{2\pi} d \psi_j \quad \frac{1}{2\pi} \cos(\omega_j \Delta t n + \psi_j) \int_0^{2\pi} d \psi_{j'} \quad \frac{1}{2\pi}  \cos(\omega_{j'} \Delta t m + \psi_{j'}) \\
& = 0 
\end{align}
 \\
 \\
  A long term time average yields:
\begin{align}
\ex{f(n)f(n + v)} & \equiv \lim_{N \to \infty} \frac{1}{2 \Delta t N} \int_{-\Delta t N}^{\Delta t N} dn \quad  \sum_j^J x(\psi_j, n) \sum_{j'}^{J} x(\psi_{j'}, n + v) \\
&= \sum_j^J  \sum_{j'}^{J} \lim_{N \to \infty} \frac{1}{2 \Delta t N} \int_{-\Delta t N}^{\Delta t N} dn \quad   A_j A_{j'} \cos (\Delta t n(\omega_j - \omega_{j'}) - \Delta t \omega_{j'} v)  \cos (\Delta t n(\omega_j + \omega_{j'}) + \Delta t \omega_{j'} v + 2\psi)\\
&= \begin{cases}
& \sum_j^J \frac{A_j^2}{2} \cos(\omega_j \Delta t v), v = |n - m |, j = j' \\
& 0, j \neq j'
\end{cases}
\end{align}

Hence, $f(n)$ is a covariance stationary erogdic process.

\subsubsection{$\state$ is a Gaussian Prcoess}

The probability density function of a sum of random variables is a convolution of the individual probability density functions,   $x(\psi_j, n)$. Here, the central limit theorum grants that each element of $f(n)$ at $n$ appears Gaussian distributed for large $J$, irrespective of the underlying properties of $x(\psi_j, n)$, or the distribution of the phases $\psi$. Numerical analysis shows that $J$ greater than 15 - 20 results in $f(n)$ appearing as Gaussian distributed. 

NEWFIG

% \begin{figure}[h!]
% 	\centering
% 	\caption[Assumptions for Stochastic Processes: Gaussian engineered noise processes]{Histogram of engineered $\state$ increments for 4000 time steps, 500 realisations, uniformly distributed phases, $\psi$. We note that $J>15$ large shows evidence of Gaussianity of noise process. }
% %	\begin{subfigure}[h!]{0.3\textwidth}
% 		\includegraphics[width=0.3\textwidth]{gaussianity_noise_J_2.png}
% 		\caption{ Distribution of random increments $\state_{n,k}$, at time $n$ in $k^{th}$ realisation, for $J=2$, uniformly distributed $\psi$ } \label{fig:gaussianity_noise_J_2}
% %	\end{subfigure}
% %	\begin{subfigure}[h!]{0.3\textwidth}
% 		\includegraphics[width=0.3\textwidth]{gaussianity_noise_J_5.png}
% 		\caption{ Random increments $\state_{n,k}$, at time $n$ in $k^{th}$ realisation, for $J=4$, uniformly distributed $\psi$ } \label{fig:gaussianity_noise_J_5}
% %	\end{subfigure}
% %	\begin{subfigure}[h!]{0.3\textwidth}
% 		\includegraphics[width=0.3\textwidth]{gaussianity_noise_J_15.png}
% 		\caption{ Distribution of random increments $\state_{n,k}$, at time $n$ in $k^{th}$ realisation, for $J=15$, uniformly distributed $\psi$ } \label{fig:gaussianity_noise_J_15}
% %	\end{subfigure}
% \end{figure}

% Our numerical analysis tests the performance of filtering algorithms as the ratio $\frac{f_0 J}{2 \pi f_s}$ is varied. This satisfies Assumption 3 ($J$ cannot be infinite) and tests performance when Assumption 4 is relaxed.

% ##############################################################################
\section{Linear Predictors for Covariance Stationary $\state$ \label{sec:app:predictors}}
% ##############################################################################

% ##############################################################################
\subsection{LSF / AKF / QKF}
% ##############################################################################

Show the mean and covariance stationarity of AKF using AR (dynamics):

% ##############################################################################
\subsection{LKFFB}
% ##############################################################################

\subsection{Liska Kalman Predictor with Fixed Basis} \label{sec:ap_liska_deriv}

First, show the mean and covariance stationarity of LKFFB with adaptive noise matrix and rotation matrix for each substate :
\\
\\
\begin{defn} \label{sec:ap_ssp_circle} Stationary Processes on a Circle. Define a stochastic initial state and a deterministic evolution given by a rotation matrix. In particular, let  $A, B$ be random variables with mean, $\mu_{A,B}$, and variance, $\sigma_{A,B}^2$. Consider abstract index $t$ to denote time:
\begin{align}
x(t) &= \begin{bmatrix} x_1(t) \\ x_2(t) \\ \end{bmatrix} = \begin{bmatrix} \cos(t) & -\sin(t) \\ \sin(t) & \cos(t) \\ \end{bmatrix} \begin{bmatrix} A \\ B \\ \end{bmatrix} \\
E[x(t)]&= \begin{bmatrix} \mu_A \cos(t) - \mu_B \sin(t)\\ \mu_A \sin(t) + \mu_B \cos(t)\end{bmatrix}  \\
& = 0 \quad \text{(constant), $\iff \mu_{A,B} =0$} \\ 
E[x(t)x(s)^T]&= \begin{bmatrix} \cos(t) & -\sin(t) \\ \sin(t) & \cos(t) \\ \end{bmatrix} \begin{bmatrix} A \\ B \\ \end{bmatrix} \begin{bmatrix} A & B \\ \end{bmatrix} \begin{bmatrix} \cos(s) & \sin(s) \\ -\sin(s) & \cos(s) \\ %
\end{bmatrix} \\ 
&=\begin{bmatrix} 
\sigma_A^2\cos(t)\cos(s) + \sigma_B^2\sin(t)\sin(s) & (\sigma_A^2 - \sigma_B^2)\cos(t)\sin(s) \\ 
(\sigma_A^2 - \sigma_B^2)\cos(s)\sin(t) & \sigma_A^2 \cos(t)\cos(s) + \sigma_B^2\sin(t)\sin(s) \\ 
\end{bmatrix}, \text{$\iff E[AB] =0$} \\ 
&=\sigma^2 \begin{bmatrix} 
\cos(v) & 0 \\ 
0 & \cos(v)  \\
\end{bmatrix}, \text{$ \iff \sigma_A^2 = \sigma_B^2 = \sigma^2$ and with $v= |t-s|$} \label{eqn:cov_circle}
\end{align}
We see that the initial state variables, $A,B$, must be zero mean and i.i.d. variables for $x(t)$ to be covariance stationary. So far, we have not made any assumptions about the distributions that the initial state is drawn from, however, if $A,B$ are Gaussian, then the joint distribution, $x(t)$, remains Gaussian.
\end{defn}

The Liska Kalman Filter has adaptive noise matrix, $Q$. We ensure that the standard classical  Kalman Filter derivations are unchanged by the presence of the proposed $Q$ in \cite{liska}.
\\
\\
We consider zero mean Gaussian process and measurement noise $w,v$ (respectively); a `known' deterministic dynamical model $\Phi$ for the unobserved true state $x$, where $x$ is measured through the action of known deterministic $H$ to yield noisy measurements $y$. The Kalman system equations at discrete time steps, indexed by $n$, are:
\begin{align}
x_n &\equiv \Phi_{n-1}x_{n-1} + \Gamma_{n-1}w_{n-1} \label{eqn:KF_statemodel}\\
\Gamma_{n-1} &\equiv \Phi_{n-1}\frac{x_{n-1}}{\norm{x_{n-1}}} \\
y_n &\equiv H_n x_n + v_n \label{eqn:KF_msmtmodel}
\end{align}
To be specific, here, we are actually considering a single state space model associated with a frequency $\omega_j$. The full Kalman model will 'stack' many resonators to form a basis of oscillators on which our data will be projected. Further, this Kalman model can also track the instantaneous phase information, namely, the real and imaginary parts of a Hilbert transformed signal with adaptive noise features, as derived in \cite{liska}. The summary definitions are:
\begin{align}
s_n & \equiv \text{true signal component strength for $\omega_j$} \\
\mathcal{H}[s_n] & \equiv \text{imaginary part of Hilbert transform of $s_n$ for $\omega_j$} \\
x_n &\equiv \begin{bmatrix} s_n \\ \mathcal{H}[s_n] \end{bmatrix} \\
\norm{x_n}&\equiv \sqrt{x_{n}[0]^2 + x_{n}[1]^2} \text{ , [i] denoting $i^{th}$ component of $x_n$}
\end{align}
Without loss of generality, we ignore the Hilbert transform layer of the model ( and is deferred to Appendix [placeholder]). Instead, we proceed onwards by taking a two component state $x$ to represent our true Kalman state. 
\\
\\
The initial condition, $x_0$, is assumed known. We standardise notation such that $Hx \equiv f$ satisfies requirements of Appendices \ref{sec:ap_setup} - \ref{sec:ap_covariancefunctions}. The equations above differ from an ordinary Kalman Filter due to elements of $\Gamma$ depending on the state $x$ itself. The form of $\Phi, H$ is arbitrary and do not require a definition yet.
\\
\\
With $n,m$ denoting time indices, the properties of noise processes are:
\begin{align}
\ex{w_n} & = \ex{v_n}= 0 \quad \forall n \label{eqn:KF_stat_noisemean}\\
\ex{w_nw_m^T} &= \sigma^2 \delta(m-n),  \quad \forall n,m \label{eqn:KF_stat_noisepros}\\
\ex{v_n v_m^T}&= R\delta(m-n), \quad  \forall n,m  \label{eqn:KF_stat_noisemsmt} \\
\ex{w_n v_m} &= 0,  \quad \forall n,m \label{eqn:KF_stat_crosscorr}
\end{align} 
The process noise variance, $\sigma^2$ is a known scalar. The measurement noise variance $R$ is a known covariance matrix in general, but will turn out to be a scalar for our choice of measurement action $H$. This means we can interpret \ref{eqn:KF_stat_crosscorr} as product between scalars. Both $\sigma^2, R$ are tunable design parameters (so called hyper-parameters in machine learning language). Tuning these parameters through an additional optimisation routine is detailed in Appendix [placeholder]. 
\\
\\
At time step $n$, we search for a linear predictor $\hat{x}_n(+)$, with properties given by Theorem \ref{thm:prediction}. In particular, we must find unknowns $\lambda_n, \gamma_n$ below:
\begin{align}
\hat{U} & \equiv \{y_1, y_2,\dots, y_n\} \\
\hat{x}_n(+) & \equiv \lambda_n \hat{x}_n(-) + \gamma_n y_n \label{eqn:KF_predictor}\\
\hat{x}_n(-) & \equiv \Phi_{n-1} \hat{x}_{n-1}(+) \\
(+) &\equiv \text{Aposteriori state estimate  (state post prediction update) for time step $n$}\\
(-) &\equiv \text{Apriori state estimate (state priori to prediction update) for time step $n$}
\end{align}
To find unknowns $\lambda_n, \gamma_n$, we use Theorem \ref{thm:prediction} to impose orthogonality of estimator to the set of all known data:
\begin{align}
\ex{(x_n - \hat{x}_n(+))U^T} &= 0 \label{eqn:KF_reln_1}\\
\implies \ex{(x_n - \hat{x}_n(+))y_i^T} &= 0, \quad i = 1,\dots, n-1 \label{eqn:KF_reln_2}\\
\implies \ex{(x_n - \hat{x}_n(+))y_n^T} &= 0 \label{eqn:KF_reln_3}
\end{align}
We assert that the following correlation relations are true:
\begin{align}
\ex{x_{n-1}w_{n-1}} & = 0 \label{eqn:KF_reln_4}\\
\ex{w_{n-1} y_i} &= 0, \quad i = 1,2,\dots n-1 \label{eqn:KF_reln_5}\\
\ex{\frac{x_{n-1}}{\norm{x_{n-1}}}w_{n-1}} & = 0 \label{eqn:KF_reln_6}\\
\ex{x_{n-1} w_{n-1} x_{n-1}^T } & = 0 \label{eqn:KF_reln_7}\\
\ex{\frac{x_{n-1}}{\norm{x_{n-1}}} w_{n-1} x_{n-1}^T } & = 0 \label{eqn:KF_reln_8} \\
\ex{x_{n-1} w_{n-1} y_i } & = 0, \quad i = 1,\dots, n-1 \label{eqn:KF_reln_9}\\
\ex{\frac{x_{n-1}}{\norm{x_{n-1}}} w_{n-1} y_i } & = 0, \quad i = 1,\dots, n-1 \label{eqn:KF_reln_10}
\end{align}
We interpret these relations by observing that $x_{n-1}$ or $y_i$ do not contain a process noise term of the form $w_{n-1}$. At most, the expansion of $x_{n-1}, y_{n-1}$ terms contain $w_{n-2}$. Where terms of the form $w_{n-k}\dots w_{n-2}w_{n-1}$ appear, we invoke \ref{eqn:KF_stat_noisepros} and set these terms to zero. Where terms of the form $\propto w_{n-2}w_{n-2}w_{n-1} = \sigma^2 w_{n-1}$ appear, we invoke zero mean noise and set these terms to zero. The $\norm{x_{n-1}}$ term depends on $x_{n-1}$, and we assert that a similar logic holds where physically, a normed state cannot be correlated with a future process noise term.
\\
\\
The above relations imply that:
\begin{align}
\ex{x_n} &= \Phi_{n-1}\ex{x_{n-1}} \label{eqn:KF_reln_11}\\
\ex{(x_n - \apx{n})x_0^T} &= 0 \label{eqn:KF_reln_12}\\
\ex{(x_n - \apx{n})\hat{y}_n(-)^T} &=0 \label{eqn:KF_reln_13} \\
\ex{(x_n - \amx{n})y_i^T} &=0, \quad i = 1,\dots, n-1\label{eqn:KF_reln_14}\\
\ex{\Gamma_n w_n \apx{n}^T} &=0 \label{eqn:KF_reln_16}\\
\ex{\Gamma_n w_n x_n^T} &=0 \label{eqn:KF_reln_17} 
\end{align}
We  find $\lambda_n$ using orthogonality conditions at different time steps:
\begin{align}
\ex{(x_n - \apx{n})y_i^T} &=0, \quad i = 1,\dots, n-1\label{eqn:KF_reln_15}\\ 
\implies 0 &= \ex{(x_n - \apx{n})y_i^T} \\
& = \ex{(\Phi_{n-1}x_{n-1}(\idn + \frac{w_{n-1}}{\norm{x_{n-1}}}) - \lambda_n \hat{x}_n(-) - \gamma_n y_n)y_i^T} \\
&= \Phi_{n-1}\ex{x_{n-1}y_i^T} + \Phi_{n-1}\ex{\frac{x_{n-1}w_{n-1}}{\norm{x_{n-1}}}y_i^T} - \lambda_n \ex{ \hat{x}_n(-)y_i^T} - \gamma_n H_n \ex{ x_n y_i^T} - \gamma_n \ex{ v_n y_i^T} \\
&= \Phi_{n-1}\ex{x_{n-1}y_i^T} + \Phi_{n-1}\ex{\frac{x_{n-1}w_{n-1}}{\norm{x_{n-1}}}y_i^T} - \lambda_n \ex{ \hat{x}_n(-)y_i^T} - \gamma_n H_n\ex{ x_n y_i^T}, \quad \text{by \ref{eqn:KF_stat_noisemsmt} } \\
&= \ex{\Phi_{n-1}x_{n-1}y_i^T} - \lambda_n \ex{ \hat{x}_n(-)y_i^T} - \gamma_n H_n\ex{ x_n y_i^T}, \quad \text{by \ref{eqn:KF_reln_10} } \\
&=\ex{x_{n}y_i^T} - \lambda_n \ex{ \hat{x}_n(-)y_i^T} - \gamma_n H_n\ex{ x_n y_i^T}, \quad \text{by \ref{eqn:KF_reln_11}} \\
&= \ex{x_{n}y_i^T} + \lambda_n \ex{( x_n - \hat{x}_n(-))y_i^T} - \lambda_n \ex{ x_n y_i^T} - \gamma_n H_n\ex{ x_n y_i^T}, \quad \text{add $\pm \lambda_n \ex{ x_n y_i^T}$} \\
&= \ex{x_{n}y_i^T} - \lambda_n \ex{ x_n y_i^T} - \gamma_n H_n\ex{ x_n y_i^T}, \quad \text{by \ref{eqn:KF_reln_2}} \\
0 &= \ex{(\idn - \lambda_n  - \gamma_n H_n)x_n y_i^T} \\
\ex{x_n y_i^T} \neq 0 \\
\implies \lambda_n &= \idn  - \gamma_n H_n
\end{align}
We find $\gamma_n$ using orthogonality conditions with all terms at the same time step:
\begin{align}
e_n & \equiv \amx{n} - x_n,  \\
\amp{n} & \equiv E[e_ne_n^T] \\
R_n & \equiv E[v_nv_n^T]\\
E[v_n e_n^T]&=0  \\
\ex{(x_n - \apx{n})(\hat{y}_n(-) - y_n)^T} &=0 \quad \text{by \ref{eqn:KF_reln_3}}, \ref{eqn:KF_reln_13} \\
\implies 0 &= E\left[(x -\lambda_n \amx{n} - \gamma_n H_n x_n - \gamma_n v_n)(H_n (\amx{n} - x_{k}) -v_n)^T\right] \\
&= E\left[(x_n - \amx{n} + \gamma_n H_n\amx{n} - \gamma_n H_n x_n - \gamma_n v_n)(H_n (\amx{n} - x_{k}) -v_n)^T\right]  \\
&= E\left[(x_n - \amx{n} + \gamma_n H_n\amx{n} - \gamma_n H_n x_n - \gamma_n v_n)(H_n (\amx{n} - x_{k}) -v_n)^T\right].  \\
&= E\left[(-e_n + \gamma_n H_ne_n - \gamma_n v_n)(H_n e_n -v_n)^T\right]  \\
&= E\left[(-e_ne_n^T H_n^T+ \gamma_n H_ne_ne_n^T H_n^T - \gamma_n v_ne_n^T H_n^T + e_nv_n^T - \gamma_n H_ne_nv_n^T + \gamma_n v_nv_n^T \right]  \\
&= -\amp{n} H_n^T+ \gamma_n H_n\amp{n}H_n^T + \gamma_n R_n \\
\implies \gamma_n &= \amp{n} H_n^T(H_n\amp{n}H_n^T + R_n)^{-1} \quad \text{(Kalman Gain)} \label{eqn:KF_update_gamma}
\end{align}
Hence, the form of optimal predictor $\apx{n}$ in the same time step $n$ is:
\begin{align}
\apx{n} &= (1 - \gamma_nH_n) \amx{n} + \gamma_ny_n \\
& =  \amx{n} + \gamma_n(y_n - H_n\amx{n}) \\
&= \amx{n} + \gamma_n (y_n - \hat{y}_n(-)) \label{eqn:KF_update_x+} 
\end{align}
We also define optimal predictor for the state variance:
\begin{align}
\text{Using $2^{nd}$ line, }\apx{n} &=  \amx{n} - \gamma_nH_n \amx{n} + \gamma_nH_n x_n + \gamma_nv_n  \nonumber \\
e^{+}_n &\equiv \apx{n} - x_n \\
&=  \amx{n} - \gamma_nH_n \amx{n} + \gamma_nH_n x_n + \gamma_nv_n - x_n \nonumber \\
&=  e_n - \gamma_nH_n e_n + \gamma_nv_n = \left[1 - \gamma_nH_n \right] e_n + \gamma_nv_n \nonumber \\
\app{n} &\equiv \ex{e_n^+ e_n^{+T}} \\
& = E \left[ \left[1 - \gamma_nH_n \right] e_n e_n^T \left[1 - \gamma_nH_n \right]^T + \gamma_nv_n e_n^T \left[1 - \gamma_nH_n \right]^T   +  \left[1 - \gamma_nH_n \right] e_n v_n^T \gamma^T + \gamma_nv_n v_n^T \gamma^T \right] \\
& = \left[1 - \gamma_n H_n \right] \amp{n}\left[1 - \gamma_n H_n \right]^T + \gamma_n R_n \gamma_n^T  \\
& =  \amp{n} - \amp{n}H_n^T\gamma_n^T - \gamma_n H_n \amp{n} + \gamma_n \left[ H_n \amp{n}H_n^T +  R_n \right] \gamma_n^T, \quad \text{using $\gamma_n$}\\
&= \amp{n} - \amp{n}H_n^T\gamma_n^T - \gamma_n H_n \amp{n} +  \amp{n}H_n^T \gamma_n^T  \\ 
&= \left[1  - \gamma_n H_n \right] \amp{n} \label{eqn:KF_update_p+}
\end{align}
This optimal variance can be propagated from one time step to the next as:
\begin{align}
e_n &= \amx{n} - x_n \\
&= \Phi_{n-1}\left[\apx{n-1} - x_{n-1} \right] - \Gamma_{n-1}w_{n-1} \\
&= \Phi_{n-1}e^{+}_{n-1} - \Gamma_{n-1}w_{n-1} \\
\amp{n} &= \Phi_{n-1}E\left[e^{+}_{n-1}e^{+T}_{n-1}\right]\Phi_{n-1}^T  \\
& - \Phi_{n-1}E\left[e^{+}_{n-1}w_{n-1}^T\Gamma_{n-1}^T\right] - E\left[\Gamma_{n-1}w_{n-1}e^{+T}_{n-1}\right]\Phi_{n-1}^T  \\
& + E\left[\Gamma_{n-1}w_{n-1}w_{n-1}^T\Gamma_{n-1}^T\right] \\
&= \Phi_{n-1}E\left[\app{n-1}\right]\Phi_{n-1}^T  + E\left[\Gamma_{n-1}w_{n-1}w_{n-1}^T\Gamma_{n-1}^T\right] \\
& - \Phi_{n-1}E\left[(\apx{n-1} - x_{n-1})w_{n-1}^T\Gamma_{n-1}^T\right] - E\left[\Gamma_{n-1}w_{n-1}(\apx{n-1} - x_{n-1})^T\right]\Phi_{n-1}^T  \\
&= \Phi_{n-1} \app{n-1} \Phi_{n-1}^T + E\left[\Gamma_{n-1}w_{n-1}w_{n-1}^T\Gamma_{n-1}^T\right], \quad \text{$2^{nd}$ line zero by \ref{eqn:KF_reln_16}, \ref{eqn:KF_reln_17}} \\
 &= \Phi_{n-1} \app{n-1} \Phi_{n-1}^T + Q_{n-1}, \quad Q_{n} \equiv E\left[\Gamma_{n}w_{n}w_{n}^T\Gamma_{n}^T\right] \label{eqn:KF_update_p-}
\end{align}
Hence, the standard Kalman predictor equations - \ref{eqn:KF_update_gamma}, \ref{eqn:KF_update_x+}, \ref{eqn:KF_update_p+}, \ref{eqn:KF_update_p-} - are valid for adaptive noise features given in \cite{liska}. They are summarised below in order of computation. 
\begin{align}
\amx{n} & = \Phi_{n-1} \apx{n-1} \\
\Gamma_{n-1} &=\Phi_{n-1}\frac{\apx{n-1}}{\norm{\apx{n-1}}} \\
Q_{n-1} & = \sigma^2 \Gamma_{n-1}\Gamma_{n-1}^T \\
\amp{n}&= \Phi_{n-1} \app{n-1} \Phi_{n-1}^T + Q_{n-1} \\
\gamma_n &= \amp{n} H_n^T(H_n\amp{n}H_n^T + R_n)^{-1} \\
\hat{y}_n(-) & = H_n \amx{n} \\
\apx{n} &= \amx{n} + \gamma_n (y_n - \hat{y}_n(-)) \\
\app{n} &= \left[1  - \gamma_n H_n \right] \amp{n}
\end{align}
Unlike a typical Kalman filter, state estimation cannot be decoupled from state variance estimation due to $\Gamma$. This means Kalman gains cannot be calculated in advance of data collection.

\newpage
\underline{\textbf{Proofs \ref{eqn:KF_reln_4} - \ref{eqn:KF_reln_17}:}}
\\
\begin{align}
\ex{x_{n-1} w_{n-1}} & = \ex{\Phi_{n-2}x_{n-2}(\idn + \frac{w_{n-2}}{\norm{x_{n-2}}}) w_{n-1}} \\
& = \ex{\Phi_{n-2}x_{n-2}w_{n-1}} +\ex{ \frac{\Phi_{n-2}x_{n-2} w_{n-2}}{\norm{x_{n-2}}} w_{n-1}} \\
& = \ex{\Phi_{n-2}x_{n-2}w_{n-1}} \\
& = \ex{\Phi_{n-2}\Phi_{n-3}x_{n-3}(\idn + \frac{w_{n-3}}{\norm{x_{n-3}}})w_{n-1}} \\
& \vdots \\
& = \Phi_{n-2}\dots\Phi_{n-i+1}x_0\ex{w_{n-1}} \\
&= 0 \\
\nonumber \\ 
\ex{w_{n-1} y_i} &= \ex{w_{n-1}  H_{n-1} x_{n-1} + w_{n-1}v_{n-1}}  \\
 &= \ex{w_{n-1}  H_{n-1} x_{n-1}}, \quad\text{by \ref{eqn:KF_stat_crosscorr} } \\ 
 &= H_{n-1} \ex{w_{n-1} x_{n-1}} \\
 &= 0, \quad\text{by \ref{eqn:KF_reln_4}} \\
\nonumber \\ 
\ex{\frac{x_{n-1}}{\norm{x_{n-1}}}w_{n-1}} & = \ex{\frac{\Phi_{n-2}x_{n-2}(\idn + \frac{w_{n-2}}{\norm{x_{n-2}}})}{\norm{x_{n-1}}}w_{n-1}} \\
& = \ex{\frac{\Phi_{n-2}x_{n-2}w_{n-1} + \frac{w_{n-2}w_{n-1}}{\norm{x_{n-2}}}}{\norm{x_{n-1}}}} \\
& = \ex{\frac{\Phi_{n-2}x_{n-2}w_{n-1}}{\norm{x_{n-1}}} + \frac{w_{n-2}w_{n-1}}{\norm{x_{n-2}}\norm{x_{n-1}}}} \\
&\vdots \\
&= 0 \quad \text{since $x_{n-1, n-2, \dots}$, $\norm{x_{n-1,n-2, \dots}}$ cannot be correlated with future  noise, $w_{n-1}$.}\\
\nonumber \\ 
\ex{x_{n-1} w_{n-1} x_{n-1}^T} &= \ex{\Phi_{n-2}x_{n-2}(\idn + \frac{w_{n-2}}{\norm{x_{n-2}}}) w_{n-1} \Phi_{n-2}x_{n-2}(\idn + \frac{w_{n-2}}{\norm{x_{n-2}}})} \\
&= \ex{\Phi_{n-2}x_{n-2} w_{n-1} x_{n-2}^T\Phi_{n-2}^T(\idn + \frac{w_{n-2}}{\norm{x_{n-2}}})^2} \\
&= 0 \quad \text{since $x_{n-1, n-2, \dots}$, $\norm{x_{n-1,n-2, \dots}}$ cannot be correlated with future  noise, $w_{n-1}$.}
\end{align}
\ref{eqn:KF_reln_8} is justified identically to \ref{eqn:KF_reln_7}. Recognising that $y_i = H_i x_i + v_i$ and that $\ex{w_jv_i} = 0 \forall i,j$, we can justify \ref{eqn:KF_reln_9}, \ref{eqn:KF_reln_10} with the same reasoning as \ref{eqn:KF_reln_7}.
\begin{align}
\ex{x_n} &= \ex{\Phi_{n-1}x_{n-1}(\idn + \frac{w_{n-1}}{\norm{x_{n-1}}})} \\
&= \Phi_{n-1}\ex{x_{n-1}} + \Phi_{n-1}\ex{x_{n-1}\frac{w_{n-1}}{\norm{x_{n-1}}})} \\
&= \Phi_{n-1}\ex{x_{n-1}}, \quad \text{by \ref{eqn:KF_reln_6}} \\
\nonumber \\ 
\ex{(x_n - \apx{n})x_0} &= \ex{(x_n - \apx{n})y_1} \\
&= 0 \quad \text{by \ref{eqn:KF_reln_2}, for $i=1$.}
\end{align}
\begin{align}
\text{To prove } \ex{(x_n - \apx{n})\hat{y}_n(-)^T} &=0, \quad \text{note:} \\
\amx{n} & \equiv \Phi_{n-1} \apx{n-1} \label{eqn:amx}\\
\hat{y}_n(-) & \equiv H_n \amx{n} \\
&= H_n \Phi_{n-1} \apx{n-1} \\
&= H_n \Phi_{n-1} \lambda_{n-1} \amx{n-1} + H_n \Phi_{n-1} \gamma_{n-1} y_{n-1}, \quad \text{by \ref{eqn:KF_predictor}} \\
&= H_n \Phi_{n-1} \lambda_{n-1} \Phi_{n-2} \apx{n-2} + H_n \Phi_{n-1} \gamma_{n-1} y_{n-1} \\
&\vdots \nonumber \\
& = \beta_0 x_0 + \sum_{k=1}^{n-1} \beta_k y_k, \quad \text{$\beta_k$ is a deterministic coefficent $H, \Phi, \gamma, \lambda$ over $\{n\}$.}\\
\implies \ex{(x_n - \apx{n})\hat{y}_n(-)} &= \ex{(x_n - \apx{n})(x_0^T\beta_0^T  + \sum_{k=1}^{n-1} y_k^T \beta_k^T } \\
&= \ex{(x_n - \apx{n})x_0^T}\beta_0^T  + \sum_{k=1}^{n-1} \ex{(x_n - \apx{n})y_k^T} \beta_k^T  \\
&= \sum_{k=1}^{n-1} \ex{(x_n - \apx{n})y_k^T} \beta_k^T, \quad \text{by \ref{eqn:KF_reln_12}}\\
&=0, \quad \text{by \ref{eqn:KF_reln_2}} \\
\nonumber \\ 
\ex{(x_n - \amx{n})y_i^T} &= \ex{(x_n - \Phi_{n-1} \apx{n-1})y_i^T}, \quad i = 1,\dots,n-1 \\
&=0, \quad \text{by \ref{eqn:KF_reln_2}} \\
\nonumber \\
\ex{\Gamma_n w_n \apx{n}^T} &=\ex{\Gamma_n w_n  \hat{x}_n(-)^T \lambda_n^T} +  \ex{\Gamma_n w_n  y_n^T \gamma_n^T} \quad \text{by \ref{eqn:KF_predictor}}\\
&=\ex{\frac{\Phi_{n}x_{n}}{\norm{x_{n}}} w_n  \hat{x}_n(-)^T \lambda_n^T} +  \ex{\frac{\Phi_{n}x_{n}}{\norm{x_{n}}} w_n  y_n^T \gamma_n^T} \\
&=\ex{\frac{\Phi_{n}x_{n}}{\norm{x_{n}}} w_n  \hat{x}_n(-)^T \lambda_n^T} \quad \text{by \ref{eqn:KF_reln_10}} \\
&=\Phi_{n}\ex{\frac{x_{n}}{\norm{x_{n}}} w_n \apx{n-1}^T} \Phi_{n-1}^T \lambda_n^T \quad \text{by \ref{eqn:amx}} \\
&\vdots \quad \text{repeatedly apply \ref{eqn:KF_predictor}, \ref{eqn:KF_reln_10}, \ref{eqn:amx}} \\
&=0, \quad \text{past terms uncorrelated with future $w_n$} \\
\nonumber \\ 
\ex{\Gamma_n w_n x_n^T} &= \Phi_{n}\ex{\frac{x_{n}}{\norm{x_{n}}} w_n x_n^T} =0 \quad \text{by \ref{eqn:KF_reln_8} } 
\end{align}

% ##############################################################################
\subsection{GPR}
% ##############################################################################
\begin{thm} Bochner's Theorum \cite{rasmussen2006gaussian}. A complex valued  function $R(v), \quad v \in \Re^D$  is the covariance function of a covariance stationary mean square continuous complex valued random process on $\Re^D$ if and only if it can be represented as:  
\begin{align}
R(v) & = \int_{\Re^D} \exp^{2 \pi s v} d\mu(s) \label{eqn:bochnerthm}
\end{align}
for $\mu$ some positive finite measure. If $\mu$ has a density with respect to the Lebesque measure, then $R(v)$ and the density $S(s)$ are Fourier duals.
\end{thm}

\begin{defn} \label{sec:ap_ssp_tp} Trigonometric Polynomials.  Any zero mean covariance stationary process can be represented as the mean square limit of a sequence of processes, called trignometric polynomials, defined below. Let $A_j, B_j$ be i.i.d random variables with mean,$\mu_j$, and variance, $\sigma_j^2$. We have not specified the distribution that $A_j, B_j$ are drawn from. In particular, let the abstract index be $j \in J, j = 0,1,2... $, and:
\begin{align}
\ex{A_k A_j} &= \ex{B_k B_j} = \sigma_j^2 \delta_{j,k} \quad \forall k,j \in J\\
\ex{A_k B_j} &= 0 \quad  \forall k,j \in J \label{eqn:karlin_trig_cross} \\
f_n &= \sum_{j=0}^{J} A_j \cos(\omega_j n) + B_j \sin(\omega_j n) \label{eqn:E_f_trigpolynomial} \\
\text{Then: }\ex{f_n} &=  \sum_{j=0}^{J} E[A_j] \cos(\omega_j n) + E[B_j] \sin(\omega_j n) \\
& =  \sum_{j=0}^{J}  \mu_j(\cos(\omega_j n) + \sin(\omega_j n)) \\
\text{$\ex{f_n}$ is constant $\forall n$} \quad \Leftrightarrow \mu_j &=0  \quad \forall j\\
\implies \ex{f_n} &=  0 \\
\ex{f_n f_{n+v}} &= \sum_j^{J} \sum_{j'}^{J} \sigma_j^2\delta{j,j'} [\cos(\omega_j n)\cos(\omega_j' (n+v)) + \sin(\omega_j n)\sin(\omega_j' (n+v)) ]\\
&= \sum_j^{J} \sigma_j^2 \cos(\omega_jv), \quad \text{$(\cos(a)\cos(b) + \sin(a)\sin(b) = \cos(a-b))$} \\
&= \sigma^2 \sum_j^{J}  p_j \cos(\omega_jv). \quad \text{$(p_j = \frac{\sigma_j^2}{\sum_j \sigma_j^2} = \frac{\sigma_j^2}{\sigma^2})$}
\end{align}
Consider $\omega_j =0$. For this frequency, \{$f_n$\} behaves like a perfectly correlated random variable (DC term), violating Assumption \ref{azm:Rbandlimit}. Consider $J \to \infty$, with all non zero $\omega_j$ equally represented, hence $p_j \to 0$ . Then we expect $R(v)$ is nearly zero for all $v\neq 0$ and will $R(v)=\sigma^2$ for $v=0$, that is, \{$f_n$\} behaves like a delta-correlated process. 
\end{defn}


\subsubsection{Gaussian Process Regression with Periodic Kernel} \label{sec:ap_approxSP:GPRPKernel}

We use high level remarks in \cite{solin2014} to explicitly work out that a sine squared exponential (Periodic Kernel) used in Gaussian Process Regression satisfies Def. \ref{sec:ap_ssp_tp}:

\begin{align}
R(v) &\equiv \sigma^2 \exp (- \frac{2\sin^2(\frac{\omega_0 v}{2})}{l^2}) \\
\omega_0 &\equiv \frac{\omega_j}{j}, j \in \{0, 1,..., J\} \\
\text{Using } 2\sin^2(\frac{\omega_0 v}{2}) &=  1 - \cos(\omega_0 v) \quad \text{we get:}\\
R(v) &=  \sigma^2 \exp (- \frac{1}{l^2}) \exp (\frac{\cos(\omega_0 v)}{l^2}) \\
 &=  \sigma^2 \exp (- \frac{1}{l^2}) \sum_{n = 0}^{\infty} \frac{1}{n!} \frac{\cos^n(\omega_0 v)}{l^{2n}} \label{eqn:periodic_1}\\
&= \sigma^2 \exp (- \frac{1}{l^2}) \left[\sum_{n_{odd}}^{\infty} \frac{1}{n!} \frac{\cos^n(\omega_0 v)}{l^{2n}} + \sum_{n_{even}}^{\infty} \frac{1}{n!} \frac{\cos^n(\omega_0 v)}{l^{2n}} + 1 \right] \label{eqn:cosine_powers} \\ \begin{split} 
& =\sigma^2 \exp (- \frac{1}{l^2})  \left[ \sum_{n_{odd}}^{\infty} \frac{1}{2^{n-1}} \frac{1}{l^{2n}} \frac{1}{n!}  \sum_{k=0}^{\frac{n}{2} - \frac{1}{2}} \binom{n}{k} \cos(\omega_0v (n -2k))\right] \\
& +\sigma^2 \exp (- \frac{1}{l^2})  \left[ \sum_{n_{even}}^{\infty}  \frac{1}{2^{n-1}} \frac{1}{l^{2n}} \frac{1}{n!}  \sum_{k=0}^{\frac{n}{2} - 1} \binom{n}{k} \cos(\omega_0v (n -2k))\right] \\
& +\sigma^2 \exp (- \frac{1}{l^2})  \left[ 1 +  \sum_{n_{even}}^{\infty}  \frac{1}{2^{n}} \binom{n}{\frac{n}{2}} \frac{1}{l^{2n}} \frac{1}{n!} \right] \label{eqn:cosine_powers2}
\end{split} 
\end{align}
At \ref{eqn:cosine_powers}, we expand each cosine using power reduction formulae for odd and even powers respectively. The $n=0$ term gives rise to the constant $1$ term.  We can expand each of the sums in \ref{eqn:cosine_powers2} and then find a pattern that allows us to regroup the terms across summations. For example, if we expand for $n = 0,1,2,3,4,5...$, we get:
\begin{align}
\text{1st Line of \ref{eqn:cosine_powers2}} &= \frac{2}{(2l^2)}\binom{1}{0} \cos(\omega_0 v) + \frac{2}{(2l^2)^3} \frac{1}{3!} \left[\binom{3}{0} \cos(3 \omega_0 v) + \binom{3}{1} \cos(\omega_0v) \right]  \\
&+ \frac{2}{(2l^2)^5} \frac{1}{5!} \left[\binom{5}{0} \cos(5\omega_0v) + \binom{5}{1} \cos(3\omega_0 v) + \binom{5}{2} \cos(\omega_0 v)\right] + ... \\
\text{2nd Line of \ref{eqn:cosine_powers2}} &= \frac{2}{(2l^2)^2} \frac{1}{2!} \binom{2}{0}\cos(2 \omega_0 v) + \frac{2}{(2l^2)^4} \frac{1}{4!} \left[\binom{4}{0} \cos(4 \omega_0 v) + \binom{4}{1} \cos(2 \omega_0 v) \right] + ... \\
\text{3rd Line of \ref{eqn:cosine_powers2}} &=  1 + \frac{1}{(2l^2)^2} \frac{1}{2!} \binom{2}{1} + \frac{1}{(2l)^4} \frac{1}{4!} \binom{4}{2} + ...
\end{align}
Substituting and rearranging, $R(v)$ becomes:
\begin{align}
R(v) &= \sigma^2 \exp (- \frac{1}{l^2}) \cos(\omega_0 v) \left[ \frac{2}{(2l^2)}\binom{1}{0} + \frac{2}{(2l^2)^3} \frac{1}{3!} \binom{3}{1} +  \frac{2}{(2l^2)^5} \frac{1}{5!}\binom{5}{2} \dots \right] \label{eqn:cosine1}\\
& + \sigma^2 \exp (- \frac{1}{l^2}) \cos(2\omega_0 v) \left[ \frac{2}{(2l^2)^2} \frac{1}{2!} \binom{2}{0} + \frac{2}{(2l^2)^4} \frac{1}{4!} \binom{4}{1} + \dots \right] \\
& + \sigma^2 \exp (- \frac{1}{l^2}) \cos(3\omega_0 v) \left[ \frac{2}{(2l^2)^3} \frac{1}{3!} \binom{3}{0} + \frac{2}{(2l^2)^5} \frac{1}{5!}\binom{5}{1} \dots \right] \\
& + \sigma^2 \exp (- \frac{1}{l^2}) \cos(4\omega_0 v) \left[ \frac{2}{(2l^2)^4} \frac{1}{4!} \binom{4}{0} + \dots \right] \\
& + \sigma^2 \exp (- \frac{1}{l^2}) \cos(5\omega_0 v) \left[ \frac{2}{(2l^2)^5} \frac{1}{5!}\binom{5}{0} + \dots \right] \label{eqn:cosine5}\\
& \vdots \nonumber \\
& + \sigma^2 \exp (- \frac{1}{l^2}) \left[ \frac{1}{(2l^2)^2} \frac{1}{2!} \binom{2}{1} + \frac{1}{(2l)^4} \frac{1}{4!} \binom{4}{2} + \dots \right] + \sigma^2 \exp (- \frac{1}{l^2}) \label{eqn:eventerms}
\end{align}
Here, the vertical and horizontal dots represent contributions from $n>5$ terms. We now summarise the amplitudes of each cosine term in \ref{eqn:cosine1} to  \ref{eqn:cosine5} as $p_j$:
\begin{align}
\text{\ref{eqn:cosine1} to  \ref{eqn:cosine5}} & =\sum_{j=1}^{\infty} p_j \cos(j\omega_0 v) \label{eqn:beta_series1} \\
p_j & \equiv \sigma^2 \exp (- \frac{1}{l^2}) \sum_{\beta = 0}^{\beta = \beta_{j,n}^{MAX}} \frac{2}{(2l^2)^{(j + 2\beta)}} \frac{1}{(j + 2\beta)!} \binom{j + 2\beta}{\beta} \label{eqn:beta_series2} \\
\beta &\equiv  0,1,..., \beta_{j,n}^{MAX} 
\end{align}
The truncation of the sum $\beta_{j,n}^{MAX}$ depends on $j$ and on the truncation, if any, of original index $n = 0,1,2....$. Hence, an analytic value for $\beta_{j,n}^{MAX}$ requires us to explicitly truncate $n$ at $N$, and we defer this for now. We summarise \ref{eqn:eventerms} as:
\begin{align}
\text{\ref{eqn:eventerms}} &= \sigma^2 \exp (- \frac{1}{l^2}) \sum_{\alpha = 0}^{\alpha = \alpha_{n}^{MAX}} \frac{1}{(2l^2)^{(2\alpha)}} \frac{1}{(2\alpha)!} \binom{2\alpha}{\alpha} \label{eqn:alpha_series}\\
\alpha &\equiv  0,1,..., \alpha_{n}^{MAX} 
\end{align}
Note that $\alpha_{n}^{MAX}$ depends only on $n$ (and the truncation of this index), but not $j$. If we set $j=0$ in \ref{eqn:beta_series2}, \ref{eqn:alpha_series} and \ref{eqn:beta_series2} differ only by a factor of $1/2$ and are otherwise identical in form. 
\\
\\
Next, we note that truncation of $n$ at $N$ effectively truncates the index $j$ at $J$. As in \cite{solin2014}, if the index $n$ is truncated at $N$, we observe that $\beta_{j,N}^{MAX} = \lfloor\frac{N-j}{2}\rfloor$ and $\alpha_{N}^{MAX} = \lfloor\frac{N}{2}\rfloor$  where $\lfloor \rfloor$ denotes the ceiling floor. As a concrete example, Table \ref{tab:truncation_n} demonstrates value of $\beta_{j,N}^{MAX}$ for two examples, namely truncation at $N=4$ (even powers) and truncation at $N=5$ (odd powers). 

\begin{table}[h]
	\centering
	\begin{tabular}{rc|c|cc} 
		\hline
		N & j & $\beta_{j,N}^{MAX}$ (from \ref{eqn:cosine1}-\ref{eqn:cosine5}) & $\frac{N-j}{2}$ & $\lfloor\frac{N-j}{2}\rfloor$ \\
		\hline
		N=5 & 1& 2& 2 & 2 \\
			& 2& 1& 1.5 & 1 \\
			& 3& 1& 1 & 1 \\
			& 4& 0& 0.5 & 0 \\
			& 5& 0& 0 & 0 \\
		\hline
		N=4 & 1& 1& 1.5 & 1 \\
			& 2& 1& 1 & 1 \\
			& 3& 0& 0.5 & 0 \\
			& 4& 0& 0 & 0 \\
		\hline
	\end{tabular}
	\caption[Covariance Functions: Values of $\beta_{j,N}^{MAX}$ given $j,N$ ]{Values of $\beta_{j,N}^{MAX}$ given $j,N$}
	\label{tab:truncation_n}
\end{table}
The values for $\alpha_{N=4}^{MAX} = \alpha_{N=5}^{MAX} = 2$ are straightforward to confirm by observing \ref{eqn:eventerms}. Hence it is possible to start the sum in \ref{eqn:beta_series1} at $j=0$ and add (or subtract) half the value of \ref{eqn:alpha_series} depending on whether we want to keep (eliminate) the true value of a $j=0$ DC term. The final result, with $j=0$ term retained, and truncation at $N \equiv J$, is:
\begin{align}
R(v) &= \sigma^2 (p_{0,J} + \sum_{j=0}^{J} p_{j,J} \cos(j\omega_0 v))\\
p_{0,J} & \equiv \frac{1}{2} \exp (- \frac{1}{l^2}) \sum_{\alpha = 0}^{\alpha = \lfloor\frac{J}{2}\rfloor} \frac{1}{(2l^2)^{(2\alpha)}} \frac{1}{(2\alpha)!} \binom{2\alpha}{\alpha} \label{eqn:p0J}\\
p_{j,J} & \equiv \exp (- \frac{1}{l^2}) \sum_{\beta = 0}^{\beta = \lfloor\frac{J-j}{2}\rfloor} \frac{2}{(2l^2)^{(j + 2\beta)}} \frac{1}{(j + 2\beta)!} \binom{j + 2\beta}{\beta} \label{eqn:pjJ}
\end{align}
This derivation agrees with final results reported in \cite{solin2014}. We adjust the covariance function for the constant $j=0$ to recover the same form as \ref{sec:ap_ssp_tp}:
\begin{align}
R'(v) & \equiv R(v) - \sigma^2 p_{0,J} = \sigma^2 \sum_{j=0}^{J} p_{j,J} \cos(j\omega_0 v) 
\end{align}
However, we note that $p_{j,J}$ is not the normalised $p_j$ defined in  \ref{sec:ap_ssp_tp}; instead the coefficients are given by \ref{eqn:p0J}.

% \section{ Predictor Performance Relative to Cramer Rao Bound}

% ##############################################################################
\section{Experimental Verification Procedure \label{sec:app:exptverfication}}
% ##############################################################################

We reproduce an alternative derivation to \cite{soare} to justify that a sequence of Ramsey measurements on a qubit interacting with a dephasing noise in the carrier will be indistinguishable from the affects of environmental dephasing. We start with the Hamiltonian for a two level system interacting with a classical field. We consider probability amplitude equations of motion for our two level energy eigenstates in the presence of a noisy classical field. We move to an interaction picture twice to see that the derivative of phase noise in the carrier is indistinguishable from environmental dephasing and both affect the expectation value of a projective $\p{z}$ measurement. In a particular parameter regime, we recover the final result of \cite{soare}. 

\subsection{Two Level System in Free Evolution}

We define a two level system interacting with a magnetic field in Fig. \ref{fig:set_up}, and expand the system Hamiltonian as: 
\\
\begin{align}
\op{H_0} &= E_1\ket{1}\bra{1} + E_2\ket{2}\bra{2} \\
\p{z} &\equiv \ket{2}\bra{2} - \ket{1}\bra{1} \\
\op{\mathcal{I}} & \equiv \ket{1}\bra{1} + \ket{2}\bra{2} \\
\Rightarrow \op{H_0} & = \frac{1}{2} (E_1\ket{1}\bra{1}+ E_2\ket{2}\bra{2}) \\
& + \frac{1}{2} [(E_2 - E_1)\p{z} + E_1 \ket{2}\bra{2} + E_2 \ket{1}\bra{1}]\\
& = \op{\mathcal{I}} \left( \frac{E_1 + E_2}{2} \right) + \p{z}(\frac{E_2 - E_1}{2}) \\
\text{Let: } \quad E_{1,2} &\equiv \mp \frac{1}{2} \hbar \omega_A, \quad \text{to result in:} \\
\op{H_0} &= \frac{1}{2} \hbar \omega_A \p{z}
\end{align}
\\
\\
% \begin{figure}[h]
% 	\centering
% 	%\includegraphics[width=0.5\textwidth]{two_level_system.png}
% 	\caption[Setup: Diagram of Two Level System]{Diagram of Two Level System}
% 	\label{fig:set_up}
% \end{figure}
% \\
% \\
\subsection{Classical Field with Noise}

We define our magnetic field with noise identically to \cite{soare}, as:
\begin{align}
\vec{B} &\equiv \Omega(t) \cos(\omega_\mu t + \phi(t)) \op{z} \\
\phi(t) & \equiv \phi_C(t) + \phi_N(t)
\end{align}
with a real control amplitude $\Omega(t)$, carrier frequency $\omega_\mu$, controlled phase $\phi_C(t)$ and stochastic phase noise $\phi_N(t)$.  It is possible to add amplitude noise to the control, i.e. $\Omega(t) \equiv \Omega_C(t) + \Omega_N(t)$, however we set $\Omega_N(t) =0$ at present. 
\\
\\
\subsection{System-Field Interaction}

We define the dipole operator for a spin half particle in a magnetic field, $\op{d}$, as:
\begin{align}
\idn \op{d} \idn & = (\pj{1} + \pj{2})\op{d} (\pj{1} + \pj{2}) \\
& = \pj{1}\op{d}\pj{2} +  \pj{2}\op{d}\pj{1} \\
& = \vec{g}\p{-} + \vec{g}^*\p{+}
\end{align}

In the second line, the absence of a permanent magnetic dipole, the symmetry of states $\ket{1}, \ket{2}$ suggests that $\bra{1}\op{d}\ket{1} = \bra{2}\op{d}\ket{2} \equiv 0$. We note that $\op{d}$ has a direction which is inherited by the complex coupling term, $\vec{g}$ in the third line.

In the Schrodinger picture:
\begin{align}
\op{H}_{AF} &\equiv -\op{d} \cdot \vec{B} \\
\kappa & \equiv \vec{g} \cdot \op{z} = \bra{1} \op{d} \cdot \op{z} \ket{2} \\
\Rightarrow \op{H}_{AF} & = -\kappa \Omega(t)  \cos(\omega_\mu t + \phi(t)) \p{-} - \kappa^* \Omega(t)  \cos(\omega_\mu t + \phi(t)) \p{+}
\end{align} 
 
In the Heisenberg picture, the Heisenberg equations of motion for $\p{\pm}(t)_H \propto e^{\pm i \omega_A t} \p{\pm}(0)$, and we implement the rotating wave approximation by throwing away terms far detuned from our two level system. This is at the same level of accuracy as  implementing a two-level system approximation, and corresponds to throwing away the first line below:

\begin{align}
\op{H}_{AF}^H &= \frac{\Omega\dd}{2} \kappa^* e^{i(\omega_A + \omega_\mu)t}e^{i\phi\dd}\p{+}(0) + h.c. \quad \text{(drop in H-picture)}\\
&+ \frac{\Omega\dd}{2} \kappa^* e^{i(\omega_A - \omega_\mu )t}e^{-i\phi\dd}\p{+}(0) + h.c. \quad \text{(keep in H-picture)} \\
\Rightarrow  \op{H}_{AF}^{RWA, S} &= \frac{\Omega\dd}{2} \kappa^*
 e^{-i( \omega_\mu t + \phi\dd}\p{+} + \frac{\Omega\dd}{2} \kappa e^{i( \omega_\mu t + \phi\dd)}\p{-} \quad \text{(S-picture)}
\end{align}

\subsection{Environmental Dephasing}

Define $\op{H}_E$ for a stochastic detuning, $\delta \omega (t)$ that models an arbirary dephasing process:
\begin{align}
\op{H}_{E}(t) & = -\frac{\hbar}{2}\delta \omega (t) \p{z}
\end{align}
Note that the Hamiltonian commutes with itself at different times and this enables a simple form for the time evolution operator.
\\
\\
% We will further approximate a slow drift, namely, that the timescales for free evolution considered subsequently, $t << t'$,  such that $\delta \omega (t') \equiv \delta \omega_{t'}$, a constant stochastic random variable, and $\op{H}_{E}(t) \equiv \op{H}_{E}$ wtih respect to the longer timescale $t'$.


\subsection{Probability Amplitudes}
Then equations of motion for probability amplitudes of our two state system, given arbitrary $\ket{\psi} = c_1 \ket{1} + c_2\ket{2} $ become:
\begin{align}
\dr{\pjs{1}{\psi}} &\equiv \dot{c_1} = \frac{-i}{\hbar}\bra{1} \op{H}_0 + \op{H}_{E} + \op{H}_{AF}^{RWA}
  \ket{\psi} = \frac{i(\omega_A - \delta\omega(t)) }{2}c_1 - \frac{i}{\hbar}Z_0\dd e^{i\omega_\mu t}c_2\\
\dr{\pjs{2}{\psi}} &\equiv \dot{c_2} = \frac{-i}{\hbar}\bra{2} \op{H}_0 + \op{H}_{E} + \op{H}_{AF}^{RWA}  \ket{\psi} = -\frac{i(\omega_A - \delta\omega(t)) }{2}c_2 - \frac{i}{\hbar}Z_0^*\dd e^{-i\omega_\mu t}c_1\\
\text{with} \quad Z_0\dd &\equiv \frac{\Omega\dd\kappa}{2}e^{i\phi\dd}.
\end{align}
\\
\\
\subsection{First Interaction Picture}

Define transformation as:
\begin{align}
\tilde{c}_1 &= c_1e^{i\lambda t} \\
\tilde{c}_2 &= c_2e^{-i\lambda t}
\end{align}
The equations of motion transform as follows:
\begin{align}
\dot{\tilde{c}}_1 & = \dot{c}_1 e^{i \lambda t} + i \lambda \dot{\tilde{c}}_1 \\
& =  i(\frac{\omega_A - \delta\omega(t)}{2} + \lambda)\tilde{c}_1 - \frac{i}{\hbar}Z_0\dd e^{i(\omega_\mu +2\lambda)t}\tilde{c}_2\\
& \nonumber \\
\dot{\tilde{c}}_2 & = \dot{c}_2 e^{-i \lambda t} - i \lambda \dot{\tilde{c}}_2 \\
& =  -i(\frac{\omega_A - \delta\omega(t)}{2} + \lambda)\tilde{c}_2- \frac{i}{\hbar}Z_0^*\dd e^{-i(\omega_\mu +2\lambda)t}\tilde{c}_1
\end{align}
We set $2\lambda \equiv -\omega_\mu$ and $\Delta \omega(t) \equiv \omega_A - \delta\omega(t) -\omega_\mu$ to remove the time dependence due to the carrier, resulting in:
\begin{align}
\dot{\tilde{c}}_1 & =  \frac{i\Delta \omega(t)}{2}\tilde{c}_1 - \frac{i}{\hbar}Z_0\dd \tilde{c}_2\\
& \nonumber \\
\dot{\tilde{c}}_2 & =  -\frac{i\Delta \omega(t) }{2}\tilde{c}_2- \frac{i}{\hbar}Z_0^*\dd \tilde{c}_1\\
 \Rightarrow \op{H}_{\omega_\mu}^{I} &\equiv \frac{\hbar \Delta \omega(t)}{2}\p{z} + Z_0\dd\p{-} + Z_0^*\dd\p{+}.
\end{align}
In the last line, $\op{H}_{\omega_\mu}^{I}$ is the effective interaction picture Hamiltonian acting on the transformed state $\ket{\psi} = \tilde{c}_1\ket{1} + \tilde{c}_2\ket{2}$ with transformed field amplitudes $Z_0\dd$.
\\
\\
\subsection{Second Interaction Picture}

Define transformation as:
\begin{align}
\alpha_1 &= \tilde{c}_1e^{i\lambda\dd} \\
\alpha_2 &= \tilde{c}_2e^{-i\lambda\dd}
\end{align}
The equations of motion transform as follows:
\begin{align}
\dot{\alpha}_1 & = \dot{\tilde{c}}_1 e^{i \lambda\dd} + i \dot{\lambda}\dd \alpha_1 \\
& =  i(\frac{\Delta \omega(t)}{2} + \dot{\lambda}\dd)\alpha_1 - \frac{i}{\hbar}Z_0\dd e^{i2\lambda\dd}\alpha_2\\
& \nonumber \\
\dot{\alpha}_2 & = \dot{\tilde{c}}_2 e^{-i \lambda\dd} - i \dot{\lambda}\dd\dot{\alpha}_2 \\
& =  -i(\frac{\Delta \omega(t)}{2} + \dot{\lambda}\dd)\alpha_2- \frac{i}{\hbar}Z_0^*\dd e^{-i2\lambda\dd}\alpha_1
\end{align}
We substitute $Z_0\dd$ and set $2\lambda\dd \equiv -\phi_N\dd$  as in \cite{soare}, resulting in:
\begin{align}
\dot{\alpha}_1 & =  i\frac{\Delta \omega(t) - \dot{\phi}\dd}{2}\alpha_1 - \frac{i}{2\hbar}(\Omega\dd \kappa e^{i\phi_C\dd}) \alpha_2\\
& \nonumber \\
\dot{\alpha}_2 & =  -i\frac{\Delta \omega(t) - \dot{\phi}\dd}{2}\alpha_2- \frac{i}{2\hbar}(\Omega\dd \kappa^* e^{-i\phi_C\dd})\alpha_1\\
&\nonumber \\
\Rightarrow \op{H}^{I}_{\omega_\mu,\phi_N\dd} &\equiv \frac{\hbar}{2}(\Delta \omega(t)- \dot{\phi}_N\dd)\p{z} + \frac{\Omega\dd}{2} (\kappa e^{i\phi_C\dd}\p{-} + \kappa^* e^{-i\phi_C\dd}\p{+}) \label{eqn:Hi}
\end{align}
In the last line, $\op{H}^{I}_{\omega_\mu,\phi_N\dd}$ is the effective interaction picture Hamiltonian acting on the transformed state $\ket{\psi} = \alpha_1\ket{1} + \alpha_2\ket{2}$. For the case where we set $\kappa^* = \kappa \equiv 1$ (real coupling constant), we recover the effective interaction picture Hamiltonian in \cite{soare}:

\begin{align}
\op{H}^{I}_{\omega_\mu,\phi_N\dd} & = \frac{\hbar}{2}(\Delta \omega(t)- \dot{\phi}_N\dd)\p{z} + \frac{\Omega\dd}{2} (e^{i\phi_C\dd}\frac{\p{x} - i\p{y}}{2} + e^{-i\phi_C\dd}\frac{\p{x} + i\p{y}}{2})\\
& = \frac{\hbar}{2}(\Delta \omega(t)- \dot{\phi}_N\dd)\p{z} + \frac{\Omega\dd}{4} (\p{x} (e^{i\phi_C\dd}\ +  e^{-i\phi_C\dd})- i\p{y} ( e^{i\phi_C\dd} - e^{-i\phi_C\dd} ))\\
& = \frac{\hbar}{2}(\Delta \omega(t)- \dot{\phi}_N\dd)\p{z} + \frac{\Omega\dd}{2} (\p{x} \cos(\phi_C\dd) + \p{y} \sin(\phi_C\dd)) \label{eqn:Hi_kappareal}
\end{align}
\\
\\
Without any changes to the approach above, we may also expand $\Delta \omega(t) \equiv \omega_A - \delta \omega(t)-\omega_\mu -\delta_N$, where $\delta_N$, is a perfectly correlated but unknown experimental error in our detuning for any single run. Although it is impossible to transform to this interaction picture (phase noise is unknown), both \ref{eqn:Hi} or \ref{eqn:Hi_kappareal} confirm that our Ramsey measurements encode the derivatives of the phase noise field, and that measurements can also encode perfectly correlated but unknown detuning errors.
\\
\\
In Appendix \ref{sec:ap_randomprocess}, we justify we can learn noise correlations for an appropriate class of stochastic processes ($\phi_N\dd$); however, resolving unknown but perfectly correlated detuning errors ($\delta_N$) will be beyond the scope of our techniques. To this end, we set $\omega_A - \omega_\mu - \delta_N = 0$.
\\
\\
\section{Liska Kalman Predictor: Choosing a Fixed Basis} \label{sec:ap_liska_fixedbasis}

The efficacy of the Liska Kalman Filter in our application assumes an appropriate choice of the `Kalman Basis'. We will define our computational basis in the Kalman Filter as:
\begin{align}
\Delta \omega_B  &\equiv \text{Kalman computational basis spacing} \\
\omega_k^B &\equiv \text{$k$-th Kalman basis frequency} \\
&= 2 \pi k f_0^B, \quad k = 0, 1, ..., k_{max};  \quad \omega_B^{MAX} = k_{max} f_0^B < B \\
B  &\equiv \text{Bandwidth assumption, $B$,  in Appendix \ref{azm:PSDbandlimit} } \\
f_0^B &=  \frac{\Delta \omega_B}{2 \pi}
\end{align}

\subsection{Types of Fixed Basis}
We define Basis A - C as below. The  difference between Basis A and Basis B is designed to test whether we should be allowed to project on anything smaller than the Fourier resolution in discrete time. Basis C is identical to Basis B but allows a projection to zero frequency. The truncation at $\omega_B^{MAX}$ is not exact, and may truncate earlier depending on the stepsize multiples.
\begin{align}
\text{Basis A: } &\equiv \{\omega_B\} \equiv \{0, \Delta \omega_B, 2\Delta \omega_B \dots  \omega_B^{MAX}\} \\
\text{Basis B: } &\equiv \{\omega_B\} \equiv \{ \Delta s, \Delta s + \Delta \omega_B \dots  \omega_B^{MAX}\} \\
\text{Basis C: } &\equiv \{\omega_B\} \equiv \{ 0, \Delta s, \Delta s + \Delta \omega_B \dots  \omega_B^{MAX}\} 
\end{align}
Of these, it is seen numerically that Basis A allows better predictions that B or C.

\subsection{Alternative Prediction Method}

For $N_{predict} \neq 0$, there are two ways to conduct Kalman predictions after data collection has stopped. The standard procedure is to set the Kalman gain $\gamma \equiv 0$ for all iterations into future time-steps. 
\\
\\
Alternatively, we may extract instantaneous amplitudes and instantaneous phase information from states, and reconstruct the signal for all future time values. This means we can create Kalman predictions outside the zone of measurement data in a single computation for all prediction steps without having to propagate the filter recursively.
\\
\\
We detail the second of these methods. At any point, $n_C$, we can extract instantaneous amplitudes and instantaneous phase information for the $k^{th}$ basis frequency, as follows:
\begin{align}
\hat{a}_{n_C}^k & \equiv \sqrt{\hat{x}_{n_C}^k[0]^2 + \hat{x}_{n_C}^k[1]^2} \label{eqn:sec:ap_liska_fixedbasis_KF_instantA} \\
\hat{\phi}_{n_C}^k & \equiv \arctan \frac{ \hat{x}_{n_C}^k[1]}{ \hat{x}_{n_C}^k[0]} \label{eqn:sec:ap_liska_fixedbasis_KF_instantP}
\end{align}
The predicted signal, $\hat{s}_n$, requires an additional (but time-constant) phase correction term $\psi_C$ that arises as a byproduct of the computational basis (i.e. Basis A, B or C):
\begin{align}
\hat{s}_n &= \sum_j \hat{a}_{n_C}^k \cos(n^*\Delta t \omega_j + \hat{\phi}_{n_C}^k + \psi_C), \quad  N_{train} < n^* < N\\
\psi_C & \equiv \begin{cases}
0,  \quad \text{(Basis A)} \\
\equiv 2\pi \frac{\Delta \omega_B - \Delta s}{\Delta \omega_B }, \quad \text{(Basis B or C)} \\
\end{cases}
\end{align}
The procedure above yields identical predictions to setting the Kalman gain equal to zero, and propagaing forwards. The phase correction term corrects for a gradual mis-alignment between Fourier and computational grids which occurs if one specifies a non-regular spacing inherent in Basis B or C. 

\subsection{Optimal Training Time}
The optimal training time to begin predictions depends on (a) the choice of experimental sampling rates and (b) the computational basis for the Liska Kalman filter. Below we justify an analytical ratio to define the optimal training time, $n_c^*$. 
\begin{align}
n_C  &\equiv \text{Time steps at which instantaneous $\hat{a}^k, \hat{\phi}^k$ are extracted; or the Kalman gain is set to zero} \\
n_C^* &\equiv \frac{1}{\Delta t \Delta \omega_B} = \frac{f_s}{\Delta \omega_B} \label{eqn:sec:ap_liska_fixedbasis_nC}
\end{align}
Consider $n_C^* = \frac{f_s}{\Delta \omega_B}$.  For $f_s$ fixed, our choice of $n_C > n_C^* $ means we are achieving a Fourier resolution which exceeds the resolution of the Kalman basis. Now consider $n_C < n_C^*$. This means that we've extracted information prematurely, and we have not waited long enough to project on the smallest basis frequency, namely, $\Delta \omega_B = 2 \pi f_0^B$. 
\\
\\
In the case where data is perfectly projected on our basis, this has no impact. For imperfect learning, we see that instantaneous amplitude and phase information slowly degrades for $n_C > n_C^*$, and trajectories for the smallest basis frequency have not stablised for $n_C < n_C^*$.

\begin{figure}[h!]
	\centering
	\caption[Forecasting: Optimal time to extracting Kalman estimates during training]{Learned amplitude trajectories v. time for Perfect and Imperfect learning scenarios. At $ t^* = \Delta t n_C^* \equiv 2.0$, amplitude trajectories (dots) should be non-zero, corressponding to true frequencies in the underlying signal. All other trajectories  (dotted lines) should be zero. Fig. \ref{fig:sinusoid_KF_extraction_perfect} shows that for $n > n_C^*$, learned amplitudes are stable when Perfect Learning is possible. In Fig. \ref{fig:sinusoid_KF_extraction_imperfect}, we illustrate that learned amplitude information is gradually lost in the Imperfect Learning case. The loss of information is severe for realistic contexts (not depicted).}
%	\begin{subfigure}[h!]{0.4\textwidth}
		\includegraphics[width=0.4\textwidth]{final_sinusoid_KF_extraction_perfect.png}
		\caption{Perfect learning} \label{fig:sinusoid_KF_extraction_perfect}
%	\end{subfigure}
%	\begin{subfigure}[h!]{0.4\textwidth}
		\includegraphics[width=0.4\textwidth]{final_sinusoid_KF_extraction_imperfect.png}
		\caption{Imperfect learning} \label{fig:sinusoid_KF_extraction_imperfect}
%	\end{subfigure}
\end{figure}
\FloatBarrier
At $n_C^* $, we optimally extract amplitude and phase information before the structure of the filter begins to systematicaly erode learned state estimates.
\\
\\