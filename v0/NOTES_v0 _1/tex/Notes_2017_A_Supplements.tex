% ##############################################################################
\section{Stochastic Qubit Dynamics under Environmental Dephasing \label{sec:app:setup}}
% ##############################################################################
Environmental dephasing manifests as a time dependent stochastic detuning which couples to a $\p{z}$ term in the Hamiltonian, $\op{H}_{N} (t)$, governing qubit dynamics. Consider the state on the equator of the Bloch sphere evolving under $\op{H}_{N} (t)$ during a Ramsey experiment for duration $\tau$. CHECK SIGN AND UP AND DOWN STATEMENTS
\begin{align} 
\op{H}_{N}(t) & \equiv \frac{\hbar}{2}\delta\omega(t)\p{z}
\end{align}
Initially, the qubit state is prepared on the equator of the Bloch sphere. The qubit evolves freely for $\tau$ time and subsequently, the qubit state is rotated before a projective measurement is performed with respect to the $\p{z}$ axis. In the semiclassical approximation, this Hamiltonian commutes with itself at different $t$, and hence we can write the time evolution operator during the Ramsey wait time as:
\begin{align}
\op{U}(t, t + \tau) & = e^{-\frac{i}{\hbar} \state(t, t + \tau)} \\
\state(t, t + \tau) & \equiv \int_{t}^{t + \tau} \op{H}_{N} (t') dt' \label{eqn:app:phases}
\end{align}
Consider that the initial $\pi/2$ control pulse perfectly delivers the state vector to the equatorial plane of the Bloch sphere, for example, the  $\ket{\p{x} +}$ state and is allowed to evolve under $\op{U}(t, t + \tau)$. Then, the probability of measuring in $\ket{\p{x} \pm}$ yielding a single shot measurement, $d \in \{0, 1\}$, is:
\begin{align} 
|\bra{\p{x} \pm} \op{U}(t, t + \tau) \ket{\p{x} +}|^2= \begin{cases} \cos(\frac{\state(t, \tau)}{2})^2 \quad \text{for $ \ket{\p{x} +}$} \\   \sin(\frac{\state(t, \tau)}{2})^2  \quad \text{for $ \ket{\p{x} -}$} \end{cases} \label{eqn:app:likelihood}
\end{align}
The second $\pi/2$ control pulse rotates the state vector such that a measurement in $\p{z}$ basis is possible, and the probabilities correspond to $\ket{\p{z} \pm}$ respectively. Hence, \cref{eqn:app:likelihood} defines the likelihood for single shot qubit measurement in \cref{eqn:main:likelihood} and defines the non linear measurement action on phase noise jitter, $\state(t, \tau)$. 
We impose a condition that $\state(t, t + \tau) < \pi$ (OR 2PI) such that accumulated phase over $\tau$ can be inferred from a projective measurement on the $\p{z}$ axis. 

% ##############################################################################
\section{Experimentally Controlled Discretisation of Dephasing Noise \label{sec:app:exptres}} 
% ##############################################################################
 In this section, we consider a sequence of Ramsey measurements. At time $t$, \cref{sec:app:setup} describes the qubit measurement likelihood at one instant under dephasing noise. We assume that the dephasing noise is slowly drifting with respect to a fast measurement action on timescales of order $\tau$. In this regime, \cref{eqn:app:phases} discretises the continuous time process $\delta\omega(t)$, at time $t$, for a number of $n= 0, 1, ..., N$ equally spaced measurements with $t = n \Delta t$. Performing the integral for $\tau \ll \Delta t$ and slowly drifting noise such that, $\delta\bar{\omega}_n \equiv \delta\omega(t')|_{t'=n \Delta t }$ in \cref{eqn:app:phases}:
\begin{align}
\state(n\Delta t, n\Delta t + \tau) & = \frac{\hbar}{2}\p{z}\delta\bar{\omega}_n \tau \label{eqn:app:phases_constantdetuning}
\end{align}
In this notation, $\delta\bar{\omega}_n $ is a random variable realised at time, $t = n \Delta t$, and it remains constant over short duration of the measurement action, $\tau$.  We use the shorthand $\state_n \equiv \state(n\Delta t, n\Delta t + \tau)$ to label a sequence of stochastic, temporally correlated qubit phases $ \state \equiv \{\state_n \}$. 
 \\
 \\
 The discretisation of $\delta\omega(t)$ into  a discrete time stochastic phase sequence, $\state$, governs the physical Fourier resolution at which dephasing noise is sampled. If we take a sequence, $\state$, to be $N$ samples long, then we define the resulting physical Fourier domain resolution :
 \begin{align}
 \Delta f_{EXPT} & \equiv \frac{1}{\Delta t N} \\
 N & \equiv N_T + N_P
 \end{align} By specifying  $\Delta t$, training points, $ N_T$, and forward prediction time steps, $ N_P$, an experiment fully defines the physical Fourier resolution. In particular, an implicit assumption about the bandwidth of dephasing noise, $f_B$ is captured in $\Delta t$, namely:
\begin{align}
\Delta t \equiv \frac{1}{f_s} \equiv \frac{1}{r_{Nqy}f_B}
 \end{align}
In numerical simulations, we choose  Nyquist multipler $r_{Nqy} \gg 2$ and fix $f_B, \Delta t, N_T, N_P$. For true noise engineering, we compare true noise spacing $\omega_0/ 2\pi$ relative to the physical resolution of the entire experimental run, i.e. $\Delta f_{EXPT}$. Further, if the oversampling regime for any algorithm is varied, it can interpreted as relaxing $f_B$ assumption or equivalently, reducing $r_{Nqy}$, however, we ensure there is no physical aliasing and $r_{Nqy}>2$ in all cases.
\\
\\
The computational resolution in algorithms has a natural interpretation such that  $\omega_0^B / 2\pi \geq \frac{1}{\Delta t N_T} > \Delta f_{EXPT}$. This is necessarily the case since an algorithm ceases to receive measurement data but we continue to `sample' true noise to confirm accuracy of predictions in our study. Numerically, this is significant in optimising spacing between adjacent oscillators for  LKFFB and GPR algorithms and linking their performance to a physical interpretation of sampling rates.  

% ##############################################################################
\section{True Dephasing Noise Engineering \label{sec:app:truenoise}} 
% ##############################################################################
In the absence of an apriori model for describing qubit dynamics under dephasing noise, we impose the following properties on $\state$ such that we can design meaningful predictors of qubit state dynamics. We assert that a stochastic process, $\state$, indexed by a set of values $\{ n : n = 0, 1, \hdots N \}$ satisfies: 
\begin{align}
\ex{\state_n} &= \mu \quad \forall n \label{eqn:app:f_mean} \\
\ex{\state_n^2} & < \infty \quad \forall n \label{eqn:app:f_var} \\
\ex{(\state_n - \mu)(\state_m - \mu)} &= R(v), \quad  v = |n-m|, \quad \forall n, m \in N  \label{eqn:app:f_covar} \\
& \lim_{K \to \infty} \frac{1}{K} \sum_{v=0}^{K-1} R(v) = 0 \nonumber \\
\iff & \lim_{K \to \infty} \ex{(\bar{\state_K} - \mu)^2} = 0 \nonumber \\
\text{for} \quad v &= |n_k - n_j|, \quad \forall k,j \in K, n_k, n_j \in N  \nonumber \\
\text{with} \quad \bar{f_K} &= \frac{1}{K} \sum_{k=0}^K f_{n_k} \quad \text{(sample mean of $\state$)}  \label{eqn:app:f_msergodic}  \\ 
R(v) & \neq \begin{cases} \sigma^2 , v = 0 \\ 0, v\neq 0 \end{cases} \label{eqn:app:f_Markovian}
\end{align}

Covariance stationarity of $\state$ is established by satisfying \cref{eqn:app:f_mean,eqn:app:f_var,eqn:app:f_covar}; and mean square ergodicity  of $\state$ is established by assuming \cref{eqn:app:f_msergodic}. The latter means that a true $R(v)$ associated with $\state$ is bandlimited for sufficiently large (but unknown) $K$. If this were not the case, then no convergence of sample moments to true moments of $\state$ is guaranteed as correlations never `die out'. Hence, our predictors for one realisation of dephasing noise will fail for a different realisation of the same true dephasing. In \cref{eqn:app:f_Markovian}, we insist that $\state$ is non-Markovian, and true $R(v)$ corressponds to a bandlimited power spectral density. If this were not the case, then delta-correlations exist in the time domain and no prediction is meaningful. With a bandlimited power spectral density, we additionally assume that the measurement action is sufficiently fast such that the experimental sampling rate $f_s = rf_B, \quad r_{Nyq} \gg 2$ in \cref{sec:app:exptres}.

For the purposes of experimental noise engineering, we satisfy the assumptions above by engineering discretised process, $\state$, as:
\begin{align}
\state_n &= \alpha \omega_0 \sum_{j=1}^{J} j F(j)\cos(\omega_j n \Delta t + \psi_j) \label{eqn:app:noiseengineering} \\
F(j) & = j^{\frac{p}{2}-1} 
\end{align}

Using the notation of \cite{soare2014}, $\alpha$ is an arbitrary scaling factor, $\omega_0$ is the fundamental spacing between true adjacent discrete frequencies, such that $\omega_j = 2 \pi f_0 j =\omega_0 j, j = 1, 2, ...J$. For each frequency component, there exists a uniformly distributed random phase, $\psi_j \in [0, \pi]$. The free parameter $p$ allows one to specify an arbitrary shape of the true power spectral density of $\state$. In particular, the free parameters $\alpha, J, \omega_0, p$ are true dephasing noise parameters which any prediction algorithm cannot know beforehand.

It is straightforward to show that $\state$ is covariance stationary. To show mean square ergodicity of $\state$, one requires phases are randomly uniformly distributed over one cycle for each harmonic component of $\state$ \cite{gelb1974applied}. Next, one shows that an ensemble average and a long time average of multi-component engineered $\state$ are equal. In both cases, we obtain:
\begin{align}
R(v) = \begin{cases} \sum_j^J \frac{A_j^2}{2} \cos(\omega_j \Delta t v), j = j' \\ 0, j \neq j'
\end{cases}
% &=  \ex{\sum_j^J x_{j,n} \sum_{j'}^{J}x_{j', m}}  \\
% & \equiv \sum_j^J \sum_{j'}^{J} \lim_{N \to \infty} \frac{1}{2 \Delta t N} \int_{-\Delta t N}^{\Delta t N}  x_{j,n} x_{j', m} dn \\
% \state_n & = \sum_j^J x_{j,n} \\
% x_{j,n} & \equiv A_j\cos(\omega_j \Delta t n + \psi_j ) \\
%  A_j  & \equiv \alpha \omega_0 j F(j) 
\end{align} For the evaluation of the long time average, we use product-to-sum formulae and observe that the case $j\neq j'$ has a zero contribution as any finite contribution from cosine terms over a symmetric integral are reduced to zero as $N \rightarrow \infty $.  For $j = j'$, only a single cosine term survives. The surviving term depends on $v$ and not $n$ - yielding a cancellation of $N$ and a finite contribution that matches the ensemble average.

We briefly comment that $\state$ is Gaussian by the central limit theorum in the regimes considered in this manuscript. The probability density function of a sum of random variables is a convolution of the individual probability density functions. The central limit theorum grants that each element of $\state_n$ at $n$ appears Gaussian distributed for large $J$, irrespective of the underlying properties of $x_{j,n}$, or the distribution of the phases $\psi$. Numerical analysis shows that $J > 15$ results in $\state_n$ appearing approximately Gaussian distributed. 


% ##############################################################################
\section{Theoretic Representation of Covariance Stationary Processes $\state$ \label{sec:app:predictors}}
% ##############################################################################
We consider two theoretic frameworks to represent any covariance stationary processes. The first is motivated by Wold's decomposition and gives rise to autogressive high order $p$ approaches in this paper. The second is given by the spectral decomposition theorum, and motivates the use of oscillator approaches in this paper. 

\subsection{Autoregressive (AR($q$)) and Moving Averages (MA($p$)) Processes}
We justify that any covariance stationary $\state$ can be approximated by a high order autoregressive process.  


First, we expand on the properties of the lag operator $\mathcal{L}$. This operator defines a map between time series sequences and enables a compact description of ARMA processes. Consider an infinite time series $\{ f_n \}_{n = -\infty}^{\infty}$ and a constant scalar, $c$, such that the lag operator, $\mathcal{L}$ is defined by the following properties:
\begin{align}
\mathcal{L} f_n & = f_{n-1} \\
\mathcal{L}^q f_n & = f_{n-q} \\
\mathcal{L}(cf_n) & = c\mathcal{L}f_n = cf_{n-1}  \\
\mathcal{L}f_n & = c, \quad \forall n, \implies \mathcal{L}^q f_n  = c
\end{align}

Next, we define a Gaussian white noise sequence, $\xi$, under the strong condition that \cref{eqn:app:ARMA:xi_indep} that $\xi_n, \xi_m$ are independent $\forall n, m $:
\begin{align}
\ex{\xi} \equiv 0  \\
\ex{\xi_n \xi_m} \equiv \sigma^2 \delta(n-m)\label{eqn:app:ARMA:xi_indep}   
\end{align}

With these definitions, we can define an autoregressive process and a moving average process of unity order.  \cref{eqn:app:ARMA:AR_1} defines an AR($q=1$) process and dynamics of $\state_n$ are given as lagged values of the variable $\state$. The second definition in \cref{eqn:app:ARMA:MA_1} depicts a MA($p = 1$) process where dynamics are given by lagged values of Gaussian white noise $\xi$. 
\begin{align}
(1 - \phi_1 \mathcal{L}) f_n  & = c + \xi_n  \label{eqn:app:ARMA:AR_1} \\
f_n & = c' + (\Psi_1 \mathcal{L} + 1)\xi_n  \label{eqn:app:ARMA:MA_1} 
\end{align}
Here, $\Psi_1, \phi_1$ are known scalars defining dynamics of $\state_n$; $w_n$ is a white noise Gaussian process, and $c, c'$ are fixed scalars. 

It is well known that an MA($\infty$) representation is equivalently an AR($1$) process, and the reverse relationship also applies. For example, we can re-write \cref{eqn:app:ARMA:AR_1} as:
\begin{align}
f_n & = c + \xi_n + \phi_1 f_{n-1} \\
& = w_n + \phi_1 f_{n-1} \\
& = w_n + \phi_1 (w_{n-1}+ \phi_1 f_{n-2} ) \\
& \vdots \\
& = \phi_1^{n+1} F_0 + \phi_1^{n} w_{0} + \phi_1^{n-1} w_{1} + \hdots w_n \\
& = \phi_1^{n+1} F_0 + \phi_1^{n} (c + \xi_{0}) + \hdots + (c + \xi_{n}) \\
& = \phi_1^{n+1} F_0 +  c (\phi_1^{n} + \phi_1^{n-1} + \hdots + 1) + \sum_{m=0}^{n} \phi_1^m \xi_{n-m} \\
w_n & \equiv c + \xi_n \\
F_0 & \equiv f_{n=-1} 
\end{align} We restrict $|\phi_1| < 1$ such that $\state$ is covariance stationary \cite{hamilton1994time}. % An MA process of any order is covariance stationary for any choice of coefficients - instead, the restrictions on the MA representation arise in the form of invertibility of an MA process \cite{hamilton1994time}.
Under these conditions, we take the limit of $f$ capturing an infinite past, or namely, as $n$ indexes an infinite number of terms. The initial state $F_0$ is eventually forgotten, $\phi_1^{n+1} F_0 \approx 0$ if $n$ is large and $|\phi_1| < 1$. Similarly, the terms $c (\phi_1^{n} + \phi_1^{n-1} + \hdots + 1)$  can be summarised as a geometric series in $\phi_1$. The remaining terms satisfy the definition of an MA($\infty$) process:
\begin{align}
f_n &= c \frac{1}{1 - |\phi_1|}  +  \sum_{m=0}^{\infty} \phi_1^m \xi_{n-m}, \quad |\phi_1| < 1
% & = c' + \sum_{m=0}^{\infty} \Psi_m \xi_{n-m}\\
% \Psi_m  & \equiv \Phi^m, \quad \sum_{m=0}^{\infty}  |\Psi_m| = \sum_{m=0}^{\infty}  |\phi|^m < \infty 
\end{align}
It is straightforward to show that the reverse is true, namely, an MR($1$) is equivalent to an AR($\infty$) representation \cite{hamilton1994time}.

The consideration of an MA($\infty$) process leads us directly to Wold's decomposition for arbitrary covariance stationary processes, namely, that any covariance stationary $\state$ can be represented as:
\begin{align}
\state_n & \equiv  \tilde{f}_n + \sum_{k=0}^{\infty} \Psi_k \xi_{n-k}   \\
& =  \tilde{f}_n + \sum_{k=0}^{\infty} \Psi_k \mathcal{L}^k \xi_{n}  \label{eqn:app:ARMA:MAinf}\\
\tilde{f}_n & \equiv \ex{\state_n | \state_{n-1}, \state_{n-2}, \hdots} \\
% \xi_n & \equiv   \state_n - \ex{\state_n | \state_{n-1}, \state_{n-2}, \hdots} \\
\Psi_0 & \equiv 1 \\
\sum_{k=0}^{\infty} \Psi_k^2 & < \infty
% & = \sum_{k=0}^{\infty} \Psi_k \xi_{n-k} 
\end{align}
\cref{eqn:app:ARMA:MAinf} defines an MA($\infty$) process derived previously as an AR($1$) process. This process is ergodic for Gaussian $\xi$. However, such a representation of $\state$ requires fitting data to an infinite number of parameters $\{\Psi_1, \Psi_2, \hdots \}$  and approximations must be made. 

% A standard approximation is to leverage the dual behaviour of MA($p$) and AR($q$) process and consider finite order polynomials in an ARMA($q,p$) model with MA coefficients given by $\theta_{p \leq p'}$ and AR coefficients given by $\phi_{p \leq p'}$:
% \begin{align}
% \sum_{k=0}^{\infty} \Psi_k \xi_{n-k} & \to \frac{ 1 + \theta_{1}\mathcal{L}^{1} + \hdots + \theta_{p}\mathcal{L}^{p}}{1 - \phi_{1}\mathcal{L}^{1} - \hdots - \phi_{1}\mathcal{L}^{q}}\\
% & \quad |\phi_i|  < 1, i = 1, \hdots, q
% \end{align}
% Instead of using an infinite past or an ARMA approximation,

We approximate an arbitrary covariance stationary $\state$ using finite but high order AR($q$) processes. Below we show that any finite order AR($q$) process has an MA($\infty$) representation satisfying Wold's theorum.

We define an arbitrary AR($q$) process as:
\begin{align}
\xi_n & \equiv (1 - \phi_1 \mathcal{L}  - \phi_2 \mathcal{L} ^2 - \hdots -\phi_q \mathcal{L} ^q) (\state_n - c)\\
& = (1 - \lambda_1 \mathcal{L}) \hdots (1 - \lambda_q \mathcal{L}) (\state_n - c) \label{eqn:app:ARMA:ar_p_1}
% \state_n - c & \equiv \frac{1}{(1 - \lambda_1 \mathcal{L}) \hdots (1 - \lambda_q \mathcal{L})}
\end{align}
In particular, we use the following result from \cite{hamilton1994time} to define $\lambda_i, i = 1, \hdots, q$ as eiqenvalues of the dynamical model, $\Phi$:
\begin{align}
\Phi &\equiv \begin{bmatrix} \phi_1 & \phi_2 & \phi_3 & \hdots & \phi_{q-1}  &\phi_q \\
1 & 0 & 0 & \hdots & 0 & 0 \\
0 & 1 & 0 & \hdots & 0 & 0 \\
0 & 0 & 1 & \hdots & 0 & 0 \\
\vdots & \vdots & \vdots & \hdots & \vdots & \vdots \\
0 & 0 & 0 & \hdots & 1 & 0 \\
 \end{bmatrix} \\
\bf{\lambda} & \equiv \begin{bmatrix} \lambda_1 \dots \lambda_q \end{bmatrix} \quad \text{s.t.} |\Phi - \bf{\lambda}\mathcal{I}_q|  = 0 \\
\implies 1 & - \phi_1 z  - \phi_2 z^2 - \hdots -\phi_q z^q \\
&\equiv (1 - \lambda_1z) \hdots (1 - \lambda_q z) 
\end{align}

In order to solve the inverse problem above, we consider an arbitrary $q$-th eigenvalue term in  process and we multiply by the operator $\Lambda_q(\mathcal{L}) $ defined below:

\begin{align}
\Lambda_q(\mathcal{L}) & \equiv \lim_{k\to \infty} (1 + \lambda_q \mathcal{L} + \hdots + \lambda_q^k\mathcal{L})\\
\Lambda_q(\mathcal{L}) \xi_n & = \Lambda_q(L) (1 - \lambda_q \mathcal{L}) (\state_n - c) \\
 & = \lim_{k\to \infty}(1 + \lambda_q^{k+1}\mathcal{L}^{k+1})(\state_n - c)
\end{align}
The residual term $\lambda_q^{k+1}\mathcal{L}^{k+1}\state_n \to 0 $ if $|\lambda_q| < 1$  for large $k$, and this case $\Lambda_q(\mathcal{L})$ defines the inverse $(1 - \lambda_q \mathcal{L})^{-1}$. This procedure is repeated for all $q$ eigenvalues to invert \cref{eqn:app:ARMA:ar_p_1} and subsequently perform a partial fraction expansion as follows:
\begin{align}
\state_n - c & = \frac{1}{(1 - \lambda_1 \mathcal{L}) \hdots (1 - \lambda_q \mathcal{L})} \xi_n\\
& = \sum_{q'=1}^{q}\frac{a_{q'}}{1- \lambda_{q'} \mathcal{L}} \xi_n \\
a_{q'} & \equiv \frac{\lambda_{q'}^{q-1}}{\prod_{q''=1, q''\neq q'}^{q} (\lambda_{q'} - \lambda_{q''})}
\end{align} The coefficients are $a_{q'}$ as obtained via the partial fraction expansion method during which $\mathcal{L}$ is treated as an ordinary polynomial. At present, we have a represent $\state$ via a finite $q$ weighted average of values of $\xi$. However, in substituting the definition of $ \Lambda_{q'} \equiv (1- \lambda_{q'} \mathcal{L})^{-1}$, we recover the form of an MA representation (setting $c \equiv \tilde{\state}_n  = 0, \quad  \forall n$ for simplicity): 
\begin{align}
\state_n & = \left[ \sum_{q'=1}^{q} a_{q'} \mathcal{L}^0 +  \lim_{k \to \infty}  \sum_{k'=1}^{k} \left( \sum_{q'=1}^{q} a_{q'}  \lambda_{q'}^{k'} \right) \mathcal{L}^{k'}\right] \xi_n \\
& = \Psi_0 + \sum_{k=1}^{\infty} \Psi_k \mathcal{L}^k \xi_{n}  \\
\Psi_0 & \equiv \sum_{q'=1}^{q} a_{q'} \mathcal{L}^0  \\
\Psi_k & \equiv \sum_{q'=1}^{q} a_{q'}  \lambda_{q'}^{k'}
\end{align}
By examining the properties of $\Phi$ raised to arbitrary powers, it can be shown that $\sum_{q'=1}^{q} a_{q'} \equiv 1$ and $\Psi_k$ is the first element of $\Phi$ raised to the $k$-the power \cite{hamilton1994time}, yielding absolute summability of $\Psi_k$ if $|\phi_{q'<q}| < 1$. This ensures that Wold's theorum is fully satisfied and an AR($p$) process has an MA($\infty$) representation. 
% In moving to an arbitrarily high $q$, we enable the approximation of any covariance stationary $\state$.


The proofs that high $q$ AR approximations for covariance stationary $f$ improve with $q$ for example, in \cite{wahlberg1989estimation}. The key correspondence is that the number of finite lag terms $q$ in an AR($q)$) model contribute to the first $q$ values of the covariance function. This approximation improves with $q$ even if $\state$ is not a true AR process \cite{wahlberg1989estimation,west1996bayesian}. Asymptotically efficient coefficient estimates for any $MA(\infty)$ representation of $\state$ are obtained by letting the order of a purely AR($q$) process tend to infinity and increasing total data size, $N$ \cite{wahlberg1989estimation}. In our application with finite data $N$, we increase $q$ to settle on a high order AR model for tracking arbitrary covariance stationary power spectral densities \cite{brockwell1996introduction}.  When data is fixed at $N$, we expect a high $q$ model to gradually saturate in predictive estimation performance and this was confirmed numerically for model selection in LSF. One can arbitrarily increase performance by increasing both $q, N$ \cite{wahlberg1989estimation}. In frequency domain language, this would equate to increased physical Fourier resolution for learning the power spectral density of $\state$. For our application, we use LSF to construct a high $q$ AR model for Kalman Filtering.  A high $q$ AR model is often the first step for developing models with smaller number of parameters, for example, considering a mixture of finite order AR($q$) and MA($p$) models and estimating $p+q$ number of coefficients using a range of standard protocols \cite{brockwell1996introduction,west1996bayesian}. We pursue a high order AR($q$) model and efficient ARMA estimators can be developed on the basis of our initial model. 


\subsection{Spectral Decomposition}

% ##############################################################################
\section{Liska Kalman Filter: Predictions via a Fixed Basis} \label{sec:ap_liska_fixedbasis}
% ##############################################################################
The efficacy of the Liska Kalman Filter in our application assumes an appropriate choice of the `Kalman Basis'. Of  Basis A - C defined below, it is seen numerically that Basis A allows better predictions that B or C. The  difference between Basis A and Basis B is designed to test whether we should be allowed to project on anything smaller than the Fourier resolution in discrete time. Basis C is identical to Basis B but allows a projection to zero frequency. For Kalman basis spacing, $\omega_0^B$, with $J^B$ total basis oscillators, such that the computational basis assumes a particular true noise bandwidth, $J^B \omega_0^B \quad \leq f_B$, we define:
\begin{align}
\Delta s & \equiv 2\pi\Delta f_{EXPT} \\
\text{Basis A: } & \equiv \{0, \omega_0^B, 2\omega_0^B \dots  J^B \omega_0^B \} \\
\text{Basis B: } & \equiv \{ \Delta s, \Delta s + \omega_0^B \dots J^B \omega_0^B \} \\
\text{Basis C: } & \equiv \{ 0, \Delta s, \Delta s + \omega_0^B \dots  J^B \omega_0^B \} 
\end{align}

In LKFFB, we can extract instantaneous amplitudes and instantaneous phase information for the $j$-th  oscillator using the $x^j_n$ Kalman sub-state estimate. We sum constributions over all $j\in J^B$ oscillators and we reconstruct the signal for all future time values in one calculation, without having to propagate the filter recursively with zero gain. Let $n_C$ denote the time-step at which instantaneous amplitudes $\norm{\hat{x}^j_{n_C}}$ and instantaneous phase $\theta_{\hat{x}^j_{n_C}}$ is extracted for the $j$-th oscillator.

The predicted signal, $\hat{s}_n$, requires an additional (but time-constant) phase correction term $\psi_C$ that arises as a byproduct of the computational basis (i.e. Basis A, B or C):
\begin{align}
\hat{f}^* &= \sum_j\norm{\hat{x}^j_{n_C}} \cos(n^*\Delta t \omega_j + \theta_{\hat{x}^j_{n_C}} + \psi_C), \\
& \quad  n, n_C \in |N_T|, \quad n^* \in N_P \nonumber \\
\psi_C & \equiv \begin{cases}
0,  \quad \text{(Basis A)} \\
\equiv 2\pi \frac{\omega_0^B - \Delta s}{\omega_0^B }, \quad \text{(Basis B or C)} \\
\end{cases}
\end{align}
The phase correction term corrects for a gradual mis-alignment between Fourier and computational grids which occurs if one specifies a non-regular spacing inherent in Basis B or C. Next, we justify an analytical ratio to define the optimal training time, $n_c^*$, at which Kalman predictions should commence. 
\begin{align}
n_C  &\equiv |N_T| \\
n_C^* &\equiv \frac{1}{\Delta t \omega_0^B} = \frac{f_s}{\omega_0^B} \label{eqn:sec:ap_liska_fixedbasis_nC}
\end{align}
Consider $n_C^* = \frac{f_s}{\omega_0^B}$.  For $f_s$ fixed, our choice of $n_C > n_C^* $ means we are achieving a Fourier resolution which exceeds the resolution of the Kalman basis. Now consider $n_C < n_C^*$. This means that we've extracted information prematurely, and we have not waited long enough to project on the smallest basis frequency, namely, $\omega_0^B$.  In the case where data is perfectly projected on our basis, this has no impact. For imperfect learning, we see that instantaneous amplitude and phase information slowly degrades for $n_C > n_C^*$, and trajectories for the smallest basis frequency have not stablised for $n_C < n_C^*$. 
All results in this manuscript are reported for Basis A with $n_C^* \equiv n_C$.

% ##############################################################################
\section{Experimental Verification Procedure \label{sec:app:exptverfication}}
% ##############################################################################

We show that a stochastic detuning is indistingushable from the derivative of phase noise in a carrier enabling experimental verification of algorithms in this manuscript.
This is considered in \cite{soare2014} and we provide an alternative description that enables direct access to equation of motion for qubit probability amplitudes during the application of engineered carrier phase noise. 

We define system and interaction Hamiltonians for a two level system with energy splitting corresponding to $\omega_A$  interacting with a magnetic field. 
\\
\begin{align}
\op{H_0} &= \frac{1}{2} \hbar \omega_A \p{z} \\
\xi & \equiv \vec{g} \cdot \op{z} = \bra{1} \op{d} \cdot \op{z} \ket{2} \\
\op{H}_{AF} & = -\xi \Omega(t)  \cos(\omega_\mu t + \phi(t)) \p{-} \\
& - \xi^* \Omega(t)  \cos(\omega_\mu t + \phi(t)) \p{+}
\end{align} Here, $\op{H_0}$ is the system Hamiltonian, $\op{d}$ is the dipole operator for a spin half particle in a magnetic field, $\vec{g}$ is a complex system-field coupling term that inherits the direction of the dipole operator, $\xi$ is a complex scalar that captures the coupling strength, and $\op{H}_{AF}$ is the Schoedinger picture atom-field interaction Hamiltonian in the rotating wave approximation.

The carrier with noise is identically defined as in \cite{soare2014}:
\begin{align}
\vec{B} &\equiv \Omega(t) \cos(\omega_\mu t + \phi(t)) \op{z} \\
\phi(t) & \equiv \phi_C(t) + \phi_N(t)
\end{align}
with a real control amplitude $\Omega(t)$, carrier frequency $\omega_\mu$, controlled phase $\phi_C(t)$ and stochastic phase noise $\phi_N(t)$.  It is possible to add amplitude noise to the control, i.e. $\Omega(t) \equiv \Omega_C(t) + \Omega_N(t)$, however we set $\Omega_N(t) =0$ at present. 

Substituting $\op{H}_{N}$ for a stochastic detuning, $\delta \omega (t)$ from \cref{sec:app:setup}, the equations of motion for probability amplitudes of our two state system under dephasing, for arbitrary $\ket{\psi} = c_1 \ket{1} + c_2\ket{2} $ are:
\begin{align}
\dr{\pjs{1}{\psi}} &\equiv \dot{c_1} \\
& = \frac{-i}{\hbar}\bra{1} \op{H}_0 + \op{H}_{N} + \op{H}_{AF}^{RWA}  \ket{\psi} \\
& = \frac{i(\omega_A - \delta\omega(t)) }{2}c_1 - \frac{i}{\hbar}Z_0\dd e^{i\omega_\mu t}c_2\\
\dr{\pjs{2}{\psi}} &\equiv \dot{c_2} \\
& = \frac{-i}{\hbar}\bra{2} \op{H}_0 + \op{H}_{N} + \op{H}_{AF}^{RWA}  \ket{\psi} \\
& = -\frac{i(\omega_A - \delta\omega(t)) }{2}c_2 - \frac{i}{\hbar}Z_0^*\dd e^{-i\omega_\mu t}c_1 \\
\quad Z_0\dd &\equiv \frac{\Omega\dd\xi}{2}e^{i\phi\dd}.
\end{align}

The first interaction picture transformation is with respect to the carrier, namely, we define an arbitrary transformation as:
\begin{align}
\tilde{c}_1 &= c_1e^{i\lambda t} \\
\tilde{c}_2 &= c_2e^{-i\lambda t}
\end{align}
The equations of motion transform as follows:
\begin{align}
\dot{\tilde{c}}_1 & = \dot{c}_1 e^{i \lambda t} + i \lambda \dot{\tilde{c}}_1 \\
% & =  i(\frac{\omega_A - \delta\omega(t)}{2} + \lambda)\tilde{c}_1 - \frac{i}{\hbar}Z_0\dd e^{i(\omega_\mu +2\lambda)t}\tilde{c}_2\\
& \nonumber \\
\dot{\tilde{c}}_2 & = \dot{c}_2 e^{-i \lambda t} - i \lambda \dot{\tilde{c}}_2
% & =  -i(\frac{\omega_A - \delta\omega(t)}{2} + \lambda)\tilde{c}_2- \frac{i}{\hbar}Z_0^*\dd e^{-i(\omega_\mu +2\lambda)t}\tilde{c}_1
\end{align}
We set $2\lambda \equiv -\omega_\mu$ and $\Delta \omega(t) \equiv \omega_A - \delta\omega(t) -\omega_\mu$ to remove the time dependence due to the carrier, resulting in:
\begin{align}
\dot{\tilde{c}}_1 & =  \frac{i\Delta \omega(t)}{2}\tilde{c}_1 - \frac{i}{\hbar}Z_0\dd \tilde{c}_2\\
& \nonumber \\
\dot{\tilde{c}}_2 & =  -\frac{i\Delta \omega(t) }{2}\tilde{c}_2- \frac{i}{\hbar}Z_0^*\dd \tilde{c}_1\\
 \Rightarrow \op{H}_{\omega_\mu}^{I} &\equiv \frac{\hbar \Delta \omega(t)}{2}\p{z} + Z_0\dd\p{-} + Z_0^*\dd\p{+}.
\end{align}
In the last line, $\op{H}_{\omega_\mu}^{I}$ is the effective interaction picture Hamiltonian acting on the transformed state $\ket{\psi} = \tilde{c}_1\ket{1} + \tilde{c}_2\ket{2}$ with transformed field amplitudes $Z_0\dd$.

The second interaction picture transformation is with respect to a classical phase noise field. While only attainable in engineered phase noise demonstrations during experiment, the resulting interaction picture Hamiltonian reveals the indistinguisbaility of engineered phase noise and environmental dephasing, as asserted in \cite{soare2014}. We define an arbitrary transformation with respect to a time-varying classical quantity $\lambda (t)$:
\begin{align}
\alpha_1 &= \tilde{c}_1e^{i\lambda\dd} \\
\alpha_2 &= \tilde{c}_2e^{-i\lambda\dd}
\end{align}
The equations of motion transform as follows:
\begin{align}
\dot{\alpha}_1 & = \dot{\tilde{c}}_1 e^{i \lambda\dd} + i \dot{\lambda}\dd \alpha_1 \nonumber \\
& =  i(\frac{\Delta \omega(t)}{2} + \dot{\lambda}\dd)\alpha_1 - \frac{i}{\hbar}Z_0\dd e^{i2\lambda\dd}\alpha_2 \\
& \nonumber \\
\dot{\alpha}_2 & = \dot{\tilde{c}}_2 e^{-i \lambda\dd} - i \dot{\lambda}\dd\dot{\alpha}_2 \nonumber \\
& =  -i(\frac{\Delta \omega(t)}{2} + \dot{\lambda}\dd)\alpha_2- \frac{i}{\hbar}Z_0^*\dd e^{-i2\lambda\dd}\alpha_1
\end{align}
We substitute $Z_0\dd$ and set $2\lambda\dd \equiv -\phi_N\dd$ resulting in:

%%%%% [SECTION BELOW IS CHANGED FROM ORIGINAL NOTES] 
\begin{align}
\dot{\alpha}_1 & =  i\frac{\Delta \omega(t) - \dot{\phi}_N \dd}{2}\alpha_1 \nonumber \\ %%%% ADDED PHI_N
& - \frac{i}{2\hbar}(\Omega\dd \xi e^{i\phi_C\dd}) \alpha_2  \\
\dot{\alpha}_2 & =  -i\frac{\Delta \omega(t) - \dot{\phi}_N\dd}{2}\alpha_2 \nonumber  \\ %%%% ADDED PHI_N
& - \frac{i}{2\hbar}(\Omega\dd \xi^* e^{-i\phi_C\dd})\alpha_1  \\
\Rightarrow \op{H}^{I}_{\omega_\mu,\phi_N\dd} &\equiv \frac{\hbar}{2}(\Delta \omega(t)- \dot{\phi}_N\dd)\p{z} \nonumber \\
& + \frac{\Omega\dd}{2} (\xi e^{i\phi_C\dd}\p{-} + \xi^* e^{-i\phi_C\dd}\p{+}) \label{eqn:Hi}
\end{align}
In the last line, $\op{H}^{I}_{\omega_\mu,\phi_N\dd}$ is the effective interaction picture Hamiltonian acting on the transformed state $\ket{\psi} = \alpha_1\ket{1} + \alpha_2\ket{2}$. For the case where we set $\xi^* = \xi \equiv 1$ (real coupling constant), we recover the effective interaction picture Hamiltonian in \cite{soare2014}.

% ##############################################################################
\section{Liska Kalman Predictor with Fixed Basis} \label{sec:ap_liska_deriv}
The Liska Kalman Filter has adaptive noise matrix, $Q$. We ensure that the standard classical  Kalman Filter derivations are unchanged by the presence of the proposed $Q$ in \cite{livska2007}. To derive the Kalman algorithm, we adapt the approach outlined in \cite{grewal2001theory} for standard applications. 

To begin, we search for a linear predictor $\hat{x}_n(+)$ defined by the unknown weights, $\lambda_n, \gamma_n$, at each time-step $n$:
\begin{align}
\hat{U} & \equiv \{y_1, y_2,\dots, y_n\} \\
\hat{x}_n(+) & \equiv \lambda_n \hat{x}_n(-) + \gamma_n y_n \label{eqn:KF_predictor}\\
\hat{x}_n(-) & \equiv \Phi_{n-1} \hat{x}_{n-1}(+) \\
(+) &\equiv \text{Aposteriori state estimate at $n$}\\
(-) &\equiv \text{Apriori state estimate at $n$}
\end{align}
To find unknowns $\lambda_n, \gamma_n$, we impose orthogonality of estimator to the set of all known data by invoking the general linear prediction theorum for covariance stationary processes, as per standard textbooks, (e.g. \cite{grewal2001theory,karlin2012first}). This means that the following must be satisfied:
\begin{align}
\ex{(x_n - \hat{x}_n(+))U^T} &= 0 \label{eqn:KF_reln_1}\\
\implies \ex{(x_n - \hat{x}_n(+))y_i^T} &= 0, \quad i = 1,\dots, n-1 \label{eqn:KF_reln_2}\\
\implies \ex{(x_n - \hat{x}_n(+))y_n^T} &= 0 \label{eqn:KF_reln_3}
\end{align}
First, we state the following properties of process and measurement noise are true:
With $n,m$ denoting time indices, the properties of noise processes are:
\begin{align}
\ex{w_n} & = \ex{v_n}= 0 \quad \forall n \label{eqn:KF_stat_noisemean}\\
\ex{w_nw_m^T} &= \sigma^2 \delta(m-n),  \quad \forall n,m \label{eqn:KF_stat_noisepros}\\
\ex{v_n v_m^T}&= R\delta(m-n), \quad  \forall n,m  \label{eqn:KF_stat_noisemsmt} \\
\ex{w_n v_m} &= 0,  \quad \forall n,m \label{eqn:KF_stat_crosscorr}
\end{align} 
The process noise variance, $\sigma^2$ is a known scalar. The measurement noise variance $R$ is a known covariance matrix in general, but will turn out to be a scalar for our choice of measurement action $H$. This means we can interpret \ref{eqn:KF_stat_crosscorr} as product between scalars. Based on noise properties and the state space models defined in the main text, we conclude that the following correlation relations are true:
\begin{align}
\ex{x_{n-1}w_{n-1}} & = 0 \label{eqn:KF_reln_4}\\
\ex{w_{n-1} y_i} &= 0, \quad i = 1,2,\dots n-1 \label{eqn:KF_reln_5}\\
\ex{\frac{x_{n-1}}{\norm{x_{n-1}}}w_{n-1}} & = 0 \label{eqn:KF_reln_6}\\
\ex{x_{n-1} w_{n-1} x_{n-1}^T } & = 0 \label{eqn:KF_reln_7}\\
\ex{\frac{x_{n-1}}{\norm{x_{n-1}}} w_{n-1} x_{n-1}^T } & = 0 \label{eqn:KF_reln_8} \\
\ex{x_{n-1} w_{n-1} y_i } & = 0, \quad i = 1,\dots, n-1 \label{eqn:KF_reln_9}\\
\ex{\frac{x_{n-1}}{\norm{x_{n-1}}} w_{n-1} y_i } & = 0, \quad i = 1,\dots, n-1 \label{eqn:KF_reln_10}
\end{align} We physically interpret these relations by observing that $x_{n-1}$ or $y_i, i = 1,2,\dots n-1$ do not contain a process noise term of the form $w_{n-1}$. At most, the expansion of $x_{n-1}, y_{n-1}$ terms contain $w_{n-2}$. Where terms of the form $w_{n-k}\dots w_{n-2}w_{n-1}$ appear, we invoke \ref{eqn:KF_stat_noisepros} and set these terms to zero. Where terms of the form $\propto w_{n-2}w_{n-2}w_{n-1}$ appear, we invoke uncorrelated process noise and zero mean process noise and set these terms to zero. The $\norm{x_{n-1}}$ term depends on $x_{n-1}$, and we assert that a similar logic holds where physically, a normed state cannot be correlated with a future process noise term. The set of correlation relations thus far imply that:
\begin{align}
\ex{x_n} &= \Phi_{n-1}\ex{x_{n-1}} \label{eqn:KF_reln_11}\\
\ex{(x_n - \apx{n})x_0^T} &= 0 \label{eqn:KF_reln_12}\\
\ex{(x_n - \apx{n})\hat{y}_n(-)^T} &=0 \label{eqn:KF_reln_13} \\
\ex{(x_n - \amx{n})y_i^T} &=0, \quad i = 1,\dots, n-1\label{eqn:KF_reln_14}\\
\ex{\Gamma_n w_n \apx{n}^T} &=0 \label{eqn:KF_reln_16}\\
\ex{\Gamma_n w_n x_n^T} &=0 \label{eqn:KF_reln_17} 
\end{align}
These relations are used to find unknown weights, $\lambda_n$ and $\gamma_n$. In particular, satisfying an orthogonality condition (\cref{eqn:KF_reln_2}) at different time steps yields the former, and satisfying an orthogonality condition at same time steps (\cref{eqn:KF_reln_3}) yields the latter (Kalman gain). 

First, we find $\lambda_n$ using \cref{eqn:KF_stat_noisemsmt,eqn:KF_reln_10,eqn:KF_reln_11,eqn:KF_reln_2}:
\begin{align}
&\ex{(x_n - \apx{n})y_i^T} =0, \quad i = 1,\dots, n-1\label{eqn:KF_reln_15}\\ 
\implies 0 & = \ex{(x_n - \apx{n})y_i^T} \\
% & = \ex{(\Phi_{n-1}x_{n-1}(\idn + \frac{w_{n-1}}{\norm{x_{n-1}}}) - \lambda_n \hat{x}_n(-) - \gamma_n y_n)y_i^T} \\
& = \Phi_{n-1}\ex{x_{n-1}y_i^T} + \Phi_{n-1}\ex{\frac{x_{n-1}w_{n-1}}{\norm{x_{n-1}}}y_i^T} \\
& - \lambda_n \ex{ \hat{x}_n(-)y_i^T} - \gamma_n H_n \ex{ x_n y_i^T}  \nonumber \\
& - \gamma_n \ex{ v_n y_i^T} \nonumber \\
% &= \Phi_{n-1}\ex{x_{n-1}y_i^T} + \Phi_{n-1}\ex{\frac{x_{n-1}w_{n-1}}{\norm{x_{n-1}}}y_i^T} - \lambda_n \ex{ \hat{x}_n(-)y_i^T} - \gamma_n H_n\ex{ x_n y_i^T}, \quad \text{by \ref{eqn:KF_stat_noisemsmt} } \\
% &= \ex{\Phi_{n-1}x_{n-1}y_i^T} - \lambda_n \ex{ \hat{x}_n(-)y_i^T} - \gamma_n H_n\ex{ x_n y_i^T}, \quad \text{by \ref{eqn:KF_reln_10} } \\
% &=\ex{x_{n}y_i^T} - \lambda_n \ex{ \hat{x}_n(-)y_i^T} - \gamma_n H_n\ex{ x_n y_i^T}, \quad \text{by \ref{eqn:KF_reln_11}} \\
&= \ex{x_{n}y_i^T} + \lambda_n \ex{( x_n - \hat{x}_n(-))y_i^T} \\
&- \lambda_n \ex{ x_n y_i^T}  - \gamma_n H_n\ex{ x_n y_i^T} \label{eqn:app:LKFFB_addproxy} \nonumber \\
&= \ex{x_{n}y_i^T} - \lambda_n \ex{ x_n y_i^T} - \gamma_n H_n\ex{ x_n y_i^T}\\
& = \ex{(\idn - \lambda_n  - \gamma_n H_n)x_n y_i^T} \\
&\ex{x_n y_i^T} \neq 0 \implies \lambda_n = \idn  - \gamma_n H_n
\end{align} We add $\pm \lambda_n \ex{ x_n y_i^T}$ in \cref{eqn:app:LKFFB_addproxy} to obtain the final result. 
The discussion so far has not made explicit reference to properties of measurement noise. The calculation of the Kalman gain incorporates information about incoming noisy measurements, and we define:
\begin{align}
e_n & \equiv \amx{n} - x_n,   \label{eqn:app:LKFFB_resid}\\
\amp{n} & \equiv E[e_ne_n^T] \label{eqn:app:LKFFB_P}\\
R_n & \equiv E[v_nv_n^T] \label{eqn:app:LKFFB_R}\\
E[v_n e_n^T]&=0 \label{eqn:app:LKFFB_uncorr_resid}
\end{align}
The Kalman Gain $\gamma_n$ satisfies orthogonality conditions at the same time step:
\begin{align}
0 &= \ex{(x_n - \apx{n})(\hat{y}_n(-) - y_n)^T} \\ 
& = E \left[ (x -\lambda_n \amx{n} - \gamma_n H_n x_n - \gamma_n v_n) \right. \\
& \left. (H_n (\amx{n} - x_{k}) -v_n)^T\right]  \label{eqn:app:LKFFB_apriori_y}\\
% &= E\left[(x_n - \amx{n} + \gamma_n H_n\amx{n} - \gamma_n H_n x_n - \gamma_n v_n)(H_n (\amx{n} - x_{k}) -v_n)^T\right]  \\
% &= E\left[(x_n - \amx{n} + \gamma_n H_n\amx{n} - \gamma_n H_n x_n - \gamma_n v_n)(H_n (\amx{n} - x_{k}) -v_n)^T\right].  \\
&= E\left[(-e_n + \gamma_n H_ne_n - \gamma_n v_n)(H_n e_n -v_n)^T\right]  \\
% &= E\left[(-e_ne_n^T H_n^T+ \gamma_n H_ne_ne_n^T H_n^T - \gamma_n v_ne_n^T H_n^T + e_nv_n^T - \gamma_n H_ne_nv_n^T + \gamma_n v_nv_n^T \right]  \\
&= -\amp{n} H_n^T+ \gamma_n H_n\amp{n}H_n^T + \gamma_n R_n \\
& \implies \gamma_n = \amp{n} H_n^T(H_n\amp{n}H_n^T + R_n)^{-1} \label{eqn:KF_update_gamma}
\end{align} 

We use \cref{eqn:KF_reln_3,eqn:KF_reln_13} to intiate the derivation. We expand terms according to the design of our apriori and aposterori predictors, as well as the measurement action in \cref{eqn:app:LKFFB_apriori_y}. Terms are regrouped such that we substitute the expression for the residuals defined in \cref{eqn:app:LKFFB_resid}, enabling a straightforward simplification using (\cref{eqn:app:LKFFB_resid,eqn:app:LKFFB_P,eqn:app:LKFFB_R,eqn:app:LKFFB_uncorr_resid}) to get to the final result. 

The first Kalman equation - the measurement update to the true Kalman state - is:
\begin{align}
\apx{n} &= (1 - \gamma_nH_n) \amx{n} + \gamma_ny_n \\
& =  \amx{n} + \gamma_n(y_n - H_n\amx{n}) \\
&= \amx{n} + \gamma_n (y_n - \hat{y}_n(-)) \label{eqn:KF_update_x+} 
\end{align}
 The second Kalman update equation - the  update to the uncertainty of the true Kalman state - is:
\begin{align}
\apx{n} &=  \amx{n} - \gamma_nH_n \amx{n} + \gamma_nH_n x_n + \gamma_nv_n  \\
e^{+}_n &\equiv \apx{n} - x_n \\
%&=  \amx{n} - \gamma_nH_n \amx{n} + \gamma_nH_n x_n + \gamma_nv_n - x_n \nonumber \\
 &=  e_n - \gamma_nH_n e_n + \gamma_nv_n \\
& = \left[1 - \gamma_nH_n \right] e_n + \gamma_nv_n  \\
\app{n} &\equiv \ex{e_n^+ e_n^{+T}} \\
% & = E \left[ \left[1 - \gamma_nH_n \right] e_n e_n^T \left[1 - \gamma_nH_n \right]^T + \gamma_nv_n e_n^T \left[1 - \gamma_nH_n \right]^T   +  \left[1 - \gamma_nH_n \right] e_n v_n^T \gamma^T + \gamma_nv_n v_n^T \gamma^T \right] \\
% & = \left[1 - \gamma_n H_n \right] \amp{n}\left[1 - \gamma_n H_n \right]^T + \gamma_n R_n \gamma_n^T  \\
% & =  \amp{n} - \amp{n}H_n^T\gamma_n^T - \gamma_n H_n \amp{n} + \gamma_n \left[ H_n \amp{n}H_n^T +  R_n \right] \gamma_n^T, \quad \text{using $\gamma_n$}\\
% &= \amp{n} - \amp{n}H_n^T\gamma_n^T - \gamma_n H_n \amp{n} +  \amp{n}H_n^T \gamma_n^T  \\ 
&= \left[1  - \gamma_n H_n \right] \amp{n} \label{eqn:KF_update_p+}
\end{align}
While the true Kalman state is propagated in time via the dynamical model, we must also propagate the Kalman estimate of the uncertainity in the true state. Unlike a typical Kalman filter, true Kalman state estimation cannot be decoupled from state variance estimation due to $\Gamma_n$ in LKFFB. This means Kalman gains cannot be calculated in advance of data collection. We confirm below that Kalman uncertainty estimates can be propagated as:
\begin{align}
e_n &= \amx{n} - x_n \\
&= \Phi_{n-1}\left[\apx{n-1} - x_{n-1} \right] - \Gamma_{n-1}w_{n-1} \\
&= \Phi_{n-1}e^{+}_{n-1} - \Gamma_{n-1}w_{n-1} \\
\amp{n} &= E[e_ne_n^T] \\
&= \Phi_{n-1}E\left[e^{+}_{n-1}e^{+T}_{n-1}\right]\Phi_{n-1}^T  \\
% & - \Phi_{n-1}E\left[e^{+}_{n-1}w_{n-1}^T\Gamma_{n-1}^T\right] - E\left[\Gamma_{n-1}w_{n-1}e^{+T}_{n-1}\right]\Phi_{n-1}^T  \\
% & + E\left[\Gamma_{n-1}w_{n-1}w_{n-1}^T\Gamma_{n-1}^T\right] \\
&= \Phi_{n-1}E\left[\app{n-1}\right]\Phi_{n-1}^T  \\
& + E\left[\Gamma_{n-1}w_{n-1}w_{n-1}^T\Gamma_{n-1}^T\right] \nonumber \\
& - \Phi_{n-1}E\left[(\apx{n-1} - x_{n-1})w_{n-1}^T\Gamma_{n-1}^T\right] \nonumber \\
& - E\left[\Gamma_{n-1}w_{n-1}(\apx{n-1} - x_{n-1})^T\right]\Phi_{n-1}^T  \nonumber  \\
% &= \Phi_{n-1} \app{n-1} \Phi_{n-1}^T + E\left[\Gamma_{n-1}w_{n-1}w_{n-1}^T\Gamma_{n-1}^T\right], \label{eqn:KF_update_p-_0} \\
&= \Phi_{n-1} \app{n-1} \Phi_{n-1}^T + Q_{n-1}, \label{eqn:KF_update_p-} \\
Q_{n} & \equiv E\left[\Gamma_{n}w_{n}w_{n}^T\Gamma_{n}^T\right] 
\end{align}
We use \cref{eqn:KF_reln_16,eqn:KF_reln_17} to obtain \cref{eqn:KF_update_p-} in the last step. Hence, the standard Kalman predictor equations - \ref{eqn:KF_update_gamma}, \ref{eqn:KF_update_x+}, \ref{eqn:KF_update_p+}, \ref{eqn:KF_update_p-} - are valid for adaptive noise features given in \cite{livska2007}, as stated in the main text. 

% ##############################################################################
\section{Periodic Kernel as Infinite Basis of Oscillators} \label{sec:ap_approxSP:GPRPKernel}
% ##############################################################################

We use high level remarks in \cite{solin2014explicit} to explicitly work out that a sine squared exponential (Periodic Kernel) used in Gaussian Process Regression satisfies covariance function of trigometric polynomials:

\begin{align}
\omega_0 &\equiv \frac{\omega_j}{j}, j \in \{0, 1,..., J\} \\
R(v) &\equiv \sigma^2 \exp (- \frac{2\sin^2(\frac{\omega_0 v}{2})}{l^2}) \\
&=  \sigma^2 \exp (- \frac{1}{l^2}) \exp (\frac{\cos(\omega_0 v)}{l^2}) \label{eqn:periodic_0}\\
&=  \sigma^2 \exp (- \frac{1}{l^2}) \sum_{n = 0}^{\infty} \frac{1}{n!} \frac{\cos^n(\omega_0 v)}{l^{2n}} \label{eqn:periodic_1}
\end{align}
Next, we expand each cosine using power reduction formulae for odd and even powers respectively, and we re-group terms to yield for $n = 0,1,2,3,4,5...$:

\begin{widetext}
\begin{align}
R(v) &= \sigma^2 \exp (- \frac{1}{l^2}) \cos(\omega_0 v) \left[ \frac{2}{(2l^2)}\binom{1}{0} + \frac{2}{(2l^2)^3} \frac{1}{3!} \binom{3}{1} +  \frac{2}{(2l^2)^5} \frac{1}{5!}\binom{5}{2} \dots \right] \label{eqn:cosine1}\\
& + \sigma^2 \exp (- \frac{1}{l^2}) \cos(2\omega_0 v) \left[ \frac{2}{(2l^2)^2} \frac{1}{2!} \binom{2}{0} + \frac{2}{(2l^2)^4} \frac{1}{4!} \binom{4}{1} + \dots \right] \\
& + \sigma^2 \exp (- \frac{1}{l^2}) \cos(3\omega_0 v) \left[ \frac{2}{(2l^2)^3} \frac{1}{3!} \binom{3}{0} + \frac{2}{(2l^2)^5} \frac{1}{5!}\binom{5}{1} \dots \right] \\
& + \sigma^2 \exp (- \frac{1}{l^2}) \cos(4\omega_0 v) \left[ \frac{2}{(2l^2)^4} \frac{1}{4!} \binom{4}{0} + \dots \right] \\
& + \sigma^2 \exp (- \frac{1}{l^2}) \cos(5\omega_0 v) \left[ \frac{2}{(2l^2)^5} \frac{1}{5!}\binom{5}{0} + \dots \right] \label{eqn:cosine5}\\
& \vdots \nonumber \\
& + \sigma^2 \exp (- \frac{1}{l^2}) \left[ \frac{1}{(2l^2)^2} \frac{1}{2!} \binom{2}{1} + \frac{1}{(2l)^4} \frac{1}{4!} \binom{4}{2} + \dots \right] + \sigma^2 \exp (- \frac{1}{l^2}) \label{eqn:eventerms}
\end{align}

Here, the vertical and horizontal dots represent contributions from $n>5$ terms. We now summarise the amplitudes \cref{eqn:cosine1} to  \cref{eqn:cosine5} in second term of $R(v)$ and  \cref{eqn:eventerms} corresponds to $p_{0,N}$ term below:
\begin{align}
R(v) &= \sigma^2 (p_{0,N} + \sum_{j=0}^{\infty} p_{j,N} \cos(j\omega_0 v))\\
% & =  \sigma^2 \sum_{j=1}^{\infty} p_j \cos(j\omega_0 v) \label{eqn:beta_series1} \\
p_{j,N} & \equiv \sigma^2 \exp (- \frac{1}{l^2}) \sum_{\beta = 0}^{\beta = \beta_{j,n}^{MAX}} \frac{2}{(2l^2)^{(j + 2\beta)}} \frac{1}{(j + 2\beta)!} \binom{j + 2\beta}{\beta} \label{eqn:beta_series2} \\
\beta &\equiv  0,1,..., \beta_{j,n}^{MAX}  \\
p_{0,N} &= \exp (- \frac{1}{l^2}) \sum_{\alpha = 0}^{\alpha = \alpha_{n}^{MAX}} \frac{1}{(2l^2)^{(2\alpha)}} \frac{1}{(2\alpha)!} \binom{2\alpha}{\alpha} \label{eqn:alpha_series}\\
\alpha &\equiv  0,1,..., \alpha_{n}^{MAX} 
\end{align}

Here, the indice $j$ labels an infinite comb of oscillators and $n$ represents the higher order terms in the power reduction forumulae. The key message is that truncating $n$ to a finite number of terms $N$ will forecably truncate $j$ to represent a finite number of oscillators, $J$. By examining the cosine expansion, one sees that a truncation at $(N, J)$ means our summarised formulae will require $\beta_{j,N}^{MAX} = \lfloor\frac{N-j}{2}\rfloor$ and $\alpha_{N}^{MAX} = \lfloor\frac{N}{2}\rfloor$  where $\lfloor \rfloor$ denotes the ceiling floor. If we truncate with $N \equiv J$ such that $\alpha_{N}^{MAX} = \lfloor\frac{J}{2}\rfloor, \beta_{j,N}^{MAX} =  \lfloor\frac{J-j}{2}\rfloor $ and re-adjust the kernel for the zero-th frequency term, then we agree with results in \cite{solin2014explicit} and we recover the covariance function for trigonometric polynomials appoximating a covariance stationary process. 
\end{widetext}

\iffalse %%%%%%%%%% THIS BELONGS IN THE APPENDIX, NOT THE MAIN TEXT AS ITS A NUMERICAL DETAIL
\begin{align}
\sigma_k, R_k &\equiv \iota_0 10^{\iota_1} \\
\iota_0 & \sim \mathcal{U}[0, 1]\\
\iota_1 & \sim \mathcal{U}[\{ -\iota_{max}, -\iota_{max} + 1,  \hdots,  \iota_{min}\}]
\end{align}
Scale magnitudes are set by $\iota_1$, a random integer chosen with uniform probability over $\{ -\iota_{max}, \hdots, \iota_{min} \}$ where we set $\iota_{min} = 3, \iota_{max} = 8$  such that $10^{-\iota_{max}}$ is sufficiently high to avoid machine floating point errors from recursive calculations over $> 10^3$ measurements. Uniformly distributed floating points for $\sigma_k, R_k $ in each order of magnitude is set by $\iota_0$. 
\fi